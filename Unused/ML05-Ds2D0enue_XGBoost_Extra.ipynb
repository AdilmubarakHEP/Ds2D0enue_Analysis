{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip uninstall -y scikit-learn\n",
    "# !pip install scikit-learn==1.3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install --upgrade pip\n",
    "# ! pip install --user xgboost seaborn\n",
    "# ! pip install --user bayesian-optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import mplhep\n",
    "import sys\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import uproot\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "import matplotlib.colors as colors\n",
    "\n",
    "from sklearn.datasets import make_classification,make_regression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import auc,roc_curve,confusion_matrix,classification_report,precision_recall_curve,mean_squared_error,accuracy_score,roc_auc_score\n",
    "from sklearn.model_selection import GridSearchCV, cross_validate, validation_curve,train_test_split,KFold,learning_curve,cross_val_score\n",
    "from scipy.stats import ks_2samp\n",
    "\n",
    "import xgboost\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "from scipy.stats import randint, uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({\n",
    "    \"axes.labelsize\": 14,\n",
    "    \"xtick.labelsize\": 12,\n",
    "    \"ytick.labelsize\": 12,\n",
    "    \"legend.fontsize\": 12,\n",
    "    \"figure.titlesize\": 18\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"/home/belle2/amubarak/Ds2D0enue_Analysis/07-Python_Functions/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep-Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this notebook we only process the main signal and the generic events,\n",
    "# for illustration purposes.\n",
    "# You can add other backgrounds after if you wish.\n",
    "samples = [\"Signal\",\"All\",\"BB\",\"ccbar\",\"ddbar\",\"ssbar\",\"taupair\",\"uubar\",\"uds\"]\n",
    "GenEvents = [\"Signal\",\"BB\",\"ccbar\",\"ddbar\",\"ssbar\",\"taupair\",\"uubar\"]\n",
    "\n",
    "DataFrames = {}  # define empty dictionary to hold dataframes\n",
    "\n",
    "# Signal:\n",
    "DataFrames[samples[0]] =  uproot.concatenate(\"/home/belle2/amubarak/C01-Simulated_Events/Ds2D0enu-Signal.root:Dstree\",library='pd')\n",
    "# Background\n",
    "for s in samples[1:]: # loop over samples\n",
    "    DataFrames[s] =  uproot.concatenate(\"/group/belle2/users2022/amubarak/TopoAna/Completed_TopoAna/TopoAna_\"+ s +\".root:Dstree\",library='pd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 200000)\n",
    "pd.set_option('display.max_columns', 200000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The line below is to look at the available variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataFrames[\"All\"].columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "The code below will be used to apply cuts to the data.  \n",
    "The range of the plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Electron ID\n",
    "#-------------------\n",
    "# DataFrames[\"Signal\"] = DataFrames[\"Signal\"][DataFrames[\"Signal\"]['e_electronID']>=0.95]\n",
    "# DataFrames[\"ccbar\"] = DataFrames[\"ccbar\"][DataFrames[\"ccbar\"]['e_electronID']>=0.95]\n",
    "# DataFrames[\"Signal\"] = DataFrames[\"Signal\"][DataFrames[\"Signal\"]['Ds_gammaveto_em_electronID']>=0.95]\n",
    "# DataFrames[\"ccbar\"] = DataFrames[\"ccbar\"][DataFrames[\"ccbar\"]['Ds_gammaveto_em_electronID']>=0.95]\n",
    "\n",
    "# Photon Conversion\n",
    "#-------------------\n",
    "# DataFrames[samples[0]] = DataFrames[samples[0]][DataFrames[samples[0]]['Ds_gammaveto_M_Correction']>=0.1]\n",
    "# DataFrames[samples[1]] = DataFrames[samples[1]][DataFrames[samples[1]]['Ds_gammaveto_M_Correction']>=0.1]\n",
    "\n",
    "# Peaking Background Removal\n",
    "#----------------------------\n",
    "# DataFrames[\"ccbar\"] = DataFrames[\"ccbar\"][(DataFrames[\"ccbar\"]['Ds_diff_D0pi']>=0.15)]\n",
    "# DataFrames[\"Signal\"] = DataFrames[\"Signal\"][(DataFrames[\"Signal\"]['Ds_diff_D0pi']>=0.15)]\n",
    "\n",
    "# # Vertex Fitting\n",
    "# #----------------\n",
    "# DataFrames[\"Signal\"] = DataFrames[\"Signal\"][DataFrames[\"Signal\"]['Ds_chiProb']>=0.01]\n",
    "# DataFrames[\"ccbar\"] = DataFrames[\"ccbar\"][DataFrames[\"ccbar\"]['Ds_chiProb']>=0.01]\n",
    "\n",
    "# Dalitz Removal\n",
    "#----------------------------\n",
    "# DataFrames[\"ccbar\"] = DataFrames[\"ccbar\"][(DataFrames[\"ccbar\"]['Ds_pi0veto_M_Correction']<=0.08) | (DataFrames[\"ccbar\"]['Ds_pi0veto_M_Correction']>=0.16)]\n",
    "# DataFrames[\"Signal\"] = DataFrames[\"Signal\"][(DataFrames[\"Signal\"]['Ds_pi0veto_M_Correction']<=0.08) | (DataFrames[\"Signal\"]['Ds_pi0veto_M_Correction']>=0.16)]\n",
    "\n",
    "# Vertex Fit\n",
    "#----------------\n",
    "# DataFrames[samples[0]] = DataFrames[samples[0]][DataFrames[samples[0]]['Ds_chiProb_rank']==1]\n",
    "# DataFrames[samples[1]] = DataFrames[samples[1]][DataFrames[samples[1]]['Ds_chiProb_rank']==1]\n",
    "\n",
    "# D0 Invariant Mass\n",
    "#-----------------------\n",
    "# DataFrames[samples[0]] = DataFrames[samples[0]][(DataFrames[samples[0]]['Ds_D0_sideband']==1)]\n",
    "# DataFrames[samples[1]] = DataFrames[samples[1]][(DataFrames[samples[1]]['Ds_D0_sideband']==1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $D^{*+}$ Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Settings ===\n",
    "Stacked = True\n",
    "Density = False\n",
    "Bins = 50\n",
    "Range = [0.0, 0.25]\n",
    "perBin = ((Range[1] - Range[0]) / Bins) * 1000\n",
    "print(\"Width Per Bin: {:.2f} MeV\".format(perBin))\n",
    "\n",
    "# Cut range on 'Ds_diff_D0pi'\n",
    "cut_low = 0.142\n",
    "cut_high = 0.15\n",
    "\n",
    "# Variable to plot after cut\n",
    "var = 'Ds_massDifference_0'\n",
    "\n",
    "# Labels and colors\n",
    "labels = [\n",
    "    r'$c \\bar{c}$',\n",
    "    r'$u \\bar{u}, \\; d \\bar{d}, \\;s \\bar{s}$',\n",
    "    r'$BB$',\n",
    "    r'$\\tau^{+} \\tau^{-}$'\n",
    "]\n",
    "\n",
    "# Apply sideband cut (outside signal region) and collect data\n",
    "data = [\n",
    "    DataFrames[\"ccbar\"].query(\"Ds_diff_D0pi <= @cut_low or Ds_diff_D0pi >= @cut_high\")[var],\n",
    "    DataFrames[\"uds\"].query(\"Ds_diff_D0pi <= @cut_low or Ds_diff_D0pi >= @cut_high\")[var],\n",
    "    DataFrames[\"BB\"].query(\"Ds_diff_D0pi <= @cut_low or Ds_diff_D0pi >= @cut_high\")[var],\n",
    "    DataFrames[\"taupair\"].query(\"Ds_diff_D0pi <= @cut_low or Ds_diff_D0pi >= @cut_high\")[var],\n",
    "]\n",
    "\n",
    "# === Plot ===\n",
    "# plt.figure(figsize=(8, 5))\n",
    "plt.hist(data[::-1],\n",
    "         label=labels[::-1],\n",
    "         density=Density,\n",
    "         stacked=Stacked,\n",
    "         bins=Bins,\n",
    "         range=Range,\n",
    "         histtype='step',\n",
    "         linewidth=2)\n",
    "\n",
    "# Titles\n",
    "plt.title(r'$D_s^{+} \\rightarrow [D^{0} \\rightarrow K^{-} \\pi^{+}] e^{+} \\nu_{e}$' + '\\n' + r'$\\Delta m_{\\pi}(D_s^{+} - D^{0}) \\notin [0.142,\\; 0.15] \\; \\mathrm{GeV}/c^{2}$', loc=\"left\")\n",
    "plt.title(r'$\\int\\mathcal{L}dt\\approx\\;1444$ fb$^{-1}$', loc=\"right\")\n",
    "\n",
    "# Labels\n",
    "plt.xlabel(r'$\\Delta m_{e}(D_s^{+} - D^{0})\\;[GeV/c^{2}]$')\n",
    "plt.ylabel(r'$Entries/(\\;{:.2f}\\;MeV/c^2)$'.format(perBin))\n",
    "plt.legend()\n",
    "# plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# === Settings ===\n",
    "Stacked = True\n",
    "Density = False\n",
    "Bins = 50\n",
    "Range = [0.0, 0.25]\n",
    "perBin = ((Range[1] - Range[0]) / Bins) * 1000\n",
    "print(\"Width Per Bin: {:.2f} MeV\".format(perBin))\n",
    "\n",
    "# Data source and variables\n",
    "df_cut = DataFrames[\"All\"]\n",
    "cut_var = \"Ds_diff_D0pi\"\n",
    "plot_var = 'Ds_massDifference_0'\n",
    "pdg_var = 'Ds_mcPDG'\n",
    "\n",
    "# === Categories based on true Ds_mcPDG ===\n",
    "dstar_plus = df_cut[abs(df_cut[pdg_var]) == 413][plot_var]\n",
    "dstar_zero = df_cut[abs(df_cut[pdg_var]) == 423][plot_var]\n",
    "other = df_cut[(abs(df_cut[pdg_var]) != 413) & (abs(df_cut[pdg_var]) != 423)][plot_var]\n",
    "\n",
    "# === Plot ===\n",
    "plt.hist([other, dstar_zero, dstar_plus],\n",
    "         color=[\"#2E2E2E\", \"#4C6EB1\", \"#007C91\"],\n",
    "         label=[\"Other\", r\"$D^{*0}$\", r\"$D^{*+}$\"],\n",
    "         density=Density,\n",
    "         stacked=Stacked,\n",
    "         bins=Bins,\n",
    "         range=Range,\n",
    "         histtype='step',\n",
    "         linewidth=2)\n",
    "\n",
    "# Titles and labels\n",
    "plt.title(r'$D_s^{+} \\rightarrow [D^{0} \\rightarrow K^{-} \\pi^{+}] e^{+} \\nu_{e}$', loc=\"left\")\n",
    "plt.title(r'$\\int\\mathcal{L}dt\\approx\\;1444$ fb$^{-1}$', loc=\"right\")\n",
    "plt.xlabel(r'$\\Delta m_{e}(D_s^{+} - D^{0})\\;[GeV/c^{2}]$')\n",
    "plt.ylabel(r'$Entries/(\\;{:.2f}\\;MeV/c^2)$'.format(perBin))\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# === Settings ===\n",
    "Stacked = True\n",
    "Density = False\n",
    "Bins = 50\n",
    "Range = [0.0, 0.25]\n",
    "perBin = ((Range[1] - Range[0]) / Bins) * 1000\n",
    "print(\"Width Per Bin: {:.2f} MeV\".format(perBin))\n",
    "\n",
    "# Data source and variables\n",
    "df = DataFrames[\"All\"]\n",
    "cut_var = \"Ds_diff_D0pi\"\n",
    "plot_var = 'Ds_massDifference_0'\n",
    "pdg_var = 'Ds_mcPDG'\n",
    "\n",
    "# Sideband cut (exclude D*⁺ peak)\n",
    "cut_low = 0.142\n",
    "cut_high = 0.15\n",
    "df_cut = df.query(f\"{cut_var} <= @cut_low or {cut_var} >= @cut_high\")\n",
    "\n",
    "# === Categories based on true Ds_mcPDG ===\n",
    "dstar_plus = df_cut[abs(df_cut[pdg_var]) == 413][plot_var]\n",
    "dstar_zero = df_cut[abs(df_cut[pdg_var]) == 423][plot_var]\n",
    "other = df_cut[(abs(df_cut[pdg_var]) != 413) & (abs(df_cut[pdg_var]) != 423)][plot_var]\n",
    "\n",
    "# === Plot ===\n",
    "plt.hist([other, dstar_zero, dstar_plus],\n",
    "         color=[\"#2E2E2E\", \"#4C6EB1\", \"#007C91\"],\n",
    "         label=[\"Other\", r\"$D^{*0}$\", r\"$D^{*+}$\"],\n",
    "         density=Density,\n",
    "         stacked=Stacked,\n",
    "         bins=Bins,\n",
    "         range=Range,\n",
    "         histtype='step',\n",
    "         linewidth=2)\n",
    "\n",
    "# Titles and labels\n",
    "plt.title(r'$D_s^{+} \\rightarrow [D^{0} \\rightarrow K^{-} \\pi^{+}] e^{+} \\nu_{e}$' + '\\n' +\n",
    "          r'$\\Delta m_{\\pi}(D_s^{+} - D^{0}) \\notin [0.142,\\; 0.15] \\; \\mathrm{GeV}/c^{2}$', loc=\"left\")\n",
    "plt.title(r'$\\int\\mathcal{L}dt\\approx\\;1444$ fb$^{-1}$', loc=\"right\")\n",
    "plt.xlabel(r'$\\Delta m_{e}(D_s^{+} - D^{0})\\;[GeV/c^{2}]$')\n",
    "plt.ylabel(r'$Entries/(\\;{:.2f}\\;MeV/c^2)$'.format(perBin))\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_2d_with_marginals(df, xvar, yvar, xrange=None, yrange=None, bins=50, title=None):\n",
    "    # Extract variables\n",
    "    x = df[xvar]\n",
    "    y = df[yvar]\n",
    "\n",
    "    # Apply range cuts if specified\n",
    "    if xrange is not None:\n",
    "        xmask = (x >= xrange[0]) & (x <= xrange[1])\n",
    "    else:\n",
    "        xmask = pd.Series([True] * len(x))\n",
    "\n",
    "    if yrange is not None:\n",
    "        ymask = (y >= yrange[0]) & (y <= yrange[1])\n",
    "    else:\n",
    "        ymask = pd.Series([True] * len(y))\n",
    "\n",
    "    # Combined mask\n",
    "    mask = xmask & ymask\n",
    "    x = x[mask]\n",
    "    y = y[mask]\n",
    "\n",
    "    # Set up figure layout\n",
    "    fig = plt.figure(figsize=(6, 6))\n",
    "    gs = gridspec.GridSpec(4, 4, hspace=0.05, wspace=0.05)\n",
    "\n",
    "    ax_main = fig.add_subplot(gs[1:4, 0:3])\n",
    "    ax_xhist = fig.add_subplot(gs[0, 0:3], sharex=ax_main)\n",
    "    ax_yhist = fig.add_subplot(gs[1:4, 3], sharey=ax_main)\n",
    "    ax_cbar = fig.add_subplot(gs[0, 3])\n",
    "\n",
    "    # Define histogram range\n",
    "    hist_range = None\n",
    "    if xrange is not None and yrange is not None:\n",
    "        hist_range = [xrange, yrange]\n",
    "\n",
    "    # 2D histogram\n",
    "    # counts, xedges, yedges, im = ax_main.hist2d(x, y, bins=bins, range=hist_range, cmap=\"viridis\", norm=colors.LogNorm())  # <-- This is the key)\n",
    "    counts, xedges, yedges, im = ax_main.hist2d(x, y, bins=bins, range=hist_range, cmap=\"viridis\")\n",
    "    cbar = fig.colorbar(im, cax=ax_cbar)\n",
    "    cbar.set_label(\"Entries\")\n",
    "\n",
    "    # Marginal histograms\n",
    "    ax_xhist.hist(x, bins=bins, range=xrange, color=\"#2E2E2E\", histtype='step', linewidth=1.5)\n",
    "    ax_yhist.hist(y, bins=bins, range=yrange, orientation=\"horizontal\", color=\"#007C91\", histtype='step', linewidth=1.5)\n",
    "\n",
    "    # Axis labels\n",
    "    ax_main.set_xlabel(r'$\\Delta m_{e}(D_s^{+} - D^{0})\\;[GeV/c^{2}]$')\n",
    "    ax_main.set_ylabel(r'$\\Delta m_{\\pi}(D_s^{+} - D^{0})\\;[GeV/c^{2}]$')\n",
    "    ax_xhist.set_ylabel(\"Entries\")\n",
    "    ax_yhist.set_xlabel(\"Entries\")\n",
    "\n",
    "    # Clean ticks\n",
    "    plt.setp(ax_xhist.get_xticklabels(), visible=False)\n",
    "    plt.setp(ax_yhist.get_yticklabels(), visible=False)\n",
    "\n",
    "    # Optional title\n",
    "    if title:\n",
    "        plt.suptitle(title, fontsize=14)\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])  # Leave space for title\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_2d_with_marginals(DataFrames[\"All\"],\n",
    "    xvar=\"Ds_massDifference_0\",\n",
    "    yvar=\"Ds_diff_D0pi\",\n",
    "    xrange=(0.0, 0.25),\n",
    "    yrange=(0.1, 0.55),\n",
    "    bins=60,\n",
    "    title=\"Generic Events\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_2d_with_marginals(DataFrames[\"Signal\"],\n",
    "    xvar=\"Ds_massDifference_0\",\n",
    "    yvar=\"Ds_diff_D0pi\",\n",
    "    xrange=(0.0, 0.25),\n",
    "    yrange=(0.1, 0.60),\n",
    "    bins=60,\n",
    "    title=\"Signal\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $D^{*+}$ BDT Cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut_low = 0.142\n",
    "cut_high = 0.151\n",
    "\n",
    "# Apply sideband cut to 'All' sample\n",
    "DataFrames[\"All\"] = DataFrames[\"All\"][\n",
    "    (DataFrames[\"All\"][\"Ds_diff_D0pi\"] <= cut_low) | (DataFrames[\"All\"][\"Ds_diff_D0pi\"] >= cut_high)\n",
    "]\n",
    "\n",
    "# Apply sideband cut to each generator-level sample\n",
    "for s in GenEvents:\n",
    "    DataFrames[s] = DataFrames[s][\n",
    "        (DataFrames[s][\"Ds_diff_D0pi\"] <= cut_low) | (DataFrames[s][\"Ds_diff_D0pi\"] >= cut_high)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fake $D^0$ Suppression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataFrames[\"All\"].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataFrames[\"All\"][\"D0_isSignal\"] = DataFrames[\"All\"][\"D0_isSignal\"].replace(np.nan, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataFrames[\"All\"][\"Ds_isSignal\"] = DataFrames[\"All\"][\"Ds_isSignal\"].replace(np.nan, 0)\n",
    "\n",
    "for s in GenEvents[0:]: # loop over samples\n",
    "    DataFrames[s][\"Ds_isSignal\"] = DataFrames[s][\"Ds_isSignal\"].replace(np.nan, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Variables = [\n",
    "            #  'K_dr',\n",
    "             'pi_dr',\n",
    "            #  'K_kaonID',\n",
    "            #  'pi_pionID',\n",
    "             'D0_dM',\n",
    "             'D0_chiProb',\n",
    "             'D0_flightDistance',\n",
    "             'D0_useCMSFrame_p',\n",
    "             'D0_cos_decayAngle_1',\n",
    "            #  'D0_daughterAngle_0_1',\n",
    "             ]\n",
    "\n",
    "features = [\n",
    "            #  r'$dr(K^{-})$',\n",
    "             r'$dr(\\pi^{+})$',\n",
    "            #  r'$kaonID(K^{-})$',\n",
    "            #  r'$pionID(\\pi^{+})$',\n",
    "             r'$m(D^{0}) - m_{PDG}(D^{0})$',\n",
    "             r'$p-value(D^{0})$',\n",
    "             r'$Flight \\; Distance(D^{0})$',\n",
    "             r'$p^{*} (D^{0})$',\n",
    "             r'$\\cos\\theta^*_{daughter_{1}}$',\n",
    "            #  r'$\\Delta \\theta (K^{-} \\pi^{+})$',\n",
    "             ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18, 15))\n",
    "\n",
    "heatmap = sns.heatmap(DataFrames[\"Signal\"][Variables].corr(), annot=True, cmap=\"coolwarm\",vmin=-1, vmax=1)\n",
    "\n",
    "heatmap.set_title('Signal Correlation Heatmap', fontdict={'fontsize':20}, pad=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18, 15))\n",
    "\n",
    "heatmap = sns.heatmap(DataFrames[\"All\"][Variables].corr(), annot=True, cmap=\"coolwarm\",vmin=-1, vmax=1)\n",
    "\n",
    "heatmap.set_title('Background Correlation Heatmap', fontdict={'fontsize':20}, pad=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your features and labels from the 'All' dataset\n",
    "X = DataFrames[\"All\"][Variables].to_numpy(dtype=np.float32)\n",
    "y = DataFrames[\"All\"]['D0_isSignal'].to_numpy(dtype=np.int64)\n",
    "\n",
    "# # Reference variable for decorrelation — this is what uBoost will try to flatten for background\n",
    "# ref_variable = DataFrames[\"All\"][\"D0_dM\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting with  Holdout method for eval_set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size=0.30,\n",
    "                                                    random_state=42,\n",
    "                                                    # stratify=y\n",
    "                                                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Compute the positive class weight\n",
    "# pos_class_weight = (len(y) - np.sum(y)) / np.sum(y)\n",
    "\n",
    "# Calculate class imbalance ratio\n",
    "neg, pos = np.bincount(y)  # 0s and 1s\n",
    "scale = neg / pos\n",
    "\n",
    "# Create EarlyStopping callback\n",
    "early_stop = xgboost.callback.EarlyStopping(\n",
    "    rounds=10,\n",
    "    metric_name='rmse',\n",
    "    data_name=\"validation_0\",\n",
    "    save_best=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_set = [(X_test, y_test)]\n",
    "xgbm_final = XGBClassifier(objective=\"binary:logistic\",\n",
    "                    eval_metric=\"logloss\",\n",
    "                    # early_stopping_rounds=10,\n",
    "                    # scale_pos_weight=pos_class_weight,\n",
    "                    scale_pos_weight=scale,\n",
    "                    max_delta_step=1,\n",
    "                    random_state=42,\n",
    "                    n_estimators=100)\n",
    "\n",
    "xgbm_final.fit(X_train, y_train, \n",
    "        eval_set=[(X_train, y_train),(X_test, y_test)], \n",
    "        # early_stopping_rounds=5,\n",
    "        verbose=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss curve of xgboost\n",
    "results = xgbm_final.evals_result()\n",
    "\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.plot(results[\"validation_0\"][\"logloss\"], label=\"Training loss\")\n",
    "plt.plot(results[\"validation_1\"][\"logloss\"], label=\"Validation loss\")\n",
    "plt.xlabel(\"Number of trees\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Optimization\n",
    "This optimization is pulling too much resources and ending the connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_dist = {\n",
    "#     \"learning_rate\": uniform(0.01, 0.2),        # e.g., 0.01 to 0.21\n",
    "#     \"max_depth\": randint(1, 5),                 # 1 to 4\n",
    "#     \"n_estimators\": randint(100, 201),          # 100 to 200\n",
    "#     \"reg_lambda\": randint(1, 5),\n",
    "#     \"gamma\": randint(0, 4),\n",
    "#     \"subsample\": uniform(0.5, 0.5),             # 0.5 to 1.0\n",
    "#     \"min_child_weight\": randint(1, 6),\n",
    "#     \"colsample_bytree\": uniform(0.3, 0.7)\n",
    "# }\n",
    "\n",
    "# random_search = RandomizedSearchCV(\n",
    "#     bdt,\n",
    "#     param_distributions=param_dist,\n",
    "#     n_iter=50,  # Try only 50 random combos (you can adjust)\n",
    "#     cv=5,\n",
    "#     n_jobs=-1,\n",
    "#     random_state=42,\n",
    "#     verbose=1\n",
    "# )\n",
    "\n",
    "# # After running RandomizedSearchCV:\n",
    "# random_search.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=0)\n",
    "\n",
    "# # Extract best parameters and apply them to the base model\n",
    "# xgbm_final = bdt.set_params(**random_search.best_params_, random_state=17).fit(X_train, y_train)\n",
    "\n",
    "# cv_results = cross_validate(xgbm_final, X, y, cv=10,\n",
    "#                             scoring=[\"f1\"],return_train_score=True)\n",
    "\n",
    "# print(cv_results['train_f1'].mean())\n",
    "# print(cv_results['test_f1'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Best parameters found by RandomizedSearchCV:\")\n",
    "# for param, value in random_search.best_params_.items():\n",
    "#     print(f\"{param:20s}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance scores\n",
    "print(xgbm_final.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_importance(model, features, num=len(X), save=False):\n",
    "    feature_imp = pd.DataFrame({'Value': model.feature_importances_, 'Feature': features})\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    sns.set(font_scale=1)\n",
    "    sns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\",\n",
    "                                                                     ascending=False)[0:num])\n",
    "    plt.title('Features')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    if save:\n",
    "        plt.savefig('importances.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_importance(xgbm_final, features)\n",
    "# When the feature importance graph is observed, \n",
    "# it is seen that the variables other than a02 and a01 are important for the xgboost model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "def get_pulls(counts,errors,pdf):\n",
    "    pull = (-pdf + counts) / errors\n",
    "    return pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_train_test(clf, X_train, y_train, X_test, y_test):\n",
    "    Density = True\n",
    "    decisions = [] # list to hold decisions of classifier\n",
    "    for X,y in ((X_train, y_train), (X_test, y_test)): # train and test\n",
    "        if hasattr(clf, \"predict_proba\"): # if predict_proba function exists\n",
    "            d1 = clf.predict_proba(X[y<0.5])[:, 1] # background\n",
    "            d2 = clf.predict_proba(X[y>0.5])[:, 1] # signal\n",
    "        else: # predict_proba function doesn't exist\n",
    "            X_tensor = torch.as_tensor(X, dtype=torch.float) # make tensor from X_test_scaled\n",
    "            y_tensor = torch.as_tensor(y, dtype=torch.long) # make tensor from y_test\n",
    "            X_var, y_var = Variable(X_tensor), Variable(y_tensor) # make variables from tensors\n",
    "            d1 = clf(X_var[y_var<0.5])[1][:, 1].cpu().detach().numpy() # background\n",
    "            d2 = clf(X_var[y_var>0.5])[1][:, 1].cpu().detach().numpy() # signal\n",
    "        decisions += [d1, d2] # add to list of classifier decision\n",
    "\n",
    "    #pd.set_option('max_columns', None)\n",
    "#     %config InlineBackend.figure_format = 'retina'\n",
    "    # plt.style.use('belle2')\n",
    "    lw=3\n",
    "\n",
    "    fig,axs=plt.subplots(3,1,figsize=(10,10),gridspec_kw={'height_ratios':[1,0.2,0.2]})\n",
    "\n",
    "    bins = 50\n",
    "    bin_edges = np.linspace(0,1,bins)\n",
    "    \n",
    "    test_bkg_count_weight=bins/len(decisions[2])\n",
    "    test_sig_count_weight=bins/len(decisions[3])\n",
    "    test_bkg_counts,test_bkg_bins = np.histogram(decisions[2],bins=bins,range=(0,1))\n",
    "    test_sig_counts,test_sig_bins = np.histogram(decisions[3],bins=bins,range=(0,1))\n",
    "\n",
    "    train_bkg_counts,train_bkg_bins,_etc=axs[0].hist(decisions[0],color = 'tab:blue',\n",
    "            histtype='step',bins=bins,density=Density,range=(0,1),linewidth=lw,label='Train Background')\n",
    "    train_sig_counts,train_sig_bins,_etc=axs[0].hist(decisions[1],color = 'tab:red',\n",
    "            histtype='step',bins=bins,density=Density,range=(0,1),linewidth=lw,label=r'Train Signal')\n",
    "    axs[0].hist(decisions[0],color = 'tab:blue',\n",
    "            histtype='stepfilled',alpha=0.4,bins=bins,density=Density,range=(0,1))\n",
    "    axs[0].hist(decisions[1],color = 'tab:red',\n",
    "            histtype='stepfilled',alpha=0.4,bins=bins,density=Density,range=(0,1))\n",
    "    bin_width=test_bkg_bins[1]-test_bkg_bins[0]\n",
    "    bin_centers=[el+(bin_width/2) for el in test_bkg_bins[:-1]]\n",
    "\n",
    "    axs[0].errorbar(bin_centers,test_bkg_count_weight*test_bkg_counts,\n",
    "                yerr=test_bkg_count_weight*np.sqrt(test_bkg_counts),label='Test Background',color='tab:blue',\n",
    "                marker='o',linewidth=lw,ls='')\n",
    "    axs[0].errorbar(bin_centers,test_sig_count_weight*test_sig_counts,\n",
    "                yerr=test_sig_count_weight*np.sqrt(test_sig_counts),label='Test Signal',color='tab:red',\n",
    "                marker='o',linewidth=lw,ls='')\n",
    "    axs[0].set_title(r'$D_{s}^{+} \\rightarrow D^{0} e^{+} \\nu_{e}$',loc='left')\n",
    "    axs[0].set_xlim(0,1)\n",
    "    axs[0].set_ylim(0)\n",
    "    axs[0].set_ylabel('Event Density')\n",
    "\n",
    "    x= decisions[1]\n",
    "    y=  decisions[3]\n",
    "    ks_p_value_sig = ks_2samp(x, y)[1]\n",
    "\n",
    "    x= decisions[0]\n",
    "    y= decisions[2]\n",
    "    ks_p_value_bkg = ks_2samp(x, y)[1]\n",
    "\n",
    "    leg=axs[0].legend(loc='upper center',title=f\"Sig K-S test score: {ks_p_value_sig:0.3f}\"+\n",
    "                      \"\\n\"+f\"Bkg K-S test score: {ks_p_value_bkg:0.3f}\")\n",
    "    leg._legend_box.align = \"left\"  \n",
    "\n",
    "    pulls=get_pulls(test_bkg_count_weight*test_bkg_counts,test_bkg_count_weight*np.sqrt(test_bkg_counts),np.array(train_bkg_counts))\n",
    "    axs[1].bar(bin_centers,pulls,width=bin_width)\n",
    "    axs[1].set_xlim(0,1)\n",
    "    axs[1].set_ylabel('Pulls')\n",
    "    axs[1].set_ylim(-5,5)\n",
    "\n",
    "    pulls=get_pulls(test_sig_count_weight*test_sig_counts,test_sig_count_weight*np.sqrt(test_sig_counts),np.array(train_sig_counts))\n",
    "    axs[2].bar(bin_centers,pulls,width=bin_width,color='tab:red')\n",
    "    axs[2].set_xlim(0,1)\n",
    "    axs[2].set_ylabel('Pulls')\n",
    "    axs[2].set_ylim(-5,5)\n",
    "    axs[2].set_xlabel(r'BDT output')\n",
    "\n",
    "    return decisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decisions = compare_train_test(xgbm_final, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basf2 ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "y_score_test = xgbm_final.predict_proba(X_test)[:, 1]\n",
    "fpr_test, tpr_test, thresholds_test = roc_curve(y_test, y_score_test)\n",
    "area_test = auc(fpr_test, tpr_test)\n",
    "\n",
    "y_score_train = xgbm_final.predict_proba(X_train)[:, 1]\n",
    "fpr_train, tpr_train, thresholds_train = roc_curve(y_train, y_score_train)\n",
    "area_train = auc(fpr_train, tpr_train)\n",
    "\n",
    "# Get classifier scores (probabilities for class 1)\n",
    "train_scores = xgbm_final.predict_proba(X_train)[:, 1]\n",
    "test_scores  = xgbm_final.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Use y_train and y_test to separate signal/background\n",
    "sig_train = train_scores[y_train == 1]\n",
    "bkg_train = train_scores[y_train == 0]\n",
    "sig_test  = test_scores[y_test == 1]\n",
    "bkg_test  = test_scores[y_test == 0]\n",
    "\n",
    "# Optionally, group them into one list like this:\n",
    "decisions = [bkg_train, sig_train, bkg_test, sig_test]\n",
    "\n",
    "bdt_cuts = np.linspace(0, 1, 100)\n",
    "\n",
    "sig_eff_train = []\n",
    "bkg_rej_train = []\n",
    "sig_eff_test = []\n",
    "bkg_rej_test = []\n",
    "fom_vals = []\n",
    "\n",
    "for cut in bdt_cuts:\n",
    "    num_sig_train = np.sum(sig_train > cut)\n",
    "    num_bkg_train = np.sum(bkg_train > cut)\n",
    "    num_sig_test = np.sum(sig_test > cut)\n",
    "    num_bkg_test = np.sum(bkg_test > cut)\n",
    "\n",
    "    # FoM calculation\n",
    "    fom = num_sig_test / np.sqrt(num_sig_test + num_bkg_test) if (num_sig_test + num_bkg_test) > 0 else 0\n",
    "    fom_vals.append(fom)\n",
    "\n",
    "    sig_eff_train.append(num_sig_train / len(sig_train))\n",
    "    bkg_rej_train.append(1 - (num_bkg_train / len(bkg_train)))\n",
    "    sig_eff_test.append(num_sig_test / len(sig_test))\n",
    "    bkg_rej_test.append(1 - (num_bkg_test / len(bkg_test)))\n",
    "\n",
    "# Find optimal FoM point\n",
    "fom_vals = np.array(fom_vals)\n",
    "best_idx = np.argmax(fom_vals)\n",
    "best_cut = bdt_cuts[best_idx]\n",
    "\n",
    "# Plot\n",
    "fig, axs = plt.subplots(1, 1, figsize=(7, 6))\n",
    "lw = 2\n",
    "\n",
    "# axs.plot([0, 1], [0, 1], color='grey', linestyle='--', label='Random')\n",
    "axs.plot(bkg_rej_train, sig_eff_train, color='tab:blue', linewidth=lw, label=f'Train (AUC = {area_train:.2f})')\n",
    "axs.plot(bkg_rej_test, sig_eff_test, color='tab:red', linestyle='--', linewidth=lw, label=f'Test (AUC = {area_test:.2f})')\n",
    "\n",
    "# ① Shade the overfit gap\n",
    "axs.fill_between(bkg_rej_test,\n",
    "                 sig_eff_train,\n",
    "                 sig_eff_test,\n",
    "                 where=(np.array(sig_eff_train) > np.array(sig_eff_test)),\n",
    "                 color='gray', alpha=0.2, label='Overfit Gap')\n",
    "\n",
    "# ② Mark the optimal cut point (from test curve)\n",
    "axs.axhline(sig_eff_test[best_idx], color='black', ls='--', linewidth=1.6,\n",
    "            label=f'Best FoM Cut = {best_cut:.3f}')\n",
    "axs.axvline(bkg_rej_test[best_idx], color='black', ls='--', linewidth=1.6)\n",
    "axs.scatter(bkg_rej_test[best_idx], sig_eff_test[best_idx], color='green', s=50)\n",
    "\n",
    "# Axis labels and formatting\n",
    "axs.set_title(r'$D_{s}^{+} \\rightarrow D^{0} e^{+} \\nu_{e}$', loc='left')\n",
    "axs.set_ylim(0, 1.05)\n",
    "axs.set_xlim(0, 1.05)\n",
    "axs.set_xlabel('Background rejection')\n",
    "axs.set_ylabel('Signal efficiency')\n",
    "axs.legend(loc='lower left')\n",
    "axs.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learing ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_score_test = xgbm_final.predict_proba(X_test)[:, 1]\n",
    "fpr_test, tpr_test, thresholds_test = roc_curve(y_test, y_score_test)\n",
    "area_test = auc(fpr_test, tpr_test)\n",
    "\n",
    "y_score_train = xgbm_final.predict_proba(X_train)[:, 1]\n",
    "fpr_train, tpr_train, thresholds_train = roc_curve(y_train, y_score_train)\n",
    "area_train = auc(fpr_train, tpr_train)\n",
    "\n",
    "plt.plot([0, 1], [0, 1], color='grey', linestyle='--')\n",
    "plt.plot(fpr_test, tpr_test, label=f'Test ROC curve (AUC = {area_test:.2f})')\n",
    "plt.plot(fpr_train, tpr_train, label=f'Train ROC curve (AUC = {area_train:.2f})')\n",
    "plt.xlim(0.0, 1.0)\n",
    "plt.ylim(0.0, 1.0)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend(loc='lower right')\n",
    "# We can make the plot look nicer by forcing the grid to be square\n",
    "plt.gca().set_aspect('equal', adjustable='box')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set\n",
    "y_pred_proba = xgbm_final.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate the ROC AUC score\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "print(f\"ROC AUC Score: {roc_auc:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on training and validation sets\n",
    "train_preds = xgbm_final.predict(X_train)\n",
    "val_preds = xgbm_final.predict(X_test)\n",
    "\n",
    "# Calculate accuracy scores\n",
    "train_accuracy = accuracy_score(y_train, train_preds)\n",
    "val_accuracy = accuracy_score(y_test, val_preds)\n",
    "\n",
    "print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "# Check for large difference between train and validation accuracy\n",
    "if train_accuracy - val_accuracy > 0.1:\n",
    "    print(\"Warning: The model may be overfitting!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if XGBoost Is Underfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on training and validation sets\n",
    "train_preds = xgbm_final.predict(X_train)\n",
    "val_preds = xgbm_final.predict(X_test)\n",
    "\n",
    "# Calculate MSE for training and validation sets\n",
    "train_mse = mean_squared_error(y_train, train_preds)\n",
    "val_mse = mean_squared_error(y_test, val_preds)\n",
    "\n",
    "print(f\"Training MSE: {train_mse:.4f}\")\n",
    "print(f\"Validation MSE: {val_mse:.4f}\")\n",
    "\n",
    "# Check if both training and validation MSE are high\n",
    "if train_mse > 100 and val_mse > 100:\n",
    "    print(\"Warning: The model may be underfitting!\")\n",
    "    print(\"Consider increasing model complexity by adding more estimators, reducing learning rate, or adjusting other hyperparameters.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BDT Cut Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataFrames[\"All\"][\"Ds_FakeD0BDT\"] = xgbm_final.predict_proba(DataFrames[\"All\"][Variables])[:,1]\n",
    "\n",
    "for s in GenEvents[0:]: # loop over samples\n",
    "    DataFrames[s][\"Ds_FakeD0BDT\"] = xgbm_final.predict_proba(DataFrames[s][Variables])[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_fom_curve(scores, labels, weights=None, n_thresholds=200):\n",
    "    \"\"\"\n",
    "    Compute FoM (S / sqrt(S + B)) across multiple BDT score thresholds.\n",
    "    \n",
    "    Parameters:\n",
    "        scores (np.array): BDT scores for the validation/test set\n",
    "        labels (np.array): True labels (1 for real D0, 0 for fake)\n",
    "        weights (np.array): Optional per-event weights\n",
    "        n_thresholds (int): Number of thresholds to scan (default=200)\n",
    "\n",
    "    Returns:\n",
    "        thresholds (np.array), foms (np.array), best_threshold (float), best_fom (float)\n",
    "    \"\"\"\n",
    "    thresholds = np.linspace(0, 1, n_thresholds)\n",
    "    foms = []\n",
    "\n",
    "    for t in thresholds:\n",
    "        mask = scores > t\n",
    "\n",
    "        if weights is not None:\n",
    "            S = np.sum(weights[(labels == 1) & mask])\n",
    "            B = np.sum(weights[(labels == 0) & mask])\n",
    "        else:\n",
    "            S = np.sum((labels == 1) & mask)\n",
    "            B = np.sum((labels == 0) & mask)\n",
    "\n",
    "        fom = S / np.sqrt(S + B) if (S + B) > 0 else 0\n",
    "        foms.append(fom)\n",
    "\n",
    "    foms = np.array(foms)\n",
    "    best_idx = np.argmax(foms)\n",
    "    return thresholds, foms, thresholds[best_idx], foms[best_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict scores from your trained model\n",
    "scores = xgbm_final.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Optionally define weights (or leave as None)\n",
    "weights = np.ones_like(y_test)  # or from your MC truth if applicable\n",
    "\n",
    "# Compute FoM curve\n",
    "thresholds, foms, best_thresh, best_fom = compute_fom_curve(scores, y_test)\n",
    "\n",
    "# Print results\n",
    "print(f\"Best threshold: {best_thresh:.3f}\")\n",
    "print(f\"Best FoM: {best_fom:.3f}\")\n",
    "\n",
    "# Plot it\n",
    "plt.plot(thresholds, foms)\n",
    "plt.axvline(best_thresh, color='red', linestyle='--', label=f'Best = {best_thresh:.3f}')\n",
    "plt.axvspan(0,best_thresh,color='gray',alpha=0.2)\n",
    "plt.xlabel(\"BDT Threshold\")\n",
    "plt.ylabel(\"FoM = S / √(S + B)\")\n",
    "plt.title(\"FoM Scan vs BDT Threshold\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from Functions import get_df_sig_bkg,get_nsig_nbkg,plot_var_sig_bkg,optimize_SB,plot_save\n",
    "# # \"Ds_extraInfo_BkgBDT\"\n",
    "# # \"Ds_BS\"\n",
    "# optimize_SB(df_sig=DataFrames[\"Signal\"][(DataFrames[\"Signal\"]['Ds_isSignal']==1)],\n",
    "#             df_bkg=DataFrames[\"All\"],\n",
    "#             var=\"Ds_FakeD0BDT\",\n",
    "#             Signal=DataFrames[\"Signal\"],\n",
    "#             Background=DataFrames[\"All\"],\n",
    "#             FoM=\"Ds_FakeD0BDT\",\n",
    "#             xlabel='Classifier Output',\n",
    "#             Bins=50,Range=[0,1],\n",
    "#             varmin=0,varmax=0.99,select='right',Width=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fake $D^0$ BDT Cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrames[\"All\"] = DataFrames[\"All\"][(DataFrames[\"All\"][\"Ds_FakeD0BDT\"]>=0.182)]\n",
    "\n",
    "# for s in GenEvents[0:]: # loop over samples\n",
    "#     DataFrames[s] = DataFrames[s][(DataFrames[s][\"Ds_FakeD0BDT\"]>=0.182)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background Suppression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataFrames[samples[0]].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Signal Number: \",len(DataFrames[samples[0]]))\n",
    "print(\"Background Number: \",len(DataFrames[samples[1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('default')\n",
    "plt.rcParams.update({\n",
    "    \"axes.labelsize\": 14,\n",
    "    \"xtick.labelsize\": 12,\n",
    "    \"ytick.labelsize\": 12,\n",
    "    \"legend.fontsize\": 12,\n",
    "    \"figure.titlesize\": 16\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Stacked = False\n",
    "Density = True\n",
    "Bins = 50\n",
    "Range = [0, 0.4]\n",
    "Op = -1\n",
    "dM = -1\n",
    "# i = \"Ds_gammaveto_M_Correction\"\n",
    "i = 'Ds_Ds_starminusDs_M_Correction'\n",
    "# i = 'Ds_extraInfo_FakeD0BDT'\n",
    "# i = 'Ds_chiProb'\n",
    "perBin = ((Range[1] - Range[0])/Bins)*1000\n",
    "# perBin = ((Range[1] - Range[0])/Bins)\n",
    "print(\"Width Per Bin: {width:.2f} MeV\".format(width = perBin))\n",
    "\n",
    "label1= r'$Signal$'\n",
    "label2= r'$Background$'\n",
    "\n",
    "labels=[label1,label2]\n",
    "colors=[\"#1f77b4\",\"#d62728\"]\n",
    "\n",
    "data = [\n",
    "        DataFrames[\"Signal\"][i], # (DataFrames[\"Signal\"]['Ds_charge']==-1) & \n",
    "        DataFrames[\"All\"][i]\n",
    "       ]\n",
    "\n",
    "\n",
    "plt.hist(data, color=colors, label=labels, alpha=1, range=Range, stacked=Stacked, density=Density, linewidth=2, bins=Bins, histtype='step')\n",
    "# plt.axvspan(Range[0],0.15,color='gray',alpha=0.2)\n",
    "# plt.axvline(0.58,ls='--',color='gray')\n",
    "\n",
    "# Title\n",
    "#---------\n",
    "# plt.title(r'$Signal: Charge(D_s^{+})=Positive$', loc = \"left\")\n",
    "# plt.title(r'$\\bf Signal\\;Events$', loc = \"right\")\n",
    "# plt.title(r'$\\int\\mathcal{L}dt\\approx\\;100$ fb$^{-1}$', loc = \"left\")\n",
    "# plt.title(r'$\\bf Generic\\;c\\bar{c}\\;Events$', loc = \"right\")\n",
    "# Label\n",
    "#---------\n",
    "plt.ylabel(r'$Entries/(\\; {width:.2f}\\;MeV/c^2)$'.format(width = perBin))\n",
    "# plt.ylabel(r'$Entries/(\\; {width:.2f}\\;)$'.format(width = perBin))\n",
    "# plt.xlabel(r'$m(e_{sig}^{+} e_{ROE}^{-})\\;[GeV/c^{2}]$')\n",
    "plt.xlabel(r'$\\Delta m(D_s^{*+} - D_{s}^{+})\\;[GeV/c^{2}]$')\n",
    "# plt.xlabel(r'$Fake \\; D^{0} \\; Suppression(D^{0})$')\n",
    "# plt.xlabel(r'$p-value_{IP}(D_{s}^{+})$')\n",
    "# plt.yscale(\"log\") \n",
    "# plt.xscale(\"log\") \n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Variables = [\n",
    "            # 'Ds_massDifference_0',\n",
    "             \"Ds_FakeD0BDT\",\n",
    "            #  'K_kaonID','K_abs_dr',\n",
    "            #  'pi_pionID','pi_abs_dr',\n",
    "            #  'D0_cos_decayAngle_1',\n",
    "            #  'D0_chiProb','D0_flightDistance','D0_useCMSFrame_p','D0_M'\n",
    "            #  'e_pt',\n",
    "            #  'Ds_chiProb_noIP',\n",
    "            'Ds_chiProb',\n",
    "            #  'Ds_KaonCount_3',\n",
    "            'Ds_gammaveto_M_Correction',\n",
    "            'Ds_Ds_starminusDs_M_Correction',\n",
    "            #  'e_pt',\n",
    "            #  'e_cos_theta',\n",
    "            #  'e_d0','e_z0'\n",
    "            # 'K_dr','pi_dr',\n",
    "            # 'K_kaonID','pi_pionID',\n",
    "            # 'D0_dM','D0_chiProb',\n",
    "            # 'D0_flightDistance',\n",
    "            # 'D0_useCMSFrame_p',\n",
    "            # 'D0_cos_decayAngle_1',\n",
    "            # #  'D0_daughterAngle_0_1',\n",
    "             ]\n",
    "\n",
    "features = [\n",
    "            # 'Ds_massDifference_0',\n",
    "            r'$Fake D^{0} Suppresion$',\n",
    "            # 'e_pt',\n",
    "            # r'$\\Delta m(D_s^{*+} - D_{s}^{+})$',\n",
    "            # r'$p-value(D_{s}^{+})$',\n",
    "            r'$p-value_{IP}(D_{s}^{+})$',\n",
    "            # r'$n_{K^{-}} - n_{K^{+}} + n_{K_{s}^{0}} (ROE)$',\n",
    "            r'$m(e_{sig}^{+} e_{ROE}^{-})$',\n",
    "            r'$\\Delta m(D_s^{*+} - D_{s}^{+})$',\n",
    "            # r'$p_{t} (e^{+})$',\n",
    "            # 'e_cos_theta',\n",
    "            # 'e_d0','e_z0',\n",
    "            # 'K_dr','pi_dr',\n",
    "            # 'K_kaonID','pi_pionID',\n",
    "            # 'D0_dM','D0_chiProb',\n",
    "            # 'D0_flightDistance',\n",
    "            # 'D0_useCMSFrame_p',\n",
    "            # 'D0_cos_decayAngle_1',\n",
    "            # #  'D0_daughterAngle_0_1',\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18, 15))\n",
    "\n",
    "heatmap = sns.heatmap(DataFrames[\"Signal\"][Variables].corr(), annot=True, cmap=\"coolwarm\",vmin=-1, vmax=1)\n",
    "\n",
    "heatmap.set_title('Signal Correlation Heatmap', fontdict={'fontsize':20}, pad=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18, 15))\n",
    "\n",
    "heatmap = sns.heatmap(DataFrames[\"All\"][Variables].corr(), annot=True, cmap=\"coolwarm\",vmin=-1, vmax=1)\n",
    "\n",
    "heatmap.set_title('Background Correlation Heatmap', fontdict={'fontsize':20}, pad=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Organise data ready for the machine learning model\n",
    "\n",
    "# for sklearn data are usually organised\n",
    "# into one 2D array of shape (n_samples x n_features)\n",
    "# containing all the data and one array of categories\n",
    "# of length n_samples\n",
    "\n",
    "all_MC = []  # define empty list that will contain all features for the MC\n",
    "for s in GenEvents:  # loop over the different samples\n",
    "    if s != \"data\":  # only MC should pass this\n",
    "        all_MC.append(\n",
    "            DataFrames[s][Variables]\n",
    "        )  # append the MC dataframe to the list containing all MC features\n",
    "X = np.concatenate(\n",
    "    all_MC\n",
    ")  # concatenate the list of MC dataframes into a single 2D array of features, called X\n",
    "\n",
    "all_y = (\n",
    "    []\n",
    ")  # define empty list that will contain labels whether an event in signal or background\n",
    "for s in GenEvents:  # loop over the different samples\n",
    "    if s != \"data\":  # only MC should pass this\n",
    "        if \"Signal\" in s:  # only signal MC should pass this\n",
    "            all_y.append(\n",
    "                np.ones(DataFrames[s].shape[0], dtype=np.int32)\n",
    "            )  # signal events are labelled with 1\n",
    "        else:  # only background MC should pass this\n",
    "            all_y.append(\n",
    "                np.zeros(DataFrames[s].shape[0], dtype=np.int32)\n",
    "            )  # background events are labelled 0\n",
    "y = np.concatenate(\n",
    "    all_y\n",
    ")  # concatenate the list of labels into a single 1D array of labels, called y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting with  Holdout method for eval_set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size=0.30,\n",
    "                                                    random_state=42,\n",
    "                                                    # stratify=y\n",
    "                                                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Compute the positive class weight\n",
    "# pos_class_weight = (len(y) - np.sum(y)) / np.sum(y)\n",
    "\n",
    "# Calculate class imbalance ratio\n",
    "neg, pos = np.bincount(y)  # 0s and 1s\n",
    "scale = neg / pos\n",
    "\n",
    "# Create EarlyStopping callback\n",
    "early_stop = xgboost.callback.EarlyStopping(\n",
    "    rounds=10,\n",
    "    metric_name='rmse',\n",
    "    data_name=\"validation_0\",\n",
    "    save_best=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_set = [(X_test, y_test)]\n",
    "xgbm_final = XGBClassifier(objective=\"binary:logistic\",\n",
    "                    eval_metric=\"logloss\",\n",
    "                    # early_stopping_rounds=10,\n",
    "                    scale_pos_weight=scale,\n",
    "                    max_delta_step=1,\n",
    "                    random_state=42,\n",
    "                    n_estimators=100)\n",
    "\n",
    "xgbm_final.fit(X_train, y_train, \n",
    "        eval_set=[(X_train, y_train),(X_test, y_test)], \n",
    "        # early_stopping_rounds=5,\n",
    "        verbose=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss curve of xgboost\n",
    "results = xgbm_final.evals_result()\n",
    "\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.plot(results[\"validation_0\"][\"logloss\"], label=\"Training loss\")\n",
    "plt.plot(results[\"validation_1\"][\"logloss\"], label=\"Validation loss\")\n",
    "plt.xlabel(\"Number of trees\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This optimization is pulling too much resources and ending the connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_dist = {\n",
    "#     \"learning_rate\": uniform(0.01, 0.2),        # e.g., 0.01 to 0.21\n",
    "#     \"max_depth\": randint(1, 5),                 # 1 to 4\n",
    "#     \"n_estimators\": randint(100, 201),          # 100 to 200\n",
    "#     \"reg_lambda\": randint(1, 5),\n",
    "#     \"gamma\": randint(0, 4),\n",
    "#     \"subsample\": uniform(0.5, 0.5),             # 0.5 to 1.0\n",
    "#     \"min_child_weight\": randint(1, 6),\n",
    "#     \"colsample_bytree\": uniform(0.3, 0.7)\n",
    "# }\n",
    "\n",
    "# random_search = RandomizedSearchCV(\n",
    "#     bdt,\n",
    "#     param_distributions=param_dist,\n",
    "#     n_iter=50,  # Try only 50 random combos (you can adjust)\n",
    "#     cv=5,\n",
    "#     n_jobs=-1,\n",
    "#     random_state=42,\n",
    "#     verbose=1\n",
    "# )\n",
    "\n",
    "# # After running RandomizedSearchCV:\n",
    "# random_search.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=0)\n",
    "\n",
    "# # Extract best parameters and apply them to the base model\n",
    "# xgbm_final = bdt.set_params(**random_search.best_params_, random_state=17).fit(X_train, y_train)\n",
    "\n",
    "# cv_results = cross_validate(xgbm_final, X, y, cv=10,\n",
    "#                             scoring=[\"f1\"],return_train_score=True)\n",
    "\n",
    "# print(cv_results['train_f1'].mean())\n",
    "# print(cv_results['test_f1'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Best parameters found by RandomizedSearchCV:\")\n",
    "# for param, value in random_search.best_params_.items():\n",
    "#     print(f\"{param:20s}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance scores\n",
    "print(xgbm_final.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_importance(model, features, num=len(X), save=False):\n",
    "    feature_imp = pd.DataFrame({'Value': model.feature_importances_, 'Feature': features})\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    sns.set(font_scale=1)\n",
    "    sns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\",\n",
    "                                                                     ascending=False)[0:num])\n",
    "    plt.title('Features')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    if save:\n",
    "        plt.savefig('importances.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_importance(xgbm_final, features)\n",
    "# When the feature importance graph is observed, \n",
    "# it is seen that the variables other than a02 and a01 are important for the xgboost model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "def get_pulls(counts,errors,pdf):\n",
    "    pull = (-pdf + counts) / errors\n",
    "    return pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_train_test(clf, X_train, y_train, X_test, y_test):\n",
    "    decisions = [] # list to hold decisions of classifier\n",
    "    for X,y in ((X_train, y_train), (X_test, y_test)): # train and test\n",
    "        if hasattr(clf, \"predict_proba\"): # if predict_proba function exists\n",
    "            d1 = clf.predict_proba(X[y<0.5])[:, 1] # background\n",
    "            d2 = clf.predict_proba(X[y>0.5])[:, 1] # signal\n",
    "        else: # predict_proba function doesn't exist\n",
    "            X_tensor = torch.as_tensor(X, dtype=torch.float) # make tensor from X_test_scaled\n",
    "            y_tensor = torch.as_tensor(y, dtype=torch.long) # make tensor from y_test\n",
    "            X_var, y_var = Variable(X_tensor), Variable(y_tensor) # make variables from tensors\n",
    "            d1 = clf(X_var[y_var<0.5])[1][:, 1].cpu().detach().numpy() # background\n",
    "            d2 = clf(X_var[y_var>0.5])[1][:, 1].cpu().detach().numpy() # signal\n",
    "        decisions += [d1, d2] # add to list of classifier decision\n",
    "\n",
    "    #pd.set_option('max_columns', None)\n",
    "#     %config InlineBackend.figure_format = 'retina'\n",
    "    # plt.style.use('belle2')\n",
    "    lw=3\n",
    "\n",
    "    fig,axs=plt.subplots(3,1,figsize=(10,10),gridspec_kw={'height_ratios':[1,0.2,0.2]})\n",
    "\n",
    "    bins = 50\n",
    "    bin_edges = np.linspace(0,1,bins)\n",
    "    \n",
    "    test_bkg_count_weight=bins/len(decisions[2])\n",
    "    test_sig_count_weight=bins/len(decisions[3])\n",
    "    test_bkg_counts,test_bkg_bins = np.histogram(decisions[2],bins=bins,range=(0,1))\n",
    "    test_sig_counts,test_sig_bins = np.histogram(decisions[3],bins=bins,range=(0,1))\n",
    "\n",
    "    train_bkg_counts,train_bkg_bins,_etc=axs[0].hist(decisions[0],color = 'tab:blue',\n",
    "            histtype='step',bins=bins,density=True,range=(0,1),linewidth=lw,label='Train Background')\n",
    "    train_sig_counts,train_sig_bins,_etc=axs[0].hist(decisions[1],color = 'tab:red',\n",
    "            histtype='step',bins=bins,density=True,range=(0,1),linewidth=lw,label=r'Train Signal')\n",
    "    axs[0].hist(decisions[0],color = 'tab:blue',\n",
    "            histtype='stepfilled',alpha=0.4,bins=bins,density=True,range=(0,1))\n",
    "    axs[0].hist(decisions[1],color = 'tab:red',\n",
    "            histtype='stepfilled',alpha=0.4,bins=bins,density=True,range=(0,1))\n",
    "    bin_width=test_bkg_bins[1]-test_bkg_bins[0]\n",
    "    bin_centers=[el+(bin_width/2) for el in test_bkg_bins[:-1]]\n",
    "\n",
    "    axs[0].errorbar(bin_centers,test_bkg_count_weight*test_bkg_counts,\n",
    "                yerr=test_bkg_count_weight*np.sqrt(test_bkg_counts),label='Test Background',color='tab:blue',\n",
    "                marker='o',linewidth=lw,ls='')\n",
    "    axs[0].errorbar(bin_centers,test_sig_count_weight*test_sig_counts,\n",
    "                yerr=test_sig_count_weight*np.sqrt(test_sig_counts),label='Test Signal',color='tab:red',\n",
    "                marker='o',linewidth=lw,ls='')\n",
    "    axs[0].set_title(r'$D_{s}^{+} \\rightarrow D^{0} e^{+} \\nu_{e}$',loc='left')\n",
    "    axs[0].set_xlim(0,1)\n",
    "    axs[0].set_ylim(0)\n",
    "    axs[0].set_ylabel('Event Density')\n",
    "\n",
    "    x= decisions[1]\n",
    "    y=  decisions[3]\n",
    "    ks_p_value_sig = ks_2samp(x, y)[1]\n",
    "\n",
    "    x= decisions[0]\n",
    "    y= decisions[2]\n",
    "    ks_p_value_bkg = ks_2samp(x, y)[1]\n",
    "\n",
    "    leg=axs[0].legend(loc='upper center',title=f\"Sig K-S test score: {ks_p_value_sig:0.3f}\"+\n",
    "                      \"\\n\"+f\"Bkg K-S test score: {ks_p_value_bkg:0.3f}\")\n",
    "    leg._legend_box.align = \"left\"  \n",
    "\n",
    "    pulls=get_pulls(test_bkg_count_weight*test_bkg_counts,test_bkg_count_weight*np.sqrt(test_bkg_counts),np.array(train_bkg_counts))\n",
    "    axs[1].bar(bin_centers,pulls,width=bin_width)\n",
    "    axs[1].set_xlim(0,1)\n",
    "    axs[1].set_ylabel('Pulls')\n",
    "    axs[1].set_ylim(-5,5)\n",
    "\n",
    "    pulls=get_pulls(test_sig_count_weight*test_sig_counts,test_sig_count_weight*np.sqrt(test_sig_counts),np.array(train_sig_counts))\n",
    "    axs[2].bar(bin_centers,pulls,width=bin_width,color='tab:red')\n",
    "    axs[2].set_xlim(0,1)\n",
    "    axs[2].set_ylabel('Pulls')\n",
    "    axs[2].set_ylim(-5,5)\n",
    "    axs[2].set_xlabel(r'BDT output')\n",
    "\n",
    "    return decisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decisions = compare_train_test(xgbm_final, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basf2 ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # compute ROC \n",
    "# sig_train=decisions[1]\n",
    "# sig_test=decisions[3]\n",
    "# bkg_train=decisions[0]\n",
    "# bkg_test=decisions[2]\n",
    "\n",
    "# bdt_cuts=np.linspace(0,1,100)\n",
    "# sig_efficiency_train=[]\n",
    "# bkg_rejection_train=[]\n",
    "# den_sig_train=len(sig_train)\n",
    "# den_bkg_train=len(bkg_train)\n",
    "\n",
    "# sig_efficiency_test=[]\n",
    "# bkg_rejection_test=[]\n",
    "# den_sig_test=len(sig_test)\n",
    "# den_bkg_test=len(bkg_test)\n",
    "\n",
    "\n",
    "# for cut in bdt_cuts:\n",
    "#     num_sig_train=len([el for el in sig_train if el>cut])\n",
    "#     num_bkg_train=len([el for el in bkg_train if el>cut])\n",
    "#     num_sig_test=len([el for el in sig_test if el>cut])\n",
    "#     num_bkg_test=len([el for el in bkg_test if el>cut])\n",
    "    \n",
    "#     sig_efficiency_test.append(num_sig_test/den_sig_test)\n",
    "#     bkg_rejection_test.append(1-(num_bkg_test/den_bkg_test))\n",
    "#     sig_efficiency_train.append(num_sig_train/den_sig_train)\n",
    "#     bkg_rejection_train.append(1-(num_bkg_train/den_bkg_train))\n",
    "\n",
    "# fig,axs=plt.subplots(1,1,figsize=(8,6))\n",
    "# lw=2\n",
    "# axs.plot([1, 0], [0, 1], color='grey', linestyle='--')\n",
    "# axs.plot(bkg_rejection_train,sig_efficiency_train,color='tab:blue',marker='',linewidth=lw,label='Train')\n",
    "# axs.plot(bkg_rejection_test,sig_efficiency_test,color='tab:red',marker='',linewidth=lw,ls='--',label='Test')\n",
    "# axs.set_title(r'$D_{s}^{+} \\rightarrow D^{0} e^{+} \\nu_{e}$',loc='left')\n",
    "\n",
    "# axs.set_ylim(0,1.05)\n",
    "# axs.set_xlim(0,1.05)\n",
    "# axs.legend(loc='lower left')\n",
    "# axs.set_xlabel('Background rejection')\n",
    "# axs.set_ylabel('Signal efficiency')\n",
    "# plt.tight_layout()\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "y_score_test = xgbm_final.predict_proba(X_test)[:, 1]\n",
    "fpr_test, tpr_test, thresholds_test = roc_curve(y_test, y_score_test)\n",
    "area_test = auc(fpr_test, tpr_test)\n",
    "\n",
    "y_score_train = xgbm_final.predict_proba(X_train)[:, 1]\n",
    "fpr_train, tpr_train, thresholds_train = roc_curve(y_train, y_score_train)\n",
    "area_train = auc(fpr_train, tpr_train)\n",
    "\n",
    "# Get classifier scores (probabilities for class 1)\n",
    "train_scores = xgbm_final.predict_proba(X_train)[:, 1]\n",
    "test_scores  = xgbm_final.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Use y_train and y_test to separate signal/background\n",
    "sig_train = train_scores[y_train == 1]\n",
    "bkg_train = train_scores[y_train == 0]\n",
    "sig_test  = test_scores[y_test == 1]\n",
    "bkg_test  = test_scores[y_test == 0]\n",
    "\n",
    "# Optionally, group them into one list like this:\n",
    "decisions = [bkg_train, sig_train, bkg_test, sig_test]\n",
    "\n",
    "bdt_cuts = np.linspace(0, 1, 100)\n",
    "\n",
    "sig_eff_train = []\n",
    "bkg_rej_train = []\n",
    "sig_eff_test = []\n",
    "bkg_rej_test = []\n",
    "fom_vals = []\n",
    "\n",
    "for cut in bdt_cuts:\n",
    "    num_sig_train = np.sum(sig_train > cut)\n",
    "    num_bkg_train = np.sum(bkg_train > cut)\n",
    "    num_sig_test = np.sum(sig_test > cut)\n",
    "    num_bkg_test = np.sum(bkg_test > cut)\n",
    "\n",
    "    # FoM calculation\n",
    "    fom = num_sig_test / np.sqrt(num_sig_test + num_bkg_test) if (num_sig_test + num_bkg_test) > 0 else 0\n",
    "    fom_vals.append(fom)\n",
    "\n",
    "    sig_eff_train.append(num_sig_train / len(sig_train))\n",
    "    bkg_rej_train.append(1 - (num_bkg_train / len(bkg_train)))\n",
    "    sig_eff_test.append(num_sig_test / len(sig_test))\n",
    "    bkg_rej_test.append(1 - (num_bkg_test / len(bkg_test)))\n",
    "\n",
    "# Find optimal FoM point\n",
    "fom_vals = np.array(fom_vals)\n",
    "best_idx = np.argmax(fom_vals)\n",
    "best_cut = bdt_cuts[best_idx]\n",
    "\n",
    "# Plot\n",
    "fig, axs = plt.subplots(1, 1, figsize=(7, 6))\n",
    "lw = 2\n",
    "\n",
    "# axs.plot([0, 1], [0, 1], color='grey', linestyle='--', label='Random')\n",
    "axs.plot(bkg_rej_train, sig_eff_train, color='tab:blue', linewidth=lw, label=f'Train (AUC = {area_train:.2f})')\n",
    "axs.plot(bkg_rej_test, sig_eff_test, color='tab:red', linestyle='--', linewidth=lw, label=f'Test (AUC = {area_test:.2f})')\n",
    "\n",
    "# ① Shade the overfit gap\n",
    "axs.fill_between(bkg_rej_test,\n",
    "                 sig_eff_train,\n",
    "                 sig_eff_test,\n",
    "                 where=(np.array(sig_eff_train) > np.array(sig_eff_test)),\n",
    "                 color='gray', alpha=0.2, label='Overfit Gap')\n",
    "\n",
    "# ② Mark the optimal cut point (from test curve)\n",
    "axs.axhline(sig_eff_test[best_idx], color='black', ls='--', linewidth=1.6,\n",
    "            label=f'Best FoM Cut = {best_cut:.3f}')\n",
    "axs.axvline(bkg_rej_test[best_idx], color='black', ls='--', linewidth=1.6)\n",
    "axs.scatter(bkg_rej_test[best_idx], sig_eff_test[best_idx], color='green', s=50)\n",
    "\n",
    "# Axis labels and formatting\n",
    "axs.set_title(r'$D_{s}^{+} \\rightarrow D^{0} e^{+} \\nu_{e}$', loc='left')\n",
    "axs.set_ylim(0, 1.05)\n",
    "axs.set_xlim(0, 1.05)\n",
    "axs.set_xlabel('Background rejection')\n",
    "axs.set_ylabel('Signal efficiency')\n",
    "axs.legend(loc='lower left')\n",
    "axs.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learing ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_score_test = xgbm_final.predict_proba(X_test)[:, 1]\n",
    "fpr_test, tpr_test, thresholds_test = roc_curve(y_test, y_score_test)\n",
    "area_test = auc(fpr_test, tpr_test)\n",
    "\n",
    "y_score_train = xgbm_final.predict_proba(X_train)[:, 1]\n",
    "fpr_train, tpr_train, thresholds_train = roc_curve(y_train, y_score_train)\n",
    "area_train = auc(fpr_train, tpr_train)\n",
    "\n",
    "plt.plot([0, 1], [0, 1], color='grey', linestyle='--')\n",
    "plt.plot(fpr_test, tpr_test, label=f'Test ROC curve (AUC = {area_test:.2f})')\n",
    "plt.plot(fpr_train, tpr_train, label=f'Train ROC curve (AUC = {area_train:.2f})')\n",
    "plt.xlim(0.0, 1.0)\n",
    "plt.ylim(0.0, 1.0)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend(loc='lower right')\n",
    "# We can make the plot look nicer by forcing the grid to be square\n",
    "plt.gca().set_aspect('equal', adjustable='box')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set\n",
    "y_pred_proba = xgbm_final.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate the ROC AUC score\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "print(f\"ROC AUC Score: {roc_auc:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if XGBoost Is Overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on training and validation sets\n",
    "train_preds = xgbm_final.predict(X_train)\n",
    "val_preds = xgbm_final.predict(X_test)\n",
    "\n",
    "# Calculate accuracy scores\n",
    "train_accuracy = accuracy_score(y_train, train_preds)\n",
    "val_accuracy = accuracy_score(y_test, val_preds)\n",
    "\n",
    "print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "# Check for large difference between train and validation accuracy\n",
    "if train_accuracy - val_accuracy > 0.1:\n",
    "    print(\"Warning: The model may be overfitting!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if XGBoost Is Underfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on training and validation sets\n",
    "train_preds = xgbm_final.predict(X_train)\n",
    "val_preds = xgbm_final.predict(X_test)\n",
    "\n",
    "# Calculate MSE for training and validation sets\n",
    "train_mse = mean_squared_error(y_train, train_preds)\n",
    "val_mse = mean_squared_error(y_test, val_preds)\n",
    "\n",
    "print(f\"Training MSE: {train_mse:.4f}\")\n",
    "print(f\"Validation MSE: {val_mse:.4f}\")\n",
    "\n",
    "# Check if both training and validation MSE are high\n",
    "if train_mse > 100 and val_mse > 100:\n",
    "    print(\"Warning: The model may be underfitting!\")\n",
    "    print(\"Consider increasing model complexity by adding more estimators, reducing learning rate, or adjusting other hyperparameters.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BDT Cut Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_fom_curve(scores, labels, weights=None, n_thresholds=200):\n",
    "    \"\"\"\n",
    "    Compute FoM (S / sqrt(S + B)) across multiple BDT score thresholds.\n",
    "    \n",
    "    Parameters:\n",
    "        scores (np.array): BDT scores for the validation/test set\n",
    "        labels (np.array): True labels (1 for real D0, 0 for fake)\n",
    "        weights (np.array): Optional per-event weights\n",
    "        n_thresholds (int): Number of thresholds to scan (default=200)\n",
    "\n",
    "    Returns:\n",
    "        thresholds (np.array), foms (np.array), best_threshold (float), best_fom (float)\n",
    "    \"\"\"\n",
    "    thresholds = np.linspace(0, 1, n_thresholds)\n",
    "    foms = []\n",
    "\n",
    "    for t in thresholds:\n",
    "        mask = scores > t\n",
    "\n",
    "        if weights is not None:\n",
    "            S = np.sum(weights[(labels == 1) & mask])\n",
    "            B = np.sum(weights[(labels == 0) & mask])\n",
    "        else:\n",
    "            S = np.sum((labels == 1) & mask)\n",
    "            B = np.sum((labels == 0) & mask)\n",
    "\n",
    "        fom = S / np.sqrt(S + B) if (S + B) > 0 else 0\n",
    "        foms.append(fom)\n",
    "\n",
    "    foms = np.array(foms)\n",
    "    best_idx = np.argmax(foms)\n",
    "    return thresholds, foms, thresholds[best_idx], foms[best_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict scores from your trained model\n",
    "scores = xgbm_final.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Optionally define weights (or leave as None)\n",
    "weights = np.ones_like(y_test)  # or from your MC truth if applicable\n",
    "\n",
    "# Compute FoM curve\n",
    "thresholds, foms, best_thresh, best_fom = compute_fom_curve(scores, y_test)\n",
    "\n",
    "# Print results\n",
    "print(f\"Best threshold: {best_thresh:.3f}\")\n",
    "print(f\"Best FoM: {best_fom:.3f}\")\n",
    "\n",
    "# Plot it\n",
    "plt.plot(thresholds, foms)\n",
    "plt.axvline(best_thresh, color='red', linestyle='--', label=f'Best = {best_thresh:.3f}')\n",
    "plt.axvspan(0,best_thresh,color='gray',alpha=0.2)\n",
    "plt.xlabel(\"BDT Threshold\")\n",
    "plt.ylabel(\"FoM = S / √(S + B)\")\n",
    "plt.title(\"FoM Scan vs BDT Threshold\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataFrames[\"Signal\"][\"Ds_BkgBDT\"] = xgbm_final.predict_proba(DataFrames[\"Signal\"][Variables])[:,1]\n",
    "DataFrames[\"All\"][\"Ds_BkgBDT\"] = xgbm_final.predict_proba(DataFrames[\"All\"][Variables])[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from Functions import get_df_sig_bkg,get_nsig_nbkg,plot_var_sig_bkg,optimize_SB,plot_save\n",
    "\n",
    "# optimize_SB(df_sig=DataFrames[\"Signal\"][(abs(DataFrames[\"Signal\"]['Ds_isSignal'])==1)],\n",
    "#             df_bkg=DataFrames[\"All\"],\n",
    "#             var=\"Ds_extraInfo_BkgBDT\",\n",
    "#             Signal=DataFrames[\"Signal\"][(abs(DataFrames[\"Signal\"]['Ds_isSignal'])==1)],\n",
    "#             Background=DataFrames[\"All\"],\n",
    "#             FoM=\"Ds_extraInfo_BkgBDT\",\n",
    "#             xlabel='Classifier Output',\n",
    "#             Bins=50,Range=[0,1],\n",
    "#             varmin=0,varmax=0.93,select='right',Width=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('default')\n",
    "plt.rcParams.update({\n",
    "    \"axes.labelsize\": 14,\n",
    "    \"xtick.labelsize\": 12,\n",
    "    \"ytick.labelsize\": 12,\n",
    "    \"legend.fontsize\": 12,\n",
    "    \"figure.titlesize\": 16\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(DataFrames[\"All\"].query('Ds_BkgBDT>=0.657')[['D0_isSignal']].value_counts(normalize=False,dropna=False).apply(lambda x: f\"{x:.6f}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Stacked = False\n",
    "Density = False\n",
    "Bins = 50\n",
    "var = 'D0_dM'\n",
    "Range = [-0.02, 0.02]\n",
    "BD = 0.657\n",
    "perBin = ((Range[1] - Range[0])/Bins)*1000\n",
    "print(\"Width Per Bin: {width:.2f} MeV\".format(width = perBin))\n",
    "\n",
    "# label1= r'$Other$'\n",
    "label1= r'$Other \\; (FDS \\; BDT \\geq 0.8)$'\n",
    "\n",
    "labels=[label1]\n",
    "colors = ['C5']\n",
    "data=[\n",
    "      DataFrames[\"All\"][((abs(DataFrames[\"All\"]['Ds_D0_Other'])==1) | ((abs(DataFrames[\"All\"]['D0_mcPDG'])==421) & (abs(DataFrames[\"All\"]['D0_isSignal'])==0))) & (DataFrames[\"All\"][\"Ds_FakeD0BDT\"]>=BD)][var],\n",
    "      ]\n",
    "\n",
    "plt.hist(data, color=colors, label=labels, density=Density, stacked=Stacked, bins=Bins, alpha=1, histtype='step', linewidth=1.5, range=Range)\n",
    "# plt.axvspan(0.02,0.04,color='gray',alpha=0.2)\n",
    "# plt.axvline(0.02,ls='--',color='gray')\n",
    "# plt.axvline(0.04,ls='--',color='gray')\n",
    "\n",
    "# Title\n",
    "#--------\n",
    "plt.title(r'$\\bf Generic \\; Events$', loc = \"left\")\n",
    "plt.title(r'$\\int\\mathcal{L}dt\\approx\\;200$ fb$^{-1}$', loc = \"right\")\n",
    "# Label\n",
    "#-------\n",
    "plt.ylabel(r'$Entries/(\\; {width:.2f}\\;MeV/c^2)$'.format(width = perBin))\n",
    "plt.xlabel(r'$m(D^{0}) - m_{PDG}(D^{0}) \\;[GeV/c^{2}]$')\n",
    "# plt.yscale(\"log\")\n",
    "# plt.xscale(\"log\")\n",
    "# plt.ylim(0, 500)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suggested Background Break-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stacked = False\n",
    "# Density = False\n",
    "# Bins = 50\n",
    "# Range = [0.1, 0.6]\n",
    "# Op = 0.721\n",
    "# dM = -1\n",
    "# # i = 'e_cos_theta'\n",
    "# # i = 'Ds_vpho_CMS_daughterAngle'\n",
    "# i = 'Ds_diff_D0pi'\n",
    "# # i = 'Ds_chiProb_noIP'\n",
    "# # i = 'Ds_chiProb'\n",
    "# # i = 'Ds_extraInfo_FastBDT'\n",
    "# # i = 'D0_chiProb'\n",
    "# # i = 'Ds_Ds_starminusDs_M_Correction'\n",
    "# # i = \"Ds_gammaveto_M_Correction\"\n",
    "# # i = 'D0_chiProb'\n",
    "# # i = \"Ds_L_diff\"\n",
    "# # var = 'e_cos_theta'\n",
    "# # i = 'e_pt'\n",
    "# perBin = ((Range[1] - Range[0])/Bins)*1000\n",
    "# # perBin = ((Range[1] - Range[0])/Bins)\n",
    "# print(\"Width Per Bin: {width:.2f} MeV\".format(width = perBin))\n",
    "\n",
    "# label1= r'$D^{*+} \\rightarrow D^{0} \\pi^{+}$'\n",
    "# label3= r'$D^{0}$'\n",
    "# label4= r'$Other$'\n",
    "\n",
    "# labels1=[label1,label3,label4]\n",
    "# colors1=['C1','C2','C3']\n",
    "# data1=[\n",
    "#       DataFrames[\"ccbar\"][(abs(DataFrames[\"ccbar\"]['Ds_D0_Dstarplus'])==1) & (DataFrames[\"ccbar\"]['Ds_gammaveto_M_Correction']>=dM) & (DataFrames[\"ccbar\"][\"Ds_BkgBDT\"]>=Op)][i],\n",
    "#       DataFrames[\"ccbar\"][(abs(DataFrames[\"ccbar\"]['Ds_D0_NoDstarplusDstar0'])==1) & (DataFrames[\"ccbar\"]['Ds_gammaveto_M_Correction']>=dM) & (DataFrames[\"ccbar\"][\"Ds_BkgBDT\"]>=Op)][i],\n",
    "#       DataFrames[\"ccbar\"][(abs(DataFrames[\"ccbar\"]['Ds_D0_Other'])==1) & (DataFrames[\"ccbar\"]['Ds_gammaveto_M_Correction']>=dM) & (DataFrames[\"ccbar\"][\"Ds_BkgBDT\"]>=Op)][i],\n",
    "#       ]\n",
    "# labels2=[r'$D^{*0} \\; (Comb.)$',r'$D^{*0} \\; (Peak)$']\n",
    "# colors2=['C4','C5']\n",
    "# data2=[\n",
    "#       DataFrames[\"ccbar\"][(abs(DataFrames[\"ccbar\"]['Ds_mcPDG'])!=423) & (abs(DataFrames[\"ccbar\"]['Ds_D0_Dstar0'])==1) & (DataFrames[\"ccbar\"][\"Ds_BkgBDT\"]>=Op)][i],\n",
    "#       DataFrames[\"ccbar\"][(abs(DataFrames[\"ccbar\"]['Ds_mcPDG'])==423) & (abs(DataFrames[\"ccbar\"]['Ds_D0_Dstar0'])==1) & (DataFrames[\"ccbar\"][\"Ds_BkgBDT\"]>=Op)][i],\n",
    "#       ]\n",
    "\n",
    "# # factor = 0.1\n",
    "# # plt.hist(DataFrames[\"Signal\"][(DataFrames[\"Signal\"]['Ds_gammaveto_M_Correction']>=dM) & (DataFrames[\"Signal\"][\"Ds_BS\"]>=Op)][i], label=\"Signal\", histtype='step', density=Density, bins=Bins, alpha=1, range=Range, weights=factor*np.ones_like(DataFrames[\"Signal\"][(DataFrames[\"Signal\"]['Ds_gammaveto_M_Correction']>=dM) & (DataFrames[\"Signal\"][\"Ds_BS\"]>=Op)][i]), ls='--', linewidth=1.5)\n",
    "# plt.hist(data1, color=colors1, label=labels1, density=Density, stacked=Stacked, bins=Bins, alpha=1, histtype='step', linewidth=1.5, range=Range)\n",
    "# plt.hist(data2, color=colors2, label=labels2, density=Density, stacked=False, bins=Bins, alpha=1, histtype='step', linewidth=1.5, range=Range)\n",
    "# # plt.axvspan(Range[0],0.15,color='gray',alpha=0.2)\n",
    "# # plt.axvline(0.58,ls='--',color='gray')\n",
    "\n",
    "# # Title\n",
    "# #--------\n",
    "# plt.title(r'$BDT \\; \\geq 0.721$', loc = \"left\")\n",
    "# # Label\n",
    "# #-------\n",
    "# # plt.ylabel(r'$Entries/(\\; {width:.2f}\\;)$'.format(width = perBin))\n",
    "# # plt.ylabel(r'$Entries/(\\; {width:.2f}\\;MeV/c)$'.format(width = perBin))\n",
    "# plt.ylabel(r'$Entries/(\\; {width:.2f}\\;MeV/c^2)$'.format(width = perBin))\n",
    "# # plt.xlabel(r'$p_{t} (e^{+}) [GeV/c]$')\n",
    "# # plt.xlabel(r'$\\Delta \\theta(D_s^{+} \\; K^{+/-}/K_{s}^{0}) \\; [rad]$')\n",
    "# # plt.xlabel(r'$cos\\theta \\; (e^{+})$')\n",
    "# # plt.xlabel(r'$p-value(D^{0})$')\n",
    "# # plt.xlabel(r'$p-value(D_{s}^{+})$')\n",
    "# # plt.xlabel(r'$p-value_{IP}(D_{s}^{+})$')\n",
    "# # plt.xlabel(r'$Fake D^{0} Suppression(D^{0})$')\n",
    "# # plt.xlabel(r'$m(e_{sig}^{+} e_{ROE}^{-})\\;[GeV/c^{2}]$')\n",
    "# # plt.xlabel(r'$p_{t} \\; (e^{+})\\;[GeV/c]$')\n",
    "# plt.xlabel(r'$\\Delta m(D_s^{+} - D^{0})\\;[GeV/c^{2}]$')\n",
    "# # plt.xlabel(r'$\\Delta m(D_s^{*+} - D_{s}^{+})\\;[GeV/c^{2}]$')\n",
    "# # plt.xlabel(r'$cos\\theta \\; (e^{+})$')\n",
    "# # plt.xlabel(r'$p-value(D^{0})$')\n",
    "# # plt.xlabel(r'$\\mid \\vec{x}_{D_{s}^{+}} - \\vec{x}_{D^{0}} \\mid \\; [cm]$')\n",
    "# # plt.xlabel(r'$dz \\; (e^{+}) \\; [cm]$')\n",
    "# # plt.yscale(\"log\")\n",
    "# # plt.xscale(\"log\")\n",
    "# # plt.ylim(0, 500)\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Stacked = True\n",
    "Density = False\n",
    "Bins = 50\n",
    "# var = 'Ds_diff_D0pi'\n",
    "var = 'Ds_massDifference_0'\n",
    "Range = [0.0, 0.25]\n",
    "BS = 0.566\n",
    "Samples = \"All\"\n",
    "perBin = ((Range[1] - Range[0])/Bins)*1000\n",
    "print(\"Width Per Bin: {width:.2f} MeV\".format(width = perBin))\n",
    "\n",
    "label1= r'$Other$'\n",
    "label2= r'$Prompt \\; D^{0}$'\n",
    "label3= r'$D^{*0} \\rightarrow D^{0} X$'\n",
    "label4= r'$D^{*+} \\rightarrow D^{0} X$'\n",
    "\n",
    "labels=[label1,label2,label3,label4]\n",
    "colors=['C5','C4','C1','C2',]\n",
    "data=[\n",
    "      DataFrames[Samples][((abs(DataFrames[Samples]['Ds_D0_Other'])==1) | ((abs(DataFrames[Samples]['D0_mcPDG'])==421) & (abs(DataFrames[Samples]['D0_isSignal'])==0))) & (DataFrames[Samples][\"Ds_BkgBDT\"]>=BS)][var],\n",
    "      DataFrames[Samples][(abs(DataFrames[Samples]['Ds_D0_NoDstarplusDstar0'])==1) & (abs(DataFrames[Samples]['D0_isSignal'])==1) & (DataFrames[Samples][\"Ds_BkgBDT\"]>=BS)][var],\n",
    "      DataFrames[Samples][(abs(DataFrames[Samples]['Ds_D0_Dstar0'])==1) & (abs(DataFrames[Samples]['D0_isSignal'])==1) & (DataFrames[Samples][\"Ds_BkgBDT\"]>=BS)][var],\n",
    "      DataFrames[Samples][(abs(DataFrames[Samples]['Ds_D0_Dstarplus'])==1) & (abs(DataFrames[Samples]['D0_isSignal'])==1) & (DataFrames[Samples][\"Ds_BkgBDT\"]>=BS)][var],\n",
    "      ]\n",
    "\n",
    "# factor = 0.7\n",
    "# plt.hist(DataFrames[\"Signal\"][(DataFrames[\"Signal\"][\"Ds_BkgBDT\"]>=BS)][var], label=\"Signal\", histtype='step', density=Density, bins=Bins, alpha=1, range=Range, weights=factor*np.ones_like(DataFrames[\"Signal\"][(DataFrames[\"Signal\"][\"Ds_BkgBDT\"]>=BS)][var]), ls='--', linewidth=1.5)\n",
    "plt.hist(data, color=colors, label=labels, density=Density, stacked=Stacked, bins=Bins, alpha=1, histtype='step', linewidth=1.5, range=Range)\n",
    "# plt.axvspan(Range[0],0.16,color='gray',alpha=0.2)\n",
    "# plt.axvline(0.16,ls='--',color='gray')\n",
    "\n",
    "# Title\n",
    "#--------\n",
    "plt.title(r'$\\bf Generic \\; Events$', loc = \"left\")\n",
    "plt.title(r'$\\int\\mathcal{L}dt\\approx\\;1443.999$ fb$^{-1}$', loc = \"right\")\n",
    "# Label\n",
    "#-------\n",
    "plt.ylabel(r'$Entries/(\\; {width:.2f}\\;MeV/c^2)$'.format(width = perBin))\n",
    "plt.xlabel(r'$\\Delta m(D_s^{+} - D^{0})\\;[GeV/c^{2}]$')\n",
    "# plt.yscale(\"log\")\n",
    "# plt.xscale(\"log\")\n",
    "# plt.ylim(0, 30000)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Stacked = True\n",
    "Density = False\n",
    "Bins = 50\n",
    "# i = 'Ds_diff_D0pi'\n",
    "i = 'Ds_massDifference_0'\n",
    "Range = [0.0, 0.25]\n",
    "BS = 0.5\n",
    "Op = -1\n",
    "dM = -1\n",
    "Hits = -1\n",
    "perBin = ((Range[1] - Range[0])/Bins)*1000\n",
    "print(\"Width Per Bin: {width:.2f} MeV\".format(width = perBin))\n",
    "\n",
    "label1= r'$Comb.$'\n",
    "label2= r'$NaN$'\n",
    "label3= r'$D^{*0}$'\n",
    "label4= r'$D^{*+} \\rightarrow D^{0} \\pi^{+}$'\n",
    "\n",
    "labels=[label1,label2,label3,label4]\n",
    "colors=[\"#DD8452\",\"#C44E52\",\"#55A868\",\"#4C72B0\"]\n",
    "data=[\n",
    "      DataFrames[\"All\"][((abs(DataFrames[\"All\"][\"Ds_mcPDG\"])!=413) & (abs(DataFrames[\"All\"][\"Ds_mcPDG\"])!=423) & (~DataFrames[\"All\"][\"Ds_mcPDG\"].isna())) & (DataFrames[\"All\"]['Ds_gammaveto_M_Correction']>=dM) & (DataFrames[\"All\"][\"Ds_BkgBDT\"]>=BS)][i],\n",
    "      DataFrames[\"All\"][(DataFrames[\"All\"][\"Ds_mcPDG\"].isna()) & (DataFrames[\"All\"]['Ds_gammaveto_M_Correction']>=dM) & (DataFrames[\"All\"][\"Ds_BkgBDT\"]>=BS)][i],\n",
    "      DataFrames[\"All\"][(abs(DataFrames[\"All\"][\"Ds_mcPDG\"])==423) & (DataFrames[\"All\"]['Ds_gammaveto_M_Correction']>=dM) & (DataFrames[\"All\"][\"Ds_BkgBDT\"]>=BS)][i],\n",
    "      DataFrames[\"All\"][(abs(DataFrames[\"All\"][\"Ds_mcPDG\"])==413) & (DataFrames[\"All\"]['Ds_gammaveto_M_Correction']>=dM) & (DataFrames[\"All\"][\"Ds_BkgBDT\"]>=BS)][i]\n",
    "      ]\n",
    "\n",
    "# factor = 0.5\n",
    "# plt.hist(DataFrames[\"Signal\"][(DataFrames[\"Signal\"]['Ds_gammaveto_M_Correction']>=dM) & (DataFrames[\"Signal\"]['e_nPXDHits']>Hits)][i], label=\"Signal\", histtype='step', density=Density, bins=Bins, alpha=1, range=Range, weights=factor*np.ones_like(DataFrames[\"Signal\"][(DataFrames[\"Signal\"]['Ds_gammaveto_M_Correction']>=dM)][i]), ls='--', linewidth=1.5)\n",
    "plt.hist(data, color=colors, label=labels, density=Density, stacked=Stacked, bins=Bins, alpha=1, histtype='step', linewidth=1.5, range=Range)\n",
    "# plt.axvspan(Range[0],0.15,color='gray',alpha=0.2)\n",
    "# plt.axvline(0.15,ls='--',color='gray')\n",
    "\n",
    "# Title\n",
    "#--------\n",
    "plt.title(r'$e^{+}$ mass hypothesis: pion', loc = \"left\")\n",
    "plt.title(r'$\\int\\mathcal{L}dt\\approx\\;1443.999$ fb$^{-1}$', loc = \"right\")\n",
    "# Label\n",
    "#-------\n",
    "plt.ylabel(r'$Entries/(\\; {width:.2f}\\;MeV/c^2)$'.format(width = perBin))\n",
    "plt.xlabel(r'$\\Delta m(D_s^{+} - D^{0})\\;[GeV/c^{2}]$')\n",
    "# plt.yscale(\"log\")\n",
    "# plt.xscale(\"log\")\n",
    "# plt.ylim(0, 30000)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Bins=50\n",
    "Density = False\n",
    "Stacked = True\n",
    "Range = [0.1,0.55]\n",
    "BS = 0.566\n",
    "perBin = ((Range[1] - Range[0])/Bins)*1000\n",
    "var = 'Ds_diff_D0pi'\n",
    "# var = 'Ds_massDifference_0'\n",
    "print(\"Width Per Bin: {width:.2f} MeV\".format(width = perBin))\n",
    "\n",
    "label1= r'$isSignal(D_s^{+})=1$'\n",
    "label2= r'$isSignal(D_s^{+})=0$'\n",
    "label3= r'$NaN$'\n",
    "\n",
    "labels=[label1,label2,label3]\n",
    "colors=['#7eb0d5','#fd7f6f','purple']\n",
    "\n",
    "data = [DataFrames[\"Signal\"][(DataFrames[\"Signal\"]['Ds_isSignal']==1) & (DataFrames[\"Signal\"][\"Ds_BkgBDT\"]>=BS)][var],\n",
    "        DataFrames[\"Signal\"][(DataFrames[\"Signal\"]['Ds_isSignal']==0) & (DataFrames[\"Signal\"][\"Ds_BkgBDT\"]>=BS)][var],\n",
    "        DataFrames[\"Signal\"][(DataFrames[\"Signal\"]['Ds_isSignal'].isna()) & (DataFrames[\"Signal\"][\"Ds_BkgBDT\"]>=BS)][var]\n",
    "       ]\n",
    "\n",
    "\n",
    "plt.hist(data[::-1], color=colors[::-1], label=labels[::-1], alpha=1, range=Range, linewidth=1.5, stacked=Stacked, density=Density, bins=Bins, histtype='step')\n",
    "plt.axvspan(Range[0],0.16,color='gray',alpha=0.2)\n",
    "plt.axvline(0.16,ls='--',color='gray')\n",
    "\n",
    "# Title\n",
    "#---------\n",
    "# Signal\n",
    "plt.title(r'$2M\\;Events$', loc = \"left\")\n",
    "plt.title(r'$\\bf Signal\\;Events$', loc = \"right\")\n",
    "# # Background\n",
    "# plt.title(r'$\\int\\mathcal{L}dt\\approx\\;100$ fb$^{-1}$', loc = \"left\")\n",
    "# plt.title(r'$\\bf Generic\\;c\\bar{c}\\;Events$', loc = \"right\")\n",
    "# Label\n",
    "#---------\n",
    "plt.ylabel(r'$Entries/(\\; {width:.2f}\\;MeV/c^2)$'.format(width = perBin))\n",
    "plt.xlabel(r'$\\Delta m(D_s^{+} - D^{0})\\;[GeV/c^{2}]$')\n",
    "# plt.yscale(\"log\") \n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# === Settings ===\n",
    "Stacked = True\n",
    "Density = False\n",
    "Bins = 50\n",
    "Range = [0.0, 0.25]\n",
    "BDT = 0.6\n",
    "perBin = ((Range[1] - Range[0]) / Bins) * 1000\n",
    "print(\"Width Per Bin: {:.2f} MeV\".format(perBin))\n",
    "\n",
    "# Data source and variables\n",
    "df = DataFrames[\"All\"][(DataFrames[\"All\"][\"Ds_BkgBDT\"]>=BDT)]\n",
    "cut_var = \"Ds_diff_D0pi\"\n",
    "plot_var = 'Ds_massDifference_0'\n",
    "pdg_var = 'Ds_mcPDG'\n",
    "\n",
    "# Sideband cut (exclude D*⁺ peak)\n",
    "cut_low = 0.142\n",
    "cut_high = 0.15\n",
    "df_cut = df.query(f\"{cut_var} <= @cut_low or {cut_var} >= @cut_high\")\n",
    "\n",
    "# === Categories based on true Ds_mcPDG ===\n",
    "dstar_plus = df_cut[abs(df_cut[pdg_var]) == 413][plot_var]\n",
    "dstar_zero = df_cut[abs(df_cut[pdg_var]) == 423][plot_var]\n",
    "other = df_cut[(abs(df_cut[pdg_var]) != 413) & (abs(df_cut[pdg_var]) != 423)][plot_var]\n",
    "\n",
    "# === Plot ===\n",
    "plt.hist([other, dstar_zero, dstar_plus],\n",
    "         color=[\"#2E2E2E\", \"#4C6EB1\", \"#007C91\"],\n",
    "         label=[\"Other\", r\"$D^{*0}$\", r\"$D^{*+}$\"],\n",
    "         density=Density,\n",
    "         stacked=Stacked,\n",
    "         bins=Bins,\n",
    "         range=Range,\n",
    "         histtype='step',\n",
    "         linewidth=2)\n",
    "\n",
    "# Titles and labels\n",
    "plt.title(r'$D_s^{+} \\rightarrow [D^{0} \\rightarrow K^{-} \\pi^{+}] e^{+} \\nu_{e}$' + '\\n' +\n",
    "          r'$\\Delta m_{\\pi}(D_s^{+} - D^{0}) \\notin [0.142,\\; 0.15] \\; \\mathrm{GeV}/c^{2}$', loc=\"left\")\n",
    "plt.title(r'$\\int\\mathcal{L}dt\\approx\\;1444$ fb$^{-1}$', loc=\"right\")\n",
    "plt.xlabel(r'$\\Delta m_{e}(D_s^{+} - D^{0})\\;[GeV/c^{2}]$')\n",
    "plt.ylabel(r'$Entries/(\\;{:.2f}\\;MeV/c^2)$'.format(perBin))\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save BDT Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import uproot\n",
    "# import os\n",
    "\n",
    "# # Make sure the output directory exists\n",
    "# output_dir = \"/group/belle2/users2022/amubarak/02-Grid/ML_Trained/\"\n",
    "# os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# # Now save each DataFrame into its own ROOT file\n",
    "# for s in samples:\n",
    "#     out_path = os.path.join(output_dir, f\"TopoAna_{s}_withBDT.root\")\n",
    "#     with uproot.recreate(out_path) as f:\n",
    "#         f[\"Dstree\"] = DataFrames[s]  # Save the DataFrame into a tree named 'Dstree'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
