{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total RAM: 540.04 GB\n",
      "Available: 420.01 GB\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "\n",
    "mem = psutil.virtual_memory()\n",
    "print(f\"Total RAM: {mem.total / 1e9:.2f} GB\")\n",
    "print(f\"Available: {mem.available / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip uninstall -y scikit-learn\n",
    "# !pip install scikit-learn==1.3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install --upgrade pip\n",
    "# ! pip install --user xgboost seaborn\n",
    "# ! pip install --user bayesian-optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import mplhep\n",
    "import sys\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import uproot\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.datasets import make_classification,make_regression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import auc,roc_curve,confusion_matrix,classification_report,precision_recall_curve,mean_squared_error,accuracy_score,roc_auc_score\n",
    "from sklearn.model_selection import GridSearchCV, cross_validate, validation_curve,train_test_split,KFold,learning_curve,cross_val_score\n",
    "from sklearn.utils import compute_sample_weight\n",
    "from scipy.stats import ks_2samp\n",
    "\n",
    "import xgboost\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "from scipy.stats import randint, uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({\n",
    "    \"axes.labelsize\": 16,\n",
    "    \"xtick.labelsize\": 12,\n",
    "    \"ytick.labelsize\": 12,\n",
    "    \"legend.fontsize\": 14,\n",
    "    \"figure.titlesize\": 20\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 200000)\n",
    "pd.set_option('display.max_columns', 200000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"/home/belle2/amubarak/Ds2D0enue_Analysis/07-Python_Functions/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep-Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correct Charge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # In this notebook we only process the main signal and the generic events,\n",
    "# # for illustration purposes.\n",
    "# # You can add other backgrounds after if you wish.\n",
    "# samples = [\"Signal\",\"All\",\"BB\",\"ccbar\",\"ddbar\",\"ssbar\",\"taupair\",\"uubar\",\"uds\"]\n",
    "# GenEvents = [\"Signal\",\"BB\",\"ccbar\",\"ddbar\",\"ssbar\",\"taupair\",\"uubar\"]\n",
    "\n",
    "# DataFrames = {}  # define empty dictionary to hold dataframes\n",
    "\n",
    "# # Signal:\n",
    "# DataFrames[samples[0]] =  uproot.concatenate(\"/home/belle2/amubarak/C01-Simulated_Events/Ds2D0enu-Signal.root:Dstree\",library='pd')\n",
    "# # Background\n",
    "# for s in samples[1:]: # loop over samples\n",
    "#     DataFrames[s] =  uproot.concatenate(\"/group/belle2/users2022/amubarak/TopoAna/Completed_TopoAna/TopoAna_\"+ s +\".root:Dstree\",library='pd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading samples:  14%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                                                         | 1/7 [00:13<01:18, 13.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úîÔ∏è Loaded Signal with 219,917 entries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading samples:  29%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                                             | 2/7 [00:23<00:57, 11.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úîÔ∏è Loaded BB with 22,121 entries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading samples:  43%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                                 | 3/7 [03:48<06:39, 99.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úîÔ∏è Loaded ccbar with 1,802,245 entries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading samples:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                    | 4/7 [03:55<03:09, 63.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úîÔ∏è Loaded ddbar with 35,557 entries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading samples:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                        | 5/7 [04:04<01:27, 43.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úîÔ∏è Loaded ssbar with 69,533 entries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading samples:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã            | 6/7 [04:07<00:29, 29.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úîÔ∏è Loaded taupair with 2,707 entries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading samples: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [04:40<00:00, 40.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úîÔ∏è Loaded uubar with 158,960 entries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import uproot\n",
    "from tqdm import tqdm\n",
    "\n",
    "# === Load only selected branches ===\n",
    "with open(\"/home/belle2/amubarak/Ds2D0enue_Analysis/03-Grid/Save_var.txt\") as f:\n",
    "    variables_to_load = [\n",
    "        line.strip().strip(\",\").strip('\"').strip(\"'\")\n",
    "        for line in f\n",
    "        if line.strip() and not line.strip().startswith(\"#\")\n",
    "    ]\n",
    "\n",
    "samples = [\"Signal\", \"BB\", \"ccbar\", \"ddbar\", \"ssbar\", \"taupair\", \"uubar\"]\n",
    "GenEvents = [\"Signal\",\"BB\",\"ccbar\",\"ddbar\",\"ssbar\",\"taupair\",\"uubar\"]\n",
    "Date = \"0530\"\n",
    "Attempt = \"0\"\n",
    "\n",
    "DataFrames = {}\n",
    "\n",
    "# === Load each sample one by one with progress bar ===\n",
    "for name in tqdm(samples, desc=\"Loading samples\"):\n",
    "    if name == \"Signal\":\n",
    "        path = \"/home/belle2/amubarak/C01-Simulated_Events/Ds2D0enu-Signal.root:Dstree\"\n",
    "    else:\n",
    "        path = f\"/group/belle/users/amubarak/02-Grid/Sample_Grid/Ds2D0e-Generic_Ds_{Date}25_{Attempt}_{name}.root:Dstree\"\n",
    "\n",
    "    try:\n",
    "        df = uproot.concatenate(path, filter_name=variables_to_load, library='pd')\n",
    "        print(f\"‚úîÔ∏è Loaded {name} with {len(df):,} entries\")\n",
    "        DataFrames[name] = df\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to load {name}: {e}\")\n",
    "        DataFrames[name] = pd.DataFrame()\n",
    "\n",
    "# === Merge background categories ===\n",
    "background_samples = [\"BB\", \"ccbar\", \"ddbar\", \"ssbar\", \"taupair\", \"uubar\"]\n",
    "DataFrames[\"All\"] = pd.concat([DataFrames[s] for s in background_samples], ignore_index=True)\n",
    "\n",
    "# === Combine uds light-quark backgrounds ===\n",
    "DataFrames[\"uds\"] = pd.concat(\n",
    "    [DataFrames[\"uubar\"], DataFrames[\"ddbar\"], DataFrames[\"ssbar\"]],\n",
    "    ignore_index=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Incorrect Charge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import uproot\n",
    "from tqdm import tqdm\n",
    "\n",
    "# === Load only selected branches ===\n",
    "with open(\"/home/belle2/amubarak/Ds2D0enue_Analysis/03-Grid/Save_var.txt\") as f:\n",
    "    variables_to_load = [\n",
    "        line.strip().strip(\",\").strip('\"').strip(\"'\")\n",
    "        for line in f\n",
    "        if line.strip() and not line.strip().startswith(\"#\")\n",
    "    ]\n",
    "\n",
    "# === Configuration ===\n",
    "samples_WCh = [\n",
    "    \"Signal_WCh\", \"BB_WCh\", \"ccbar_WCh\", \"ddbar_WCh\",\n",
    "    \"ssbar_WCh\", \"taupair_WCh\", \"uubar_WCh\", \"Data_WCh\"\n",
    "]\n",
    "Date_WCh = \"0630\"\n",
    "Attempt_WCh = \"0\"\n",
    "\n",
    "# === Load one sample at a time ===\n",
    "for sample in tqdm(samples_WCh, desc=\"Loading WCh samples\"):\n",
    "    if sample == \"Signal_WCh\":\n",
    "        path = \"/home/belle2/amubarak/C01-Simulated_Events/Ds2D0enu-Signal_WCh.root:Dstree\"\n",
    "    else:\n",
    "        path = f\"/group/belle/users/amubarak/02-Grid/Sample_Grid_WCh/Ds2D0e-Generic_Ds_{Date_WCh}25_{Attempt_WCh}_{sample}.root:Dstree\"\n",
    "\n",
    "    try:\n",
    "        df = uproot.concatenate(path, filter_name=variables_to_load, library='pd')\n",
    "        DataFrames[sample] = df\n",
    "        print(f\"‚úîÔ∏è Loaded: {path} [{len(df):,} entries]\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed: {sample} ‚Äî {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reversed PID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import uproot\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# === Configuration ===\n",
    "Date_ReverseID = \"0626\"\n",
    "Attempt_ReverseID = \"0\"\n",
    "\n",
    "# === Prompt user for veto toggle ===\n",
    "apply_veto = input(\"Apply veto cut on Ds_diff_D0pi? (y/n): \").strip().lower() == \"y\"\n",
    "\n",
    "# === Define veto window ===\n",
    "cut_low = 0.14543 - (3*0.00041124)\n",
    "cut_high = 0.14543 + (3*0.00041124)\n",
    "\n",
    "# === Variables to load ===\n",
    "with open(\"/home/belle2/amubarak/Ds2D0enue_Analysis/03-Grid/Save_var.txt\") as f:\n",
    "    variables_to_load = [\n",
    "        line.strip().strip(\",\").strip('\"').strip(\"'\")\n",
    "        for line in f if line.strip() and not line.strip().startswith(\"#\")\n",
    "    ]\n",
    "\n",
    "if apply_veto and \"Ds_diff_D0pi\" not in variables_to_load:\n",
    "    variables_to_load.append(\"Ds_diff_D0pi\")\n",
    "\n",
    "# === Load merged background and signal samples ===\n",
    "merged_samples = {\n",
    "    \"Signal_ReverseID\": \"/home/belle2/amubarak/C01-Simulated_Events/Ds2D0enu-Signal_ReverseID.root\",\n",
    "    \"BB_ReverseID\": f\"/group/belle/users/amubarak/02-Grid/Sample_Grid_ReverseID/Ds2D0e-Generic_Ds_{Date_ReverseID}25_{Attempt_ReverseID}_BB_ReverseID.root\",\n",
    "    \"ddbar_ReverseID\": f\"/group/belle/users/amubarak/02-Grid/Sample_Grid_ReverseID/Ds2D0e-Generic_Ds_{Date_ReverseID}25_{Attempt_ReverseID}_ddbar_ReverseID.root\",\n",
    "    \"ssbar_ReverseID\": f\"/group/belle/users/amubarak/02-Grid/Sample_Grid_ReverseID/Ds2D0e-Generic_Ds_{Date_ReverseID}25_{Attempt_ReverseID}_ssbar_ReverseID.root\",\n",
    "    \"taupair_ReverseID\": f\"/group/belle/users/amubarak/02-Grid/Sample_Grid_ReverseID/Ds2D0e-Generic_Ds_{Date_ReverseID}25_{Attempt_ReverseID}_taupair_ReverseID.root\",\n",
    "    \"uubar_ReverseID\": f\"/group/belle/users/amubarak/02-Grid/Sample_Grid_ReverseID/Ds2D0e-Generic_Ds_{Date_ReverseID}25_{Attempt_ReverseID}_uubar_ReverseID.root\",\n",
    "    \"Data_ReverseID\": f\"/group/belle/users/amubarak/02-Grid/Sample_Grid_ReverseID/Ds2D0e-Generic_Ds_{Date_ReverseID}25_{Attempt_ReverseID}_Data_ReverseID.root\",\n",
    "}\n",
    "\n",
    "print(\"\\nüì¶ Loading merged samples...\")\n",
    "for sample, path in tqdm(merged_samples.items(), desc=\"Merged Samples\"):\n",
    "    try:\n",
    "        df = uproot.concatenate(f\"{path}:Dstree\", filter_name=variables_to_load, library=\"pd\")\n",
    "        if apply_veto:\n",
    "            df = df[(df[\"Ds_diff_D0pi\"] <= cut_low) | (df[\"Ds_diff_D0pi\"] >= cut_high)]\n",
    "        DataFrames[sample] = df\n",
    "        print(f\"‚úîÔ∏è {sample}: {len(df):,} entries\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to load {sample}: {e}\")\n",
    "\n",
    "# === Load ccbar_ReverseID chunks sequentially ===\n",
    "chunk_dir = f\"/group/belle/users/amubarak/02-Grid/Sample_Grid_ReverseID/Ds_{Date_ReverseID}25_{Attempt_ReverseID}_ccbar_ReverseID_Chunks\"\n",
    "chunk_paths = [os.path.join(chunk_dir, f\"ccbar_chunk_{i:02d}.root\") for i in range(40)]\n",
    "\n",
    "DataFrames[\"ccbar_ReverseID\"] = []\n",
    "\n",
    "print(\"\\nüß± Loading ccbar_ReverseID chunks one by one...\")\n",
    "for path in tqdm(chunk_paths, desc=\"ccbar Chunks\"):\n",
    "    try:\n",
    "        df = uproot.concatenate(f\"{path}:Dstree\", filter_name=variables_to_load, library=\"pd\")\n",
    "        if apply_veto:\n",
    "            df = df[(df[\"Ds_diff_D0pi\"] <= cut_low) | (df[\"Ds_diff_D0pi\"] >= cut_high)]\n",
    "        DataFrames[\"ccbar_ReverseID\"].append(df)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading {path}: {e}\")\n",
    "\n",
    "# === Concatenate all loaded DataFrames ===\n",
    "if DataFrames[\"ccbar_ReverseID\"]:\n",
    "    DataFrames[\"ccbar_ReverseID\"] = pd.concat(DataFrames[\"ccbar_ReverseID\"], ignore_index=True)\n",
    "    print(f\"‚úÖ ccbar_ReverseID: {len(DataFrames['ccbar_ReverseID']):,} entries\")\n",
    "else:\n",
    "    print(\"‚ùå ccbar_ReverseID failed to load any chunks.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reverse PID and Wrong Charge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import uproot\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# === Configuration ===\n",
    "Date_ReverseID = \"0708\"\n",
    "Attempt_ReverseID = \"0\"\n",
    "\n",
    "# === Prompt user for veto toggle ===\n",
    "apply_veto = input(\"Apply veto cut on Ds_diff_D0pi? (y/n): \").strip().lower() == \"y\"\n",
    "\n",
    "# === Define veto window ===\n",
    "cut_low = 0.14543 - (3*0.00041124)\n",
    "cut_high = 0.14543 + (3*0.00041124)\n",
    "\n",
    "# === Variables to load ===\n",
    "with open(\"/home/belle2/amubarak/Ds2D0enue_Analysis/03-Grid/Save_var.txt\") as f:\n",
    "    variables_to_load = [\n",
    "        line.strip().strip(\",\").strip('\"').strip(\"'\")\n",
    "        for line in f if line.strip() and not line.strip().startswith(\"#\")\n",
    "    ]\n",
    "\n",
    "if apply_veto and \"Ds_diff_D0pi\" not in variables_to_load:\n",
    "    variables_to_load.append(\"Ds_diff_D0pi\")\n",
    "\n",
    "# === Load merged background and signal samples ===\n",
    "merged_samples = {\n",
    "    \"BB_ReverseID_WCh\": f\"/group/belle/users/amubarak/02-Grid/Sample_Grid_ReverseID_WCh/Ds2D0e-Generic_Ds_{Date_ReverseID}25_{Attempt_ReverseID}_BB_ReverseID_WCh.root\",\n",
    "    \"ccbar_ReverseID_WCh\": f\"/group/belle/users/amubarak/02-Grid/Sample_Grid_ReverseID_WCh/Ds2D0e-Generic_Ds_{Date_ReverseID}25_{Attempt_ReverseID}_ccbar_ReverseID_WCh.root\",\n",
    "    \"ddbar_ReverseID_WCh\": f\"/group/belle/users/amubarak/02-Grid/Sample_Grid_ReverseID_WCh/Ds2D0e-Generic_Ds_{Date_ReverseID}25_{Attempt_ReverseID}_ddbar_ReverseID_WCh.root\",\n",
    "    \"ssbar_ReverseID_WCh\": f\"/group/belle/users/amubarak/02-Grid/Sample_Grid_ReverseID_WCh/Ds2D0e-Generic_Ds_{Date_ReverseID}25_{Attempt_ReverseID}_ssbar_ReverseID_WCh.root\",\n",
    "    \"taupair_ReverseID_WCh\": f\"/group/belle/users/amubarak/02-Grid/Sample_Grid_ReverseID_WCh/Ds2D0e-Generic_Ds_{Date_ReverseID}25_{Attempt_ReverseID}_taupair_ReverseID_WCh.root\",\n",
    "    \"uubar_ReverseID_WCh\": f\"/group/belle/users/amubarak/02-Grid/Sample_Grid_ReverseID_WCh/Ds2D0e-Generic_Ds_{Date_ReverseID}25_{Attempt_ReverseID}_uubar_ReverseID_WCh.root\",\n",
    "    \"Data_ReverseID_WCh\": f\"/group/belle/users/amubarak/02-Grid/Sample_Grid_ReverseID_WCh/Ds2D0e-Generic_Ds_{Date_ReverseID}25_{Attempt_ReverseID}_Data_ReverseID_WCh.root\",\n",
    "}\n",
    "\n",
    "print(\"\\nüì¶ Loading merged samples...\")\n",
    "for sample, path in tqdm(merged_samples.items(), desc=\"Merged Samples\"):\n",
    "    try:\n",
    "        df = uproot.concatenate(f\"{path}:Dstree\", filter_name=variables_to_load, library=\"pd\")\n",
    "        if apply_veto:\n",
    "            df = df[(df[\"Ds_diff_D0pi\"] <= cut_low) | (df[\"Ds_diff_D0pi\"] >= cut_high)]\n",
    "        DataFrames[sample] = df\n",
    "        print(f\"‚úîÔ∏è {sample}: {len(df):,} entries\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to load {sample}: {e}\")\n",
    "\n",
    "# # === Load ccbar_ReverseID_WCh chunks sequentially ===\n",
    "# chunk_dir = f\"/group/belle/users/amubarak/02-Grid/Sample_Grid_ReverseID_WCh/Ds_{Date_ReverseID}25_{Attempt_ReverseID}_ccbar_ReverseID_WCh_Chunks\"\n",
    "# chunk_paths = [os.path.join(chunk_dir, f\"ccbar_chunk_{i:02d}.root\") for i in range(40)]\n",
    "\n",
    "# DataFrames[\"ccbar_ReverseID_WCh\"] = []\n",
    "\n",
    "# print(\"\\nüß± Loading ccbar_ReverseID chunks one by one...\")\n",
    "# for path in tqdm(chunk_paths, desc=\"ccbar Chunks\"):\n",
    "#     try:\n",
    "#         df = uproot.concatenate(f\"{path}:Dstree\", filter_name=variables_to_load, library=\"pd\")\n",
    "#         if apply_veto:\n",
    "#             df = df[(df[\"Ds_diff_D0pi\"] <= cut_low) | (df[\"Ds_diff_D0pi\"] >= cut_high)]\n",
    "#         DataFrames[\"ccbar_ReverseID_WCh\"].append(df)\n",
    "#     except Exception as e:\n",
    "#         print(f\"‚ùå Error loading {path}: {e}\")\n",
    "\n",
    "# # === Concatenate all loaded DataFrames ===\n",
    "# if DataFrames[\"ccbar_ReverseID_WCh\"]:\n",
    "#     DataFrames[\"ccbar_ReverseID_WCh\"] = pd.concat(DataFrames[\"ccbar_ReverseID_WCh\"], ignore_index=True)\n",
    "#     print(f\"‚úÖ ccbar_ReverseID_WCh: {len(DataFrames['ccbar_ReverseID_WCh']):,} entries\")\n",
    "# else:\n",
    "#     print(\"‚ùå ccbar_ReverseID_WCh failed to load any chunks.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The line below is to look at the available variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['Signal', 'BB', 'ccbar', 'ddbar', 'ssbar', 'taupair', 'uubar', 'All', 'uds'])\n"
     ]
    }
   ],
   "source": [
    "print(DataFrames.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataFrames[\"All\"].columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "The code below will be used to apply cuts to the data.  \n",
    "The range of the plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Electron ID\n",
    "#-------------------\n",
    "# DataFrames[\"Signal\"] = DataFrames[\"Signal\"][DataFrames[\"Signal\"]['e_electronID']>=0.95]\n",
    "# DataFrames[\"ccbar\"] = DataFrames[\"ccbar\"][DataFrames[\"ccbar\"]['e_electronID']>=0.95]\n",
    "# DataFrames[\"Signal\"] = DataFrames[\"Signal\"][DataFrames[\"Signal\"]['Ds_gammaveto_em_electronID']>=0.95]\n",
    "# DataFrames[\"ccbar\"] = DataFrames[\"ccbar\"][DataFrames[\"ccbar\"]['Ds_gammaveto_em_electronID']>=0.95]\n",
    "\n",
    "# Photon Conversion\n",
    "#-------------------\n",
    "# DataFrames[samples[0]] = DataFrames[samples[0]][DataFrames[samples[0]]['Ds_gammaveto_M_Correction']>=0.1]\n",
    "# DataFrames[samples[1]] = DataFrames[samples[1]][DataFrames[samples[1]]['Ds_gammaveto_M_Correction']>=0.1]\n",
    "\n",
    "# Peaking Background Removal\n",
    "#----------------------------\n",
    "# DataFrames[\"ccbar\"] = DataFrames[\"ccbar\"][(DataFrames[\"ccbar\"]['Ds_diff_D0pi']>=0.15)]\n",
    "# DataFrames[\"Signal\"] = DataFrames[\"Signal\"][(DataFrames[\"Signal\"]['Ds_diff_D0pi']>=0.15)]\n",
    "\n",
    "# # Vertex Fitting\n",
    "# #----------------\n",
    "# DataFrames[\"Signal\"] = DataFrames[\"Signal\"][DataFrames[\"Signal\"]['Ds_chiProb']>=0.01]\n",
    "# DataFrames[\"ccbar\"] = DataFrames[\"ccbar\"][DataFrames[\"ccbar\"]['Ds_chiProb']>=0.01]\n",
    "\n",
    "# Dalitz Removal\n",
    "#----------------------------\n",
    "# DataFrames[\"ccbar\"] = DataFrames[\"ccbar\"][(DataFrames[\"ccbar\"]['Ds_pi0veto_M_Correction']<=0.08) | (DataFrames[\"ccbar\"]['Ds_pi0veto_M_Correction']>=0.16)]\n",
    "# DataFrames[\"Signal\"] = DataFrames[\"Signal\"][(DataFrames[\"Signal\"]['Ds_pi0veto_M_Correction']<=0.08) | (DataFrames[\"Signal\"]['Ds_pi0veto_M_Correction']>=0.16)]\n",
    "\n",
    "# Vertex Fit\n",
    "#----------------\n",
    "# DataFrames[samples[0]] = DataFrames[samples[0]][DataFrames[samples[0]]['Ds_chiProb_rank']==1]\n",
    "# DataFrames[samples[1]] = DataFrames[samples[1]][DataFrames[samples[1]]['Ds_chiProb_rank']==1]\n",
    "\n",
    "# D0 Invariant Mass\n",
    "#-----------------------\n",
    "# DataFrames[samples[0]] = DataFrames[samples[0]][(DataFrames[samples[0]]['Ds_D0_sideband']==1)]\n",
    "# DataFrames[samples[1]] = DataFrames[samples[1]][(DataFrames[samples[1]]['Ds_D0_sideband']==1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Photon Conversion Veto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in DataFrames.keys():\n",
    "    df = DataFrames[key]\n",
    "    if \"Ds_gammaveto_M_Correction\" in df.columns:\n",
    "        DataFrames[key] = df[\n",
    "            (df[\"Ds_gammaveto_M_Correction\"] >= 0.1)\n",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $D^{*+}$ Veto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut_low = 0.14541 - (3*0.00039706)\n",
    "cut_high = 0.14541 + (3*0.00042495)\n",
    "\n",
    "for key in DataFrames.keys():\n",
    "    df = DataFrames[key]\n",
    "    if \"Ds_diff_D0pi\" in df.columns:\n",
    "        DataFrames[key] = df[\n",
    "            (df[\"Ds_diff_D0pi\"] <= cut_low) | (df[\"Ds_diff_D0pi\"] >= cut_high)\n",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fake $D^0$ Suppression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # === Updated Variables ===\n",
    "# Variables = [\n",
    "#     'pi_dr',\n",
    "# #     'pi_dz',\n",
    "#     'K_dr',\n",
    "# #     'K_dz',\n",
    "#     'D0_dM',\n",
    "#     'D0_chiProb',\n",
    "#     'D0_flightDistance',\n",
    "#     'D0_flightTime',\n",
    "#     'D0_useCMSFrame_p',\n",
    "#     'D0_cos_decayAngle_1',\n",
    "# ]\n",
    "\n",
    "# features = [\n",
    "#     r'$dr(\\pi^{+})\\;[\\mathrm{cm}]$',\n",
    "# #     r'$dz(\\pi^{+})\\;[\\mathrm{cm}]$',\n",
    "#     r'$dr(K^{-})\\;[\\mathrm{cm}]$',\n",
    "# #     r'$dz(K^{-})\\;[\\mathrm{cm}]$',\n",
    "#     r'$m(D^{0}) - m_{\\mathrm{PDG}}(D^{0})\\;[\\mathrm{GeV}/c^{2}]$',\n",
    "#     r'p-value of $D^{0}$',\n",
    "#     r'$Flight\\;Distance(D^{0})\\;[\\mathrm{cm}]$',\n",
    "#     r'$Flight\\;Time(D^{0})\\;[\\mathrm{ns}]$',\n",
    "#     r'$p^{*}(D^{0})\\;[\\mathrm{GeV}/c]$',\n",
    "#     r'$\\cos\\theta^*_{\\mathrm{daughter}_1}$',\n",
    "# ]\n",
    "\n",
    "# ranges = {\n",
    "#     'pi_dr': [0, 0.1],\n",
    "# #     'pi_dz': [-0.5, 0.5],\n",
    "#     'K_dr': [0, 0.1],\n",
    "# #     'K_dz': [-0.5, 0.5],\n",
    "#     'D0_dM': [-0.02, 0.02],\n",
    "#     'D0_chiProb': [0, 1],\n",
    "#     'D0_flightDistance': [-0.4, 0.4],\n",
    "#     'D0_flightTime': [-0.005, 0.005],\n",
    "#     'D0_useCMSFrame_p': [2.5, 5.0],\n",
    "#     'D0_cos_decayAngle_1': [-1, 1],\n",
    "# }\n",
    "\n",
    "# bins = 50\n",
    "# density = True\n",
    "# samples = \"All\"\n",
    "\n",
    "# # === Fixed Color Scheme ===\n",
    "# colors = {\n",
    "#     'signal': '#007C91',   # Real signal\n",
    "#     'other': '#2E2E2E',    # Everything else\n",
    "# }\n",
    "\n",
    "# bg_labels = [\n",
    "#     r'$Other$',\n",
    "#     r'$D^{0}$',\n",
    "#     r'$D^{*0} \\rightarrow D^{0} \\; \\pi^{0} / \\gamma$',\n",
    "#     r'$D^{*+} \\rightarrow D^{0} \\; \\pi^{+}$'\n",
    "# ]\n",
    "\n",
    "# bg_masks = [\n",
    "#     lambda df: df['D0_isSignal'].isna() | (df['D0_isSignal'] == 0),\n",
    "#     lambda df: (df['Ds_D0_NoDstarplusDstar0'] == 1) & (df['D0_isSignal'] == 1),\n",
    "#     lambda df: (df['Ds_D0_Dstar0'] == 1) & (df['D0_isSignal'] == 1),\n",
    "#     lambda df: (df['Ds_D0_Dstarplus'] == 1) & (df['D0_isSignal'] == 1),\n",
    "# ]\n",
    "\n",
    "# # === Plotting ===\n",
    "# if \"All\" in DataFrames and \"Signal\" in DataFrames:\n",
    "#     for idx, (var, label) in enumerate(zip(Variables, features)):\n",
    "#         var_range = ranges[var]\n",
    "#         bin_width = (var_range[1] - var_range[0]) / bins\n",
    "\n",
    "#         real_signal_data = DataFrames[\"Signal\"][DataFrames[\"Signal\"]['Ds_isSignal'] == 1][var]\n",
    "\n",
    "#         for jdx, (mask, bg_label) in enumerate(zip(bg_masks, bg_labels)):\n",
    "#             bg_data = DataFrames[samples][mask(DataFrames[samples])][var]\n",
    "\n",
    "#             plt.hist(real_signal_data, label=\"Real Signal\", histtype='step', density=density,\n",
    "#                      bins=bins, alpha=1, range=var_range, linewidth=2, color=colors['signal'])\n",
    "\n",
    "#             plt.hist(bg_data, label=bg_label, histtype='step', density=density,\n",
    "#                      bins=bins, alpha=1, range=var_range, linewidth=2, color=colors['other'])\n",
    "\n",
    "#             plt.xlabel(label)\n",
    "#             if bin_width < 0.01:\n",
    "#                 exponent = int(np.floor(np.log10(bin_width)))\n",
    "#                 base = bin_width / (10**exponent)\n",
    "#                 ylabel = r'$Norm.\\;Entries/({:.2f} \\times 10^{{{}}})$'.format(base, exponent)\n",
    "#             else:\n",
    "#                 ylabel = r'$Norm.\\;Entries/({:.2f})$'.format(bin_width)\n",
    "\n",
    "#             plt.ylabel(ylabel)\n",
    "#             plt.legend(loc='upper right')\n",
    "# #             plt.title(f\"Real Signal vs {bg_label}\", fontsize=15)\n",
    "#             plt.show()\n",
    "\n",
    "#         # Real vs Fake Signal\n",
    "#         fake_signal = DataFrames[\"Signal\"][DataFrames[\"Signal\"]['Ds_isSignal'] == 0][var]\n",
    "\n",
    "#         plt.hist(real_signal_data, label=\"Real Signal\", histtype='step', density=density,\n",
    "#                  bins=bins, alpha=1, range=var_range, linewidth=2, color=colors['signal'])\n",
    "\n",
    "#         plt.hist(fake_signal, label=\"Fake Signal\", histtype='step', density=density,\n",
    "#                  bins=bins, alpha=1, range=var_range, linewidth=2, color=colors['other'])\n",
    "\n",
    "#         plt.xlabel(label)\n",
    "#         plt.ylabel(r'$Normalized\\;Entries/({:.2f})$'.format(bin_width))\n",
    "#         plt.legend(loc='upper right')\n",
    "#         plt.title(\"Real vs Fake Signal\", fontsize=15)\n",
    "#         plt.show()\n",
    "# else:\n",
    "#     print(\"DataFrames['All'] and DataFrames['Signal'] must be defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Input Variables for the BDT ===\n",
    "Variables = [\n",
    "    'K_dr',\n",
    "    'pi_dr',\n",
    "    'K_kaonID',\n",
    "    'pi_pionID',\n",
    "    'D0_dM',\n",
    "    'D0_chiProb',\n",
    "    'D0_flightDistance',\n",
    "    'D0_useCMSFrame_p',\n",
    "    'D0_cos_decayAngle_1',\n",
    "]\n",
    "\n",
    "features = [\n",
    "    r'$dr(K^{-})\\;[\\mathrm{cm}]$',\n",
    "    r'$dr(\\pi^{+})\\;[\\mathrm{cm}]$',\n",
    "    r'$kaonID(K^{-})$',  # unitless PID likelihood\n",
    "    r'$pionID(\\pi^{+})$',  # unitless PID likelihood\n",
    "    r'$m(D^{0}) - m_{PDG}(D^{0})\\;[\\mathrm{GeV}/c^{2}]$',\n",
    "    r'$p$-value$(D^{0})$',  # unitless\n",
    "    r'$Flight \\; Distance(D^{0})\\;[\\mathrm{cm}]$',\n",
    "    r'$p^{*} (D^{0})\\;[\\mathrm{GeV}/c]$',\n",
    "    r'$\\cos\\theta^*_{daughter_1}$',  # unitless angle cosine\n",
    "]\n",
    "\n",
    "# === Plot Ranges ===\n",
    "ranges = {\n",
    "    'K_dr': [0, 0.08],\n",
    "    'pi_dr': [0, 0.08],\n",
    "    'K_kaonID': [0.5, 1],\n",
    "    'pi_pionID': [0.2, 1],\n",
    "    'D0_dM': [-0.02, 0.02],\n",
    "    'D0_chiProb': [0, 1],\n",
    "    'D0_flightDistance': [-0.4, 0.4],\n",
    "    'D0_useCMSFrame_p': [2.5, 5.0],\n",
    "    'D0_cos_decayAngle_1': [-1, 1],\n",
    "}\n",
    "\n",
    "bins = 50\n",
    "density = True\n",
    "\n",
    "# === Scientific Colors ===\n",
    "colors = {\n",
    "    'signal': '#007C91',  # Blue for real signal\n",
    "    'fake': '#C44E52',    # Red for fake D0\n",
    "}\n",
    "\n",
    "# === Extract Samples ===\n",
    "df_signal = DataFrames[\"Signal\"]\n",
    "df_generic = DataFrames[\"All\"]\n",
    "\n",
    "df_true_signal = df_signal[\n",
    "    (df_signal[\"Ds_isSignal\"] == 1) & (df_signal[\"D0_isSignal\"] == 1)\n",
    "]\n",
    "df_fake_d0 = df_generic[\n",
    "    (df_generic[\"D0_isSignal\"] == 0) | (df_generic[\"D0_isSignal\"].isna())\n",
    "]\n",
    "\n",
    "# === Plotting ===\n",
    "for var, label in zip(Variables, features):\n",
    "    if var not in ranges:\n",
    "        print(f\"Skipping {var}: no range defined.\")\n",
    "        continue\n",
    "\n",
    "    var_range = ranges[var]\n",
    "    bin_width = (var_range[1] - var_range[0]) / bins\n",
    "\n",
    "    signal_data = df_true_signal[var].dropna()\n",
    "    fake_data = df_fake_d0[var].dropna()\n",
    "\n",
    "    plt.hist(signal_data, label=\"Real $D^0$ (Signal MC)\",\n",
    "             histtype='step', density=density,\n",
    "             bins=bins, range=var_range, linewidth=2, color=colors['signal'])\n",
    "\n",
    "    plt.hist(fake_data, label=\"Fake $D^0$ (Generic MC)\",\n",
    "             histtype='step', density=density,\n",
    "             bins=bins, range=var_range, linewidth=2, color=colors['fake'])\n",
    "\n",
    "    plt.xlabel(label)\n",
    "\n",
    "    # if bin_width < 0.01:\n",
    "    #     exponent = int(np.floor(np.log10(bin_width)))\n",
    "    #     base = bin_width / (10**exponent)\n",
    "    #     ylabel = r'$Norm.\\;Entries/({:.2f} \\times 10^{{{}}})$'.format(base, exponent)\n",
    "    # else:\n",
    "    ylabel = r'$Norm.\\;Entries/({:.3f})$'.format(bin_width)\n",
    "\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.legend(loc='upper right')\n",
    "#     plt.title(\"Real vs Fake $D^0$\", fontsize=15)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataFrames[\"All\"].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataFrames[\"All\"][\"D0_isSignal\"] = DataFrames[\"All\"][\"D0_isSignal\"].replace(np.nan, 0)\n",
    "\n",
    "for s in GenEvents[0:]: # loop over samples\n",
    "    DataFrames[s][\"D0_isSignal\"] = DataFrames[s][\"D0_isSignal\"].replace(np.nan, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataFrames[\"All\"][\"Ds_isSignal\"] = DataFrames[\"All\"][\"Ds_isSignal\"].replace(np.nan, 0)\n",
    "\n",
    "for s in GenEvents[0:]: # loop over samples\n",
    "    DataFrames[s][\"Ds_isSignal\"] = DataFrames[s][\"Ds_isSignal\"].replace(np.nan, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Variables = [\n",
    "             'K_dr',\n",
    "             'pi_dr',\n",
    "             'D0_significanceOfDistance',\n",
    "             'D0_chiProb',\n",
    "             'D0_flightDistance',\n",
    "             'D0_useCMSFrame_p',\n",
    "             'D0_cos_decayAngle_1',\n",
    "             ]\n",
    "\n",
    "features = [\n",
    "             r'$dr(K^{-})$',\n",
    "             r'$dr(\\pi^{+})$',\n",
    "             'D0_significanceOfDistance',\n",
    "             # r'$m(D^{0}) - m_{PDG}(D^{0})$',\n",
    "             r'$p-value(D^{0})$',\n",
    "             r'$Flight \\; Distance(D^{0})$',\n",
    "             r'$p^{*} (D^{0})$',\n",
    "             r'$\\cos\\theta^*_{daughter_1}$',\n",
    "             ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18, 15))\n",
    "\n",
    "heatmap = sns.heatmap(DataFrames[\"Signal\"][Variables].corr(), annot=True, cmap=\"coolwarm\",vmin=-1, vmax=1)\n",
    "\n",
    "heatmap.set_title('Signal Correlation Heatmap', fontdict={'fontsize':20}, pad=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18, 15))\n",
    "\n",
    "heatmap = sns.heatmap(DataFrames[\"All\"][Variables].corr(), annot=True, cmap=\"coolwarm\",vmin=-1, vmax=1)\n",
    "\n",
    "heatmap.set_title('Background Correlation Heatmap', fontdict={'fontsize':20}, pad=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define your features and labels from the 'All' dataset\n",
    "# X = DataFrames[\"All\"][Variables].to_numpy(dtype=np.float32)\n",
    "# y = DataFrames[\"All\"]['D0_isSignal'].to_numpy(dtype=np.int64)\n",
    "\n",
    "# # # Reference variable for decorrelation ‚Äî this is what uBoost will try to flatten for background\n",
    "# # ref_variable = DataFrames[\"All\"][\"D0_dM\"]\n",
    "\n",
    "# #splitting with  Holdout method for eval_set\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "#                                                     test_size=0.30,\n",
    "#                                                     random_state=42,\n",
    "#                                                     # stratify=y\n",
    "#                                                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Signal: Real D‚Å∞ from signal MC\n",
    "df_signal = DataFrames[\"Signal\"]\n",
    "real_signal_mask = (df_signal[\"Ds_isSignal\"] == 1) & (df_signal[\"D0_isSignal\"] == 1)\n",
    "df_true_signal = df_signal[real_signal_mask]\n",
    "\n",
    "# Background: Fake D‚Å∞ from generic MC\n",
    "df_generic = DataFrames[\"All\"]\n",
    "df_fake_d0 = df_generic[(df_generic[\"D0_isSignal\"] == 0) | (df_generic[\"D0_isSignal\"].isna())]\n",
    "\n",
    "# Combine\n",
    "df_train = pd.concat([df_true_signal, df_fake_d0], axis=0)\n",
    "\n",
    "# Labels\n",
    "labels = np.concatenate([\n",
    "    np.ones(len(df_true_signal), dtype=np.int64),   # Signal = 1\n",
    "    np.zeros(len(df_fake_d0), dtype=np.int64)       # Fake D‚Å∞ = 0\n",
    "])\n",
    "\n",
    "# Features\n",
    "X = df_train[Variables].to_numpy(dtype=np.float32)\n",
    "y = labels\n",
    "\n",
    "#splitting with  Holdout method for eval_set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size=0.30,\n",
    "                                                    random_state=42,\n",
    "                                                    # stratify=y\n",
    "                                                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = compute_sample_weight('balanced', y_train)\n",
    "\n",
    "# Create EarlyStopping callback\n",
    "early_stop = xgboost.callback.EarlyStopping(\n",
    "    rounds=10,\n",
    "    metric_name='rmse',\n",
    "    data_name=\"validation_0\",\n",
    "    save_best=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_set = [(X_test, y_test)]\n",
    "bdt = XGBClassifier(objective=\"binary:logistic\",\n",
    "                    eval_metric=\"logloss\",\n",
    "                    # early_stopping_rounds=10,\n",
    "                    # scale_pos_weight=pos_class_weight,\n",
    "                #     scale_pos_weight=scale,\n",
    "                    max_delta_step=1,\n",
    "                    random_state=42,\n",
    "                    n_estimators=100)\n",
    "\n",
    "bdt.fit(X_train, y_train, \n",
    "        eval_set=[(X_train, y_train),(X_test, y_test)], \n",
    "        sample_weight=weights,\n",
    "        verbose=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss curve of xgboost\n",
    "results = bdt.evals_result()\n",
    "\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.plot(results[\"validation_0\"][\"logloss\"], label=\"Training loss\")\n",
    "plt.plot(results[\"validation_1\"][\"logloss\"], label=\"Validation loss\")\n",
    "plt.xlabel(\"Number of trees\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Optimization\n",
    "This optimization is pulling too much resources and ending the connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dist = {\n",
    "    \"learning_rate\": uniform(0.01, 0.2),        # e.g., 0.01 to 0.21\n",
    "    \"max_depth\": randint(1, 5),                 # 1 to 4\n",
    "    \"n_estimators\": randint(100, 201),          # 100 to 200\n",
    "    \"reg_lambda\": randint(1, 5),\n",
    "    \"gamma\": randint(0, 4),\n",
    "    \"subsample\": uniform(0.5, 0.5),             # 0.5 to 1.0\n",
    "    \"min_child_weight\": randint(1, 6),\n",
    "    \"colsample_bytree\": uniform(0.3, 0.7)\n",
    "}\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    bdt,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=50,  # Try only 50 random combos (you can adjust)\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# After running RandomizedSearchCV:\n",
    "random_search.fit(X_train, y_train, eval_set=[(X_test, y_test)], sample_weight=weights, verbose=0)\n",
    "\n",
    "# Extract best parameters and apply them to the base model\n",
    "xgbm_final = bdt.set_params(**random_search.best_params_, random_state=17).fit(X_train, y_train, sample_weight=weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv_results = cross_validate(xgbm_final, X, y, cv=10,\n",
    "#                             scoring=[\"f1\"],return_train_score=True)\n",
    "\n",
    "# print(cv_results['train_f1'].mean())\n",
    "# print(cv_results['test_f1'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best parameters found by RandomizedSearchCV:\")\n",
    "for param, value in random_search.best_params_.items():\n",
    "    print(f\"{param:20s}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance scores\n",
    "print(xgbm_final.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_importance(model, features, num=len(X), save=False):\n",
    "    feature_imp = pd.DataFrame({'Value': model.feature_importances_, 'Feature': features})\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    sns.set(font_scale=1)\n",
    "    sns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\",\n",
    "                                                                     ascending=False)[0:num])\n",
    "    plt.title('Feature Importance', fontsize=20)\n",
    "    plt.xlabel('Importance Value', fontsize=16)\n",
    "    plt.ylabel('Features', fontsize=20)\n",
    "    # ax.tick_params(axis='both', labelsize=14)  # Tick labels\n",
    "    if save:\n",
    "        plt.savefig('importances.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_importance(xgbm_final, features)\n",
    "# When the feature importance graph is observed, \n",
    "# it is seen that the variables other than a02 and a01 are important for the xgboost model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "def get_pulls(counts,errors,pdf):\n",
    "    pull = (-pdf + counts) / errors\n",
    "    return pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_train_test(clf, X_train, y_train, X_test, y_test):\n",
    "    Density = True\n",
    "    decisions = [] # list to hold decisions of classifier\n",
    "    for X,y in ((X_train, y_train), (X_test, y_test)): # train and test\n",
    "        if hasattr(clf, \"predict_proba\"): # if predict_proba function exists\n",
    "            d1 = clf.predict_proba(X[y<0.5])[:, 1] # background\n",
    "            d2 = clf.predict_proba(X[y>0.5])[:, 1] # signal\n",
    "        else: # predict_proba function doesn't exist\n",
    "            X_tensor = torch.as_tensor(X, dtype=torch.float) # make tensor from X_test_scaled\n",
    "            y_tensor = torch.as_tensor(y, dtype=torch.long) # make tensor from y_test\n",
    "            X_var, y_var = Variable(X_tensor), Variable(y_tensor) # make variables from tensors\n",
    "            d1 = clf(X_var[y_var<0.5])[1][:, 1].cpu().detach().numpy() # background\n",
    "            d2 = clf(X_var[y_var>0.5])[1][:, 1].cpu().detach().numpy() # signal\n",
    "        decisions += [d1, d2] # add to list of classifier decision\n",
    "\n",
    "    #pd.set_option('max_columns', None)\n",
    "#     %config InlineBackend.figure_format = 'retina'\n",
    "    # plt.style.use('belle2')\n",
    "    lw=3\n",
    "\n",
    "    fig,axs=plt.subplots(3,1,figsize=(10,10),gridspec_kw={'height_ratios':[1,0.2,0.2]})\n",
    "\n",
    "    bins = 50\n",
    "    bin_edges = np.linspace(0,1,bins)\n",
    "    \n",
    "    test_bkg_count_weight=bins/len(decisions[2])\n",
    "    test_sig_count_weight=bins/len(decisions[3])\n",
    "    test_bkg_counts,test_bkg_bins = np.histogram(decisions[2],bins=bins,range=(0,1))\n",
    "    test_sig_counts,test_sig_bins = np.histogram(decisions[3],bins=bins,range=(0,1))\n",
    "\n",
    "    train_bkg_counts,train_bkg_bins,_etc=axs[0].hist(decisions[0],color = 'tab:blue',\n",
    "            histtype='step',bins=bins,density=Density,range=(0,1),linewidth=lw,label='Train Background')\n",
    "    train_sig_counts,train_sig_bins,_etc=axs[0].hist(decisions[1],color = 'tab:red',\n",
    "            histtype='step',bins=bins,density=Density,range=(0,1),linewidth=lw,label=r'Train Signal')\n",
    "    axs[0].hist(decisions[0],color = 'tab:blue',\n",
    "            histtype='stepfilled',alpha=0.4,bins=bins,density=Density,range=(0,1))\n",
    "    axs[0].hist(decisions[1],color = 'tab:red',\n",
    "            histtype='stepfilled',alpha=0.4,bins=bins,density=Density,range=(0,1))\n",
    "    bin_width=test_bkg_bins[1]-test_bkg_bins[0]\n",
    "    bin_centers=[el+(bin_width/2) for el in test_bkg_bins[:-1]]\n",
    "\n",
    "    axs[0].errorbar(bin_centers,test_bkg_count_weight*test_bkg_counts,\n",
    "                yerr=test_bkg_count_weight*np.sqrt(test_bkg_counts),label='Test Background',color='tab:blue',\n",
    "                marker='o',linewidth=lw,ls='')\n",
    "    axs[0].errorbar(bin_centers,test_sig_count_weight*test_sig_counts,\n",
    "                yerr=test_sig_count_weight*np.sqrt(test_sig_counts),label='Test Signal',color='tab:red',\n",
    "                marker='o',linewidth=lw,ls='')\n",
    "    axs[0].set_title(r'$D_{s}^{+} \\rightarrow D^{0} e^{+} \\nu_{e}$',loc='left')\n",
    "    axs[0].set_xlim(0,1)\n",
    "    axs[0].set_ylim(0)\n",
    "    axs[0].set_ylabel('Event Density')\n",
    "\n",
    "    x= decisions[1]\n",
    "    y=  decisions[3]\n",
    "    ks_p_value_sig = ks_2samp(x, y)[1]\n",
    "\n",
    "    x= decisions[0]\n",
    "    y= decisions[2]\n",
    "    ks_p_value_bkg = ks_2samp(x, y)[1]\n",
    "\n",
    "    leg=axs[0].legend(loc='upper center',title=f\"Sig K-S test score: {ks_p_value_sig:0.3f}\"+\n",
    "                      \"\\n\"+f\"Bkg K-S test score: {ks_p_value_bkg:0.3f}\")\n",
    "    leg._legend_box.align = \"left\"  \n",
    "\n",
    "    pulls=get_pulls(test_bkg_count_weight*test_bkg_counts,test_bkg_count_weight*np.sqrt(test_bkg_counts),np.array(train_bkg_counts))\n",
    "    axs[1].bar(bin_centers,pulls,width=bin_width)\n",
    "    axs[1].set_xlim(0,1)\n",
    "    axs[1].set_ylabel('Pulls')\n",
    "    axs[1].set_ylim(-5,5)\n",
    "\n",
    "    pulls=get_pulls(test_sig_count_weight*test_sig_counts,test_sig_count_weight*np.sqrt(test_sig_counts),np.array(train_sig_counts))\n",
    "    axs[2].bar(bin_centers,pulls,width=bin_width,color='tab:red')\n",
    "    axs[2].set_xlim(0,1)\n",
    "    axs[2].set_ylabel('Pulls')\n",
    "    axs[2].set_ylim(-5,5)\n",
    "    axs[2].set_xlabel(r'BDT output')\n",
    "\n",
    "    return decisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decisions = compare_train_test(xgbm_final, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basf2 ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_score_test = xgbm_final.predict_proba(X_test)[:, 1]\n",
    "fpr_test, tpr_test, thresholds_test = roc_curve(y_test, y_score_test)\n",
    "area_test = auc(fpr_test, tpr_test)\n",
    "\n",
    "y_score_train = xgbm_final.predict_proba(X_train)[:, 1]\n",
    "fpr_train, tpr_train, thresholds_train = roc_curve(y_train, y_score_train)\n",
    "area_train = auc(fpr_train, tpr_train)\n",
    "\n",
    "# Get classifier scores (probabilities for class 1)\n",
    "train_scores = xgbm_final.predict_proba(X_train)[:, 1]\n",
    "test_scores  = xgbm_final.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Use y_train and y_test to separate signal/background\n",
    "sig_train = train_scores[y_train == 1]\n",
    "bkg_train = train_scores[y_train == 0]\n",
    "sig_test  = test_scores[y_test == 1]\n",
    "bkg_test  = test_scores[y_test == 0]\n",
    "\n",
    "# Optionally, group them into one list like this:\n",
    "decisions = [bkg_train, sig_train, bkg_test, sig_test]\n",
    "\n",
    "bdt_cuts = np.linspace(0, 1, 100)\n",
    "\n",
    "sig_eff_train = []\n",
    "bkg_rej_train = []\n",
    "sig_eff_test = []\n",
    "bkg_rej_test = []\n",
    "fom_vals = []\n",
    "\n",
    "for cut in bdt_cuts:\n",
    "    num_sig_train = np.sum(sig_train > cut)\n",
    "    num_bkg_train = np.sum(bkg_train > cut)\n",
    "    num_sig_test = np.sum(sig_test > cut)\n",
    "    num_bkg_test = np.sum(bkg_test > cut)\n",
    "\n",
    "    # FoM calculation\n",
    "    fom = num_sig_test / np.sqrt(num_sig_test + num_bkg_test) if (num_sig_test + num_bkg_test) > 0 else 0\n",
    "    fom_vals.append(fom)\n",
    "\n",
    "    sig_eff_train.append(num_sig_train / len(sig_train))\n",
    "    bkg_rej_train.append(1 - (num_bkg_train / len(bkg_train)))\n",
    "    sig_eff_test.append(num_sig_test / len(sig_test))\n",
    "    bkg_rej_test.append(1 - (num_bkg_test / len(bkg_test)))\n",
    "\n",
    "# Find optimal FoM point\n",
    "fom_vals = np.array(fom_vals)\n",
    "best_idx = np.argmax(fom_vals)\n",
    "best_cut = bdt_cuts[best_idx]\n",
    "\n",
    "# Plot\n",
    "fig, axs = plt.subplots(1, 1, figsize=(7, 6))\n",
    "lw = 2\n",
    "\n",
    "# axs.plot([0, 1], [0, 1], color='grey', linestyle='--', label='Random')\n",
    "axs.plot(bkg_rej_train, sig_eff_train, color='tab:blue', linewidth=lw, label=f'Train (AUC = {area_train:.2f})')\n",
    "axs.plot(bkg_rej_test, sig_eff_test, color='tab:red', linestyle='--', linewidth=lw, label=f'Test (AUC = {area_test:.2f})')\n",
    "\n",
    "# ‚ë† Shade the overfit gap\n",
    "axs.fill_between(bkg_rej_test,\n",
    "                 sig_eff_train,\n",
    "                 sig_eff_test,\n",
    "                 where=(np.array(sig_eff_train) > np.array(sig_eff_test)),\n",
    "                 color='gray', alpha=0.2, label='Overfit Gap')\n",
    "\n",
    "# ‚ë° Mark the optimal cut point (from test curve)\n",
    "axs.axhline(sig_eff_test[best_idx], color='black', ls='--', linewidth=1.6)\n",
    "# axs.axhline(sig_eff_test[best_idx], color='black', ls='--', linewidth=1.6,\n",
    "#             label=f'Best FoM Cut = {best_cut:.3f}')\n",
    "axs.axvline(bkg_rej_test[best_idx], color='black', ls='--', linewidth=1.6)\n",
    "axs.scatter(bkg_rej_test[best_idx], sig_eff_test[best_idx], color='green', s=50)\n",
    "\n",
    "# Axis labels and formatting\n",
    "axs.set_title(r'$D_{s}^{+} \\rightarrow D^{0} e^{+} \\nu_{e}$', loc='left')\n",
    "axs.set_ylim(0, 1.05)\n",
    "axs.set_xlim(0, 1.05)\n",
    "axs.set_xlabel('Background rejection')\n",
    "axs.set_ylabel('Signal efficiency')\n",
    "axs.legend(loc='lower left')\n",
    "axs.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learing ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_score_test = xgbm_final.predict_proba(X_test)[:, 1]\n",
    "fpr_test, tpr_test, thresholds_test = roc_curve(y_test, y_score_test)\n",
    "area_test = auc(fpr_test, tpr_test)\n",
    "\n",
    "y_score_train = xgbm_final.predict_proba(X_train)[:, 1]\n",
    "fpr_train, tpr_train, thresholds_train = roc_curve(y_train, y_score_train)\n",
    "area_train = auc(fpr_train, tpr_train)\n",
    "\n",
    "plt.plot([0, 1], [0, 1], color='grey', linestyle='--')\n",
    "plt.plot(fpr_test, tpr_test, label=f'Test ROC curve (AUC = {area_test:.2f})')\n",
    "plt.plot(fpr_train, tpr_train, label=f'Train ROC curve (AUC = {area_train:.2f})')\n",
    "plt.xlim(0.0, 1.0)\n",
    "plt.ylim(0.0, 1.0)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend(loc='lower right')\n",
    "# We can make the plot look nicer by forcing the grid to be square\n",
    "plt.gca().set_aspect('equal', adjustable='box')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set\n",
    "y_pred_proba = xgbm_final.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate the ROC AUC score\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "print(f\"ROC AUC Score: {roc_auc:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on training and validation sets\n",
    "train_preds = xgbm_final.predict(X_train)\n",
    "val_preds = xgbm_final.predict(X_test)\n",
    "\n",
    "# Calculate accuracy scores\n",
    "train_accuracy = accuracy_score(y_train, train_preds)\n",
    "val_accuracy = accuracy_score(y_test, val_preds)\n",
    "\n",
    "print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "# Check for large difference between train and validation accuracy\n",
    "if train_accuracy - val_accuracy > 0.1:\n",
    "    print(\"Warning: The model may be overfitting!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if XGBoost Is Underfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on training and validation sets\n",
    "train_preds = xgbm_final.predict(X_train)\n",
    "val_preds = xgbm_final.predict(X_test)\n",
    "\n",
    "# Calculate MSE for training and validation sets\n",
    "train_mse = mean_squared_error(y_train, train_preds)\n",
    "val_mse = mean_squared_error(y_test, val_preds)\n",
    "\n",
    "print(f\"Training MSE: {train_mse:.4f}\")\n",
    "print(f\"Validation MSE: {val_mse:.4f}\")\n",
    "\n",
    "# Check if both training and validation MSE are high\n",
    "if train_mse > 100 and val_mse > 100:\n",
    "    print(\"Warning: The model may be underfitting!\")\n",
    "    print(\"Consider increasing model complexity by adding more estimators, reducing learning rate, or adjusting other hyperparameters.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BDT Cut Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply BDT to all DataFrames that contain the required Variables\n",
    "for key in DataFrames.keys():\n",
    "    df = DataFrames[key]\n",
    "    \n",
    "    # Check: make sure all input BDT variables exist in this DataFrame\n",
    "    if all(var in df.columns for var in Variables):\n",
    "        # Apply BDT and store the result\n",
    "        DataFrames[key][\"Ds_FakeD0BDT\"] = xgbm_final.predict_proba(df[Variables])[:, 1].astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_fom_curve(scores, labels, weights=None, n_thresholds=200):\n",
    "    \"\"\"\n",
    "    Compute FoM (S / sqrt(S + B)) across multiple BDT score thresholds.\n",
    "    \n",
    "    Parameters:\n",
    "        scores (np.array): BDT scores for the validation/test set\n",
    "        labels (np.array): True labels (1 for real D0, 0 for fake)\n",
    "        weights (np.array): Optional per-event weights\n",
    "        n_thresholds (int): Number of thresholds to scan (default=200)\n",
    "\n",
    "    Returns:\n",
    "        thresholds (np.array), foms (np.array), best_threshold (float), best_fom (float)\n",
    "    \"\"\"\n",
    "    thresholds = np.linspace(0, 1, n_thresholds)\n",
    "    foms = []\n",
    "\n",
    "    for t in thresholds:\n",
    "        mask = scores > t\n",
    "\n",
    "        if weights is not None:\n",
    "            S = np.sum(weights[(labels == 1) & mask])\n",
    "            B = np.sum(weights[(labels == 0) & mask])\n",
    "        else:\n",
    "            S = np.sum((labels == 1) & mask)\n",
    "            B = np.sum((labels == 0) & mask)\n",
    "\n",
    "        fom = S / np.sqrt(S + B) if (S + B) > 0 else 0\n",
    "        foms.append(fom)\n",
    "\n",
    "    foms = np.array(foms)\n",
    "    best_idx = np.argmax(foms)\n",
    "    return thresholds, foms, thresholds[best_idx], foms[best_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict scores from your trained model\n",
    "scores = xgbm_final.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Optionally define weights (or leave as None)\n",
    "weights = np.ones_like(y_test)  # or from your MC truth if applicable\n",
    "\n",
    "# Compute FoM curve\n",
    "thresholds, foms, best_thresh, best_fom = compute_fom_curve(scores, y_test)\n",
    "\n",
    "# Print results\n",
    "print(f\"Best threshold: {best_thresh:.3f}\")\n",
    "print(f\"Best FoM: {best_fom:.3f}\")\n",
    "\n",
    "# Plot it\n",
    "plt.plot(thresholds, foms)\n",
    "plt.axvline(best_thresh, color='red', linestyle='--', label=f'Best = {best_thresh:.3f}')\n",
    "plt.axvspan(0,best_thresh,color='gray',alpha=0.2)\n",
    "plt.xlabel(\"BDT Threshold\")\n",
    "plt.ylabel(\"FoM = S / ‚àö(S + B)\")\n",
    "plt.title(\"FoM Scan vs BDT Threshold\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Functions import optimize_cut, plot_save\n",
    "\n",
    "cut = optimize_cut(\n",
    "    df_sig=DataFrames[\"Signal\"],                  # DataFrame used for signal plotting\n",
    "    df_bkg=DataFrames[\"All\"],                     # DataFrame used for background plotting\n",
    "    Signal=DataFrames[\"Signal\"],                  # DataFrame used for signal FoM calculation\n",
    "    Background=DataFrames[\"All\"],                 # DataFrame used for background FoM calculation\n",
    "    var=\"Ds_FakeD0BDT\",                           # Variable to plot\n",
    "    FoM=\"Ds_FakeD0BDT\",                           # Variable to optimize over (can be same as var)\n",
    "    xlabel=\"Classifier Output\",                   # X-axis label\n",
    "    Bins=50,\n",
    "    Range=[0, 1],\n",
    "    varmin=0,\n",
    "    varmax=0.99,\n",
    "    select=\"right\",                               # \"right\" for >= cut, \"left\" for <= cut\n",
    "    Width=False,\n",
    "    query_signal=\"Ds_isSignal == 1\"               # Only consider true signal\n",
    ")\n",
    "\n",
    "print(f\"Best cut is: {cut:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fake $D^0$ BDT Cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrames[\"All\"] = DataFrames[\"All\"][(DataFrames[\"All\"][\"Ds_FakeD0BDT\"]>=0.556)]\n",
    "\n",
    "# for s in GenEvents[0:]: # loop over samples\n",
    "#     DataFrames[s] = DataFrames[s][(DataFrames[s][\"Ds_FakeD0BDT\"]>=0.556)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save BDT Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correct Charge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import uproot\n",
    "\n",
    "# # === Make sure samples is a list ===\n",
    "# samples = [\"Signal\", \"BB\", \"ccbar\", \"ddbar\", \"ssbar\", \"taupair\", \"uubar\"]\n",
    "\n",
    "# # === Output directory ===\n",
    "# output_dir = \"/group/belle/users/amubarak/03-ML/FakeD0/\"\n",
    "# os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# # === Base input path for original files ===\n",
    "# base_input_dir = \"/group/belle/users/amubarak/02-Grid/Sample_Grid\"\n",
    "# Date = \"0530\"\n",
    "# Attempt = \"0\"\n",
    "\n",
    "# # === Save each DataFrame using original filename with _withBDT suffix ===\n",
    "# for s in samples:\n",
    "#     if s not in DataFrames:\n",
    "#         print(f\"Warning: {s} not in DataFrames ‚Äî skipping.\")\n",
    "#         continue\n",
    "\n",
    "#     # Convert Fake D‚Å∞ BDT output to float32 if it exists\n",
    "#     if \"Ds_FakeD0BDT\" in DataFrames[s].columns:\n",
    "#         DataFrames[s][\"Ds_FakeD0BDT\"] = DataFrames[s][\"Ds_FakeD0BDT\"].astype(np.float32)\n",
    "\n",
    "#     # Set original file path\n",
    "#     if s == \"Signal\":\n",
    "#         original_name = \"Ds2D0enu-Signal.root\"\n",
    "#     else:\n",
    "#         original_name = f\"Ds2D0e-Generic_Ds_{Date}25_{Attempt}_{s}.root\"\n",
    "\n",
    "#     # Build output path with _withBDT suffix\n",
    "#     output_name = original_name.replace(\".root\", \"_withBDT.root\")\n",
    "#     out_path = os.path.join(output_dir, output_name)\n",
    "\n",
    "#     # Save DataFrame to ROOT\n",
    "#     with uproot.recreate(out_path) as f:\n",
    "#         f[\"Dstree\"] = DataFrames[s]\n",
    "\n",
    "#     print(f\"Saved: {out_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrong Charge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import uproot\n",
    "\n",
    "# # === Make sure wrong-charge samples list is defined ===\n",
    "# samples_WCh = [\"Signal_WCh\", \"BB_WCh\", \"ccbar_WCh\", \"ddbar_WCh\", \"ssbar_WCh\", \"taupair_WCh\", \"uubar_WCh\", \"Data_WCh\"]\n",
    "\n",
    "# # === Output directory for wrong charge ===\n",
    "# output_dir_WCh = \"/group/belle/users/amubarak/03-ML/FakeD0_WCh/\"\n",
    "# os.makedirs(output_dir_WCh, exist_ok=True)\n",
    "\n",
    "# # === Base input path for original files (wrong charge) ===\n",
    "# base_input_dir_WCh = \"/group/belle/users/amubarak/02-Grid/Sample_Grid_WCh\"\n",
    "# Date_WCh = \"0630\"\n",
    "# Attempt_WCh = \"0\"\n",
    "\n",
    "# # === Save each wrong-charge DataFrame using original filename with _withBDT suffix ===\n",
    "# for s in samples_WCh:\n",
    "#     if s not in DataFrames:\n",
    "#         print(f\"Warning: {s} not in DataFrames ‚Äî skipping.\")\n",
    "#         continue\n",
    "\n",
    "#     # Convert Fake D‚Å∞ BDT output to float32 if it exists\n",
    "#     if \"Ds_FakeD0BDT\" in DataFrames[s].columns:\n",
    "#         DataFrames[s][\"Ds_FakeD0BDT\"] = DataFrames[s][\"Ds_FakeD0BDT\"].astype(np.float32)\n",
    "\n",
    "#     # Set original file name\n",
    "#     if s == \"Signal_WCh\":\n",
    "#         original_name = \"Ds2D0enu-Signal_WCh.root\"\n",
    "#     else:\n",
    "#         tag = s.replace(\"_WCh\", \"\")\n",
    "#         original_name = f\"Ds2D0e-Generic_Ds_{Date_WCh}25_{Attempt_WCh}_{tag}.root\"\n",
    "\n",
    "#     # Build output path with _withBDT suffix\n",
    "#     output_name = original_name.replace(\".root\", \"_withBDT.root\")\n",
    "#     out_path = os.path.join(output_dir_WCh, output_name)\n",
    "\n",
    "#     # Save DataFrame to ROOT\n",
    "#     with uproot.recreate(out_path) as f:\n",
    "#         f[\"Dstree\"] = DataFrames[s]\n",
    "\n",
    "#     print(f\"Saved: {out_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reverse PID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import uproot\n",
    "\n",
    "# # === Make sure ReverseID samples list is defined ===\n",
    "# samples_ReverseID = [\"Signal_ReverseID\", \"BB_ReverseID\", \"ccbar_ReverseID\", \"ddbar_ReverseID\", \"ssbar_ReverseID\", \"taupair_ReverseID\", \"uubar_ReverseID\", \"Data_ReverseID\"]\n",
    "\n",
    "# # === Output directory for ReverseID ===\n",
    "# output_dir_ReverseID = \"/group/belle/users/amubarak/03-ML/FakeD0_ReverseID/\"\n",
    "# os.makedirs(output_dir_ReverseID, exist_ok=True)\n",
    "\n",
    "# # === Base input path for original files (ReverseID) ===\n",
    "# base_input_dir_ReverseID = \"/group/belle/users/amubarak/02-Grid/Sample_Grid_ReverseID\"\n",
    "# Date_ReverseID = \"0626\"\n",
    "# Attempt_ReverseID = \"0\"\n",
    "\n",
    "# # === Save each ReverseID DataFrame using original filename with _withBDT suffix ===\n",
    "# for s in samples_ReverseID:\n",
    "#     if s not in DataFrames:\n",
    "#         print(f\"Warning: {s} not in DataFrames ‚Äî skipping.\")\n",
    "#         continue\n",
    "\n",
    "#     # Convert Fake D‚Å∞ BDT output to float32 if it exists\n",
    "#     if \"Ds_FakeD0BDT\" in DataFrames[s].columns:\n",
    "#         DataFrames[s][\"Ds_FakeD0BDT\"] = DataFrames[s][\"Ds_FakeD0BDT\"].astype(np.float32)\n",
    "\n",
    "#     # Set original file name\n",
    "#     if s == \"Signal_ReverseID\":\n",
    "#         original_name = \"Ds2D0enu-Signal_ReverseID.root\"\n",
    "#     else:\n",
    "#         tag = s.replace(\"_ReverseID\", \"\")\n",
    "#         original_name = f\"Ds2D0e-Generic_Ds_{Date_ReverseID}25_{Attempt_ReverseID}_{tag}.root\"\n",
    "\n",
    "#     # Build output path with _withBDT suffix\n",
    "#     output_name = original_name.replace(\".root\", \"_withBDT.root\")\n",
    "#     out_path = os.path.join(output_dir_ReverseID, output_name)\n",
    "\n",
    "#     # Save DataFrame to ROOT\n",
    "#     with uproot.recreate(out_path) as f:\n",
    "#         f[\"Dstree\"] = DataFrames[s]\n",
    "\n",
    "#     print(f\"Saved: {out_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reverse PID and Wrong Charge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import uproot\n",
    "\n",
    "# === Make sure ReverseID samples list is defined ===\n",
    "samples_ReverseID_WCh = [\"BB_ReverseID_WCh\", \"ccbar_ReverseID_WCh\", \"ddbar_ReverseID_WCh\", \"ssbar_ReverseID_WCh\", \n",
    "                         \"taupair_ReverseID_WCh\", \"uubar_ReverseID_WCh\", \"Data_ReverseID_WCh\"]\n",
    "\n",
    "# === Output directory for ReverseID ===\n",
    "output_dir_ReverseID_WCh = \"/group/belle/users/amubarak/03-ML/FakeD0_ReverseID_WCh/\"\n",
    "os.makedirs(output_dir_ReverseID_WCh, exist_ok=True)\n",
    "\n",
    "# === Base input path for original files (ReverseID) ===\n",
    "base_input_dir_ReverseID_WCh = \"/group/belle/users/amubarak/02-Grid/Sample_Grid_ReverseID_WCh\"\n",
    "Date_ReverseID_WCh = \"0708\"\n",
    "Attempt_ReverseID_WCh = \"0\"\n",
    "\n",
    "# === Save each ReverseID DataFrame using original filename with _withBDT suffix ===\n",
    "for s in samples_ReverseID_WCh:\n",
    "    if s not in DataFrames:\n",
    "        print(f\"Warning: {s} not in DataFrames ‚Äî skipping.\")\n",
    "        continue\n",
    "\n",
    "    # Convert Fake D‚Å∞ BDT output to float32 if it exists\n",
    "    if \"Ds_FakeD0BDT\" in DataFrames[s].columns:\n",
    "        DataFrames[s][\"Ds_FakeD0BDT\"] = DataFrames[s][\"Ds_FakeD0BDT\"].astype(np.float32)\n",
    "\n",
    "    # Set original file name\n",
    "    if s == \"Signal_ReverseID_WCh\":\n",
    "        original_name = \"Ds2D0enu-Signal_ReverseID_WCh.root\"\n",
    "    else:\n",
    "        tag = s.replace(\"_ReverseID_WCh\", \"\")\n",
    "        original_name = f\"Ds2D0e-Generic_Ds_{Date_ReverseID_WCh}25_{Attempt_ReverseID_WCh}_{tag}.root\"\n",
    "\n",
    "    # Build output path with _withBDT suffix\n",
    "    output_name = original_name.replace(\".root\", \"_withBDT.root\")\n",
    "    out_path = os.path.join(output_dir_ReverseID_WCh, output_name)\n",
    "\n",
    "    # Save DataFrame to ROOT\n",
    "    with uproot.recreate(out_path) as f:\n",
    "        f[\"Dstree\"] = DataFrames[s]\n",
    "\n",
    "    print(f\"Saved: {out_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Belle2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
