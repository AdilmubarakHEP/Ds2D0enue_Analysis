{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fake D⁰ BDT Training - Multi-Mode Implementation\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook trains an XGBoost BDT to suppress fake D⁰ backgrounds using the **corrected labeling procedure**:\n",
    "\n",
    "- **Real D⁰**: `abs(D0_mcPDG) == 421` from BOTH signal MC AND generic MC\n",
    "- **Fake D⁰**: `abs(D0_mcPDG) != 421` or NaN from generic MC\n",
    "\n",
    "**Key Features**:\n",
    "- Multi-mode support: kmpip, km3pi, kmpippi0_eff20_May2020\n",
    "- Uses ALL variables from `final_variables.py` (278, 478, 788 vars per mode)\n",
    "- Sequential processing to avoid memory bloat\n",
    "- Punzi FoM for cut optimization (90% CL, 95% CL, 3σ)\n",
    "- Configurable hyperparameter optimization toggle\n",
    "- Control sample support with toggle\n",
    "- Comprehensive plotting and diagnostics\n",
    "- Automatic saving to `/group/belle/users/amubarak/04-ML/`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System Information\n",
    "\n",
    "Check available CPU cores and RAM before processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total RAM: 810.42 GB\n",
      "Available: 642.48 GB\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "\n",
    "mem = psutil.virtual_memory()\n",
    "print(f\"Total RAM: {mem.total / 1e9:.2f} GB\")\n",
    "print(f\"Available: {mem.available / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import gc\n",
    "import seaborn as sns\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import uproot\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import auc, roc_curve, confusion_matrix, classification_report\n",
    "from sklearn.metrics import precision_recall_curve, mean_squared_error, accuracy_score, roc_auc_score\n",
    "from sklearn.model_selection import GridSearchCV, cross_validate, validation_curve\n",
    "from sklearn.model_selection import train_test_split, KFold, learning_curve, cross_val_score\n",
    "from sklearn.utils import compute_sample_weight\n",
    "from scipy.stats import ks_2samp, randint, uniform\n",
    "\n",
    "import xgboost\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({\n",
    "    \"axes.labelsize\": 16,\n",
    "    \"xtick.labelsize\": 12,\n",
    "    \"ytick.labelsize\": 12,\n",
    "    \"legend.fontsize\": 14,\n",
    "    \"figure.titlesize\": 20\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 200000)\n",
    "pd.set_option('display.max_columns', 200000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported custom functions successfully\n"
     ]
    }
   ],
   "source": [
    "sys.path.append(\"/home/belle2/amubarak/Ds2D0enue_Analysis/08-Python_Functions/\")\n",
    "sys.path.append(\"/home/belle2/amubarak/Ds2D0enue_Analysis/05-ML/Variables/\")\n",
    "\n",
    "# Import custom functions\n",
    "from Functions import optimize_cut, plot_save\n",
    "from Ds2D0e_config import DECAY_CONFIG, BACKGROUND_SAMPLES, get_signal_file, get_generic_file\n",
    "from final_variables import VARIABLES\n",
    "\n",
    "print(\"Imported custom functions successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Set the parameters for BDT training, data loading, and output saving.\n",
    "\n",
    "**Important toggles**:\n",
    "- `LOAD_CONTROL_SAMPLES`: Enable/disable control sample processing (WCh, ReverseID, etc.)\n",
    "- `RUN_OPTIMIZATION`: Enable/disable hyperparameter optimization via RandomizedSearchCV\n",
    "- `SAVE_OUTPUT`: Enable/disable ROOT file output saving\n",
    "- `SAVE_IMAGES`: Enable/disable plot saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded:\n",
      "  Train mode: all\n",
      "  Load control samples: False\n",
      "  Save images: True\n",
      "  Save output: False\n",
      "  Run hyperparameter optimization: False\n",
      "  N signal events: 100,000\n",
      "  Luminosity: 200 fb^-1\n"
     ]
    }
   ],
   "source": [
    "# ========================================================================\n",
    "# CONFIGURATION SECTION\n",
    "# ========================================================================\n",
    "\n",
    "# === Mode Selection ===\n",
    "# Set to specific mode: \"kmpip\", \"km3pi\", \"kmpippi0_eff20_May2020\"\n",
    "# Set to \"all\" to train on all modes sequentially\n",
    "TRAIN_MODE = \"all\"  # Change this to train different modes\n",
    "\n",
    "# === Control Samples ===\n",
    "# Toggle for loading control samples (noEID, wrongCharge, reverseID)\n",
    "LOAD_CONTROL_SAMPLES = False  # Set to True to load control samples\n",
    "CONTROL_SAMPLES = [\"WCh\", \"ReverseID\", \"ReverseID_WCh\"]  # Which control samples to load\n",
    "\n",
    "# === Image Saving ===\n",
    "SAVE_IMAGES = True  # Set to True to save all plots\n",
    "OUTPUT_DIR = \"/home/belle2/amubarak/Ds2D0enue_Analysis/05-ML/Figures/FakeD0_BDT\"\n",
    "\n",
    "# === ROOT File Output Saving ===\n",
    "SAVE_OUTPUT = False  # Set to True to save DataFrames with BDT to ROOT files\n",
    "OUTPUT_BASE_DIR = \"/group/belle/users/amubarak/04-ML/\"\n",
    "\n",
    "# === Normalization Parameters for Punzi FoM ===\n",
    "N_SIGNAL_EVENTS = 100_000  # Number of signal MC events generated\n",
    "LUMINOSITY_FB = 200  # Integrated luminosity in fb^-1\n",
    "PUNZI_A_VALUES = [1.64, 1.96, 3.0]  # 90% CL, 95% CL, 3σ discovery\n",
    "\n",
    "# === BDT Training Parameters ===\n",
    "RANDOM_STATE = 42\n",
    "TEST_SIZE = 0.30\n",
    "N_ESTIMATORS = 100\n",
    "\n",
    "# === Hyperparameter Optimization Toggle ===\n",
    "RUN_OPTIMIZATION = False  # Set to False to skip hyperparameter optimization\n",
    "RANDOM_SEARCH_ITERS = 50  # Number of random search iterations (only if RUN_OPTIMIZATION=True)\n",
    "\n",
    "print(f\"Configuration loaded:\")\n",
    "print(f\"  Train mode: {TRAIN_MODE}\")\n",
    "print(f\"  Load control samples: {LOAD_CONTROL_SAMPLES}\")\n",
    "print(f\"  Save images: {SAVE_IMAGES}\")\n",
    "print(f\"  Save output: {SAVE_OUTPUT}\")\n",
    "print(f\"  Run hyperparameter optimization: {RUN_OPTIMIZATION}\")\n",
    "print(f\"  N signal events: {N_SIGNAL_EVENTS:,}\")\n",
    "print(f\"  Luminosity: {LUMINOSITY_FB} fb^-1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data Configuration and Variables\n",
    "\n",
    "Load decay mode configurations from `Ds2D0e_config.py` and the final variable lists from `final_variables.py`.\n",
    "\n",
    "**Variable counts per mode**:\n",
    "- kmpip: 278 variables\n",
    "- km3pi: 478 variables  \n",
    "- kmpippi0_eff20_May2020: 788 variables\n",
    "\n",
    "We use ALL variables from `final_variables.py` without reduction at this stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available decay modes:\n",
      "  - kmpip\n",
      "  - km3pi\n",
      "  - kmpippi0_eff20_May2020\n",
      "\n",
      "Variable counts per mode:\n",
      "  kmpip: 18 variables\n",
      "  km3pi: 52 variables\n",
      "  kmpippi0_eff20_May2020: 78 variables\n"
     ]
    }
   ],
   "source": [
    "from Ds2D0e_config import DECAY_CONFIG, BACKGROUND_SAMPLES, get_signal_file, get_generic_file\n",
    "from final_variables import VARIABLES\n",
    "\n",
    "print(\"Available decay modes:\")\n",
    "for mode in DECAY_CONFIG.keys():\n",
    "    print(f\"  - {mode}\")\n",
    "\n",
    "print(\"\\nVariable counts per mode:\")\n",
    "for mode in VARIABLES.keys():\n",
    "    print(f\"  {mode}: {len(VARIABLES[mode]['all_vars'])} variables\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions\n",
    "\n",
    "### Punzi Figure of Merit\n",
    "\n",
    "The Punzi FoM is used for cut optimization in the presence of systematic uncertainties:\n",
    "\n",
    "$$\\text{Punzi FoM} = \\frac{\\epsilon_S}{\\frac{a}{2} + \\sqrt{B}}$$\n",
    "\n",
    "where:\n",
    "- $\\epsilon_S$ is the signal efficiency\n",
    "- $B$ is the number of background events\n",
    "- $a$ is the confidence level parameter:\n",
    "  - $a = 1.64$ for 90% CL\n",
    "  - $a = 1.96$ for 95% CL\n",
    "  - $a = 3.0$ for 3σ discovery\n",
    "\n",
    "This is preferred over the simple $S/\\sqrt{S+B}$ FoM when systematic uncertainties are significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def punzi_fom(epsS, B, a=1.96):\n",
    "    \"\"\"\n",
    "    Compute Punzi figure of merit.\n",
    "    \n",
    "    Parameters:\n",
    "        epsS (float): Signal efficiency\n",
    "        B (float): Number of background events\n",
    "        a (float): Confidence level parameter (default 1.96 for 95% CL)\n",
    "    \n",
    "    Returns:\n",
    "        float: Punzi FoM = ε_S / (a/2 + √B)\n",
    "    \"\"\"\n",
    "    denom = (a / 2.0) + np.sqrt(max(B, 0.0))\n",
    "    return 0.0 if denom <= 0.0 else epsS / denom\n",
    "\n",
    "\n",
    "def compute_punzi_scan(bdt_scores_sig, bdt_scores_bkg, n_sig_total, n_thresholds=200, a=1.96):\n",
    "    \"\"\"\n",
    "    Scan BDT thresholds and compute Punzi FoM.\n",
    "    \n",
    "    Parameters:\n",
    "        bdt_scores_sig (array): BDT scores for signal events\n",
    "        bdt_scores_bkg (array): BDT scores for background events\n",
    "        n_sig_total (int): Total number of signal events (for efficiency calculation)\n",
    "        n_thresholds (int): Number of thresholds to scan\n",
    "        a (float): Confidence level parameter\n",
    "    \n",
    "    Returns:\n",
    "        thresholds, foms, best_threshold, best_fom\n",
    "    \"\"\"\n",
    "    thresholds = np.linspace(0, 1, n_thresholds)\n",
    "    foms = []\n",
    "    \n",
    "    for t in thresholds:\n",
    "        n_sig_pass = np.sum(bdt_scores_sig > t)\n",
    "        n_bkg_pass = np.sum(bdt_scores_bkg > t)\n",
    "        \n",
    "        epsS = n_sig_pass / n_sig_total if n_sig_total > 0 else 0.0\n",
    "        fom = punzi_fom(epsS, n_bkg_pass, a)\n",
    "        foms.append(fom)\n",
    "    \n",
    "    foms = np.array(foms)\n",
    "    best_idx = np.argmax(foms)\n",
    "    return thresholds, foms, thresholds[best_idx], foms[best_idx]\n",
    "\n",
    "\n",
    "def get_pulls(counts, errors, pdf):\n",
    "    \"\"\"\n",
    "    Compute pull values for comparing test vs train distributions.\n",
    "    \n",
    "    Parameters:\n",
    "        counts (array): Test counts\n",
    "        errors (array): Test errors (uncertainties)\n",
    "        pdf (array): Train probability density (normalized)\n",
    "    \n",
    "    Returns:\n",
    "        array: Pull values = (counts - pdf) / errors\n",
    "    \"\"\"\n",
    "    pull = (counts - pdf) / errors\n",
    "    return pull"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading Functions\n",
    "\n",
    "Functions to load signal and generic MC for each decay mode, applying D⁰ mass window cuts from the configuration.\n",
    "\n",
    "**Data sources**:\n",
    "- Signal MC: Ds → D⁰ e ν samples\n",
    "- Generic MC: Combined generic BB̄ backgrounds (charged, mixed, uubar, ddbar, ssbar, ccbar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mode titles loaded:\n",
      "  kmpip: $D_s^{+} \\rightarrow [D^{0} \\rightarrow K^{-} \\pi^{+}] e^{+} \\nu_{e}$\n",
      "  kmpippi0_eff20_May2020: $D_s^{+} \\rightarrow [D^{0} \\rightarrow K^{-} \\pi^{+} \\pi^{0}] e^{+} \\nu_{e}$\n",
      "  km3pi: $D_s^{+} \\rightarrow [D^{0} \\rightarrow K^{-} 3\\pi] e^{+} \\nu_{e}$\n"
     ]
    }
   ],
   "source": [
    "# Mode titles with proper LaTeX formatting for decay chains\n",
    "MODE_TITLES = {\n",
    "    \"kmpip\": r\"$D_s^{+} \\rightarrow [D^{0} \\rightarrow K^{-} \\pi^{+}] e^{+} \\nu_{e}$\",\n",
    "    \"kmpippi0_eff20_May2020\": r\"$D_s^{+} \\rightarrow [D^{0} \\rightarrow K^{-} \\pi^{+} \\pi^{0}] e^{+} \\nu_{e}$\",\n",
    "    \"km3pi\": r\"$D_s^{+} \\rightarrow [D^{0} \\rightarrow K^{-} 3\\pi] e^{+} \\nu_{e}$\",\n",
    "}\n",
    "\n",
    "print(\"Mode titles loaded:\")\n",
    "for mode, title in MODE_TITLES.items():\n",
    "    print(f\"  {mode}: {title}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importance(model, features, mode, num=None, save_dir=None):\n",
    "    \"\"\"\n",
    "    Plot feature importance from trained XGBoost model and save all importances to CSV.\n",
    "    \n",
    "    Parameters:\n",
    "        model: Trained XGBoost classifier\n",
    "        features: List of feature names\n",
    "        mode: Decay mode (for title)\n",
    "        num: Number of top features to show in plot (None = show all)\n",
    "        save_dir: Directory to save plot and CSV (None = show plot)\n",
    "    \"\"\"\n",
    "    feature_imp = pd.DataFrame({\n",
    "        'Value': model.feature_importances_,\n",
    "        'Feature': features\n",
    "    })\n",
    "    \n",
    "    # Sort by importance\n",
    "    feature_imp_sorted = feature_imp.sort_values(by=\"Value\", ascending=False)\n",
    "    \n",
    "    # Save complete table to CSV\n",
    "    if save_dir:\n",
    "        csv_path = os.path.join(save_dir, \"feature_importance_all.csv\")\n",
    "        feature_imp_sorted.to_csv(csv_path, index=False)\n",
    "        print(f\"\\n✓ Saved complete feature importance table to: {csv_path}\")\n",
    "    \n",
    "    # If num is specified, take top N for plotting, otherwise take all\n",
    "    if num is not None:\n",
    "        feature_imp_plot = feature_imp_sorted.head(num)\n",
    "        title_suffix = f\"Top {num} Feature Importances\"\n",
    "    else:\n",
    "        feature_imp_plot = feature_imp_sorted\n",
    "        title_suffix = f\"Feature Importances (All {len(features)} variables)\"\n",
    "    \n",
    "    # Adjust figure size based on number of features\n",
    "    n_features = len(feature_imp_plot)\n",
    "    fig_height = max(8, min(100, n_features * 0.3))  # At least 8, at most 100\n",
    "    \n",
    "    plt.figure(figsize=(16, fig_height))\n",
    "    sns.barplot(\n",
    "        x=\"Value\", y=\"Feature\",\n",
    "        data=feature_imp_plot\n",
    "    )\n",
    "    \n",
    "    title = MODE_TITLES.get(mode, mode)\n",
    "    plt.title(f'{title}\\n{title_suffix}', loc='left')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_dir:\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        if num is not None:\n",
    "            plt.savefig(os.path.join(save_dir, f\"feature_importance_top{num}.png\"), dpi=150, bbox_inches='tight')\n",
    "        else:\n",
    "            plt.savefig(os.path.join(save_dir, \"feature_importance_all.png\"), dpi=150, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()\n",
    "    \n",
    "    return feature_imp_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_train_test(clf, X_train, y_train, X_test, y_test, mode):\n",
    "    \"\"\"\n",
    "    Plot BDT output comparing train vs test with pull plots.\n",
    "    \n",
    "    Parameters:\n",
    "        clf: Trained classifier\n",
    "        X_train, y_train: Training data and labels\n",
    "        X_test, y_test: Test data and labels\n",
    "        mode: Decay mode (for title)\n",
    "    \n",
    "    Returns:\n",
    "        decisions: List of [train_bkg, train_sig, test_bkg, test_sig] BDT outputs\n",
    "    \"\"\"\n",
    "    decisions = []  # list to hold decisions of classifier\n",
    "    for X, y in ((X_train, y_train), (X_test, y_test)):  # train and test\n",
    "        if hasattr(clf, \"predict_proba\"):  # if predict_proba function exists\n",
    "            d1 = clf.predict_proba(X[y<0.5])[:, 1]  # background\n",
    "            d2 = clf.predict_proba(X[y>0.5])[:, 1]  # signal\n",
    "        else:  # predict_proba function doesn't exist\n",
    "            X_tensor = torch.as_tensor(X, dtype=torch.float)\n",
    "            y_tensor = torch.as_tensor(y, dtype=torch.long)\n",
    "            X_var, y_var = Variable(X_tensor), Variable(y_tensor)\n",
    "            d1 = clf(X_var[y_var<0.5])[1][:, 1].cpu().detach().numpy()  # background\n",
    "            d2 = clf(X_var[y_var>0.5])[1][:, 1].cpu().detach().numpy()  # signal\n",
    "        decisions += [d1, d2]  # add to list of classifier decision\n",
    "\n",
    "    lw = 3\n",
    "    fig, axs = plt.subplots(3, 1, figsize=(10, 10), gridspec_kw={'height_ratios':[1, 0.2, 0.2]})\n",
    "\n",
    "    bins = 50\n",
    "    bin_edges = np.linspace(0, 1, bins)\n",
    "    \n",
    "    test_bkg_count_weight = bins / len(decisions[2])\n",
    "    test_sig_count_weight = bins / len(decisions[3])\n",
    "    test_bkg_counts, test_bkg_bins = np.histogram(decisions[2], bins=bins, range=(0, 1))\n",
    "    test_sig_counts, test_sig_bins = np.histogram(decisions[3], bins=bins, range=(0, 1))\n",
    "\n",
    "    train_bkg_counts, train_bkg_bins, _etc = axs[0].hist(\n",
    "        decisions[0], color='tab:blue', histtype='step', bins=bins,\n",
    "        density=True, range=(0, 1), linewidth=lw, label='Train Background'\n",
    "    )\n",
    "    train_sig_counts, train_sig_bins, _etc = axs[0].hist(\n",
    "        decisions[1], color='tab:red', histtype='step', bins=bins,\n",
    "        density=True, range=(0, 1), linewidth=lw, label=r'Train Signal'\n",
    "    )\n",
    "    axs[0].hist(decisions[0], color='tab:blue', histtype='stepfilled',\n",
    "                alpha=0.4, bins=bins, density=True, range=(0, 1))\n",
    "    axs[0].hist(decisions[1], color='tab:red', histtype='stepfilled',\n",
    "                alpha=0.4, bins=bins, density=True, range=(0, 1))\n",
    "    \n",
    "    bin_width = test_bkg_bins[1] - test_bkg_bins[0]\n",
    "    bin_centers = [el + (bin_width/2) for el in test_bkg_bins[:-1]]\n",
    "\n",
    "    axs[0].errorbar(\n",
    "        bin_centers, test_bkg_count_weight * test_bkg_counts,\n",
    "        yerr=test_bkg_count_weight * np.sqrt(test_bkg_counts),\n",
    "        label='Test Background', color='tab:blue', marker='o', linewidth=lw, ls=''\n",
    "    )\n",
    "    axs[0].errorbar(\n",
    "        bin_centers, test_sig_count_weight * test_sig_counts,\n",
    "        yerr=test_sig_count_weight * np.sqrt(test_sig_counts),\n",
    "        label='Test Signal', color='tab:red', marker='o', linewidth=lw, ls=''\n",
    "    )\n",
    "    \n",
    "    # Title with proper LaTeX decay chain\n",
    "    title = MODE_TITLES.get(mode, mode)\n",
    "    axs[0].set_title(title, loc='left')\n",
    "    axs[0].set_xlim(0, 1)\n",
    "    axs[0].set_ylim(0)\n",
    "    axs[0].set_ylabel('Event Density')\n",
    "\n",
    "    # K-S test scores\n",
    "    ks_p_value_sig = ks_2samp(decisions[1], decisions[3])[1]\n",
    "    ks_p_value_bkg = ks_2samp(decisions[0], decisions[2])[1]\n",
    "\n",
    "    leg = axs[0].legend(\n",
    "        loc='upper center',\n",
    "        title=f\"Sig K-S test score: {ks_p_value_sig:0.3f}\\nBkg K-S test score: {ks_p_value_bkg:0.3f}\"\n",
    "    )\n",
    "    leg._legend_box.align = \"left\"\n",
    "\n",
    "    # Background pulls\n",
    "    pulls = get_pulls(\n",
    "        test_bkg_count_weight * test_bkg_counts,\n",
    "        test_bkg_count_weight * np.sqrt(test_bkg_counts),\n",
    "        np.array(train_bkg_counts)\n",
    "    )\n",
    "    axs[1].bar(bin_centers, pulls, width=bin_width)\n",
    "    axs[1].set_xlim(0, 1)\n",
    "    axs[1].set_ylabel('Pulls')\n",
    "    axs[1].set_ylim(-5, 5)\n",
    "\n",
    "    # Signal pulls\n",
    "    pulls = get_pulls(\n",
    "        test_sig_count_weight * test_sig_counts,\n",
    "        test_sig_count_weight * np.sqrt(test_sig_counts),\n",
    "        np.array(train_sig_counts)\n",
    "    )\n",
    "    axs[2].bar(bin_centers, pulls, width=bin_width, color='tab:red')\n",
    "    axs[2].set_xlim(0, 1)\n",
    "    axs[2].set_ylabel('Pulls')\n",
    "    axs[2].set_ylim(-5, 5)\n",
    "    axs[2].set_xlabel(r'BDT output')\n",
    "\n",
    "    return decisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_valid_variables(df, variable_list, nan_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Filter variable list to only include valid columns from dataframe.\n",
    "    \n",
    "    Removes variables that:\n",
    "    - Don't exist in the dataframe\n",
    "    - Are not numeric\n",
    "    - Have >50% NaN values (by default)\n",
    "    - Have any infinity values\n",
    "    \n",
    "    Parameters:\n",
    "        df: DataFrame to check\n",
    "        variable_list: List of variable names to filter\n",
    "        nan_threshold: Maximum fraction of NaN values allowed (0.5 = 50%)\n",
    "    \n",
    "    Returns:\n",
    "        List of valid variable names\n",
    "    \"\"\"\n",
    "    valid_vars = []\n",
    "    \n",
    "    for var in variable_list:\n",
    "        # Check if variable exists\n",
    "        if var not in df.columns:\n",
    "            continue\n",
    "        \n",
    "        # Check if numeric\n",
    "        if not pd.api.types.is_numeric_dtype(df[var]):\n",
    "            continue\n",
    "        \n",
    "        col_data = df[var]\n",
    "        \n",
    "        # Check NaN fraction\n",
    "        nan_fraction = col_data.isna().sum() / len(col_data)\n",
    "        if nan_fraction > nan_threshold:\n",
    "            print(f\"  ⚠ Skipping {var}: {nan_fraction*100:.1f}% NaN values\")\n",
    "            continue\n",
    "        \n",
    "        # Check for infinity\n",
    "        if np.isinf(col_data.dropna()).any():\n",
    "            print(f\"  ⚠ Skipping {var}: contains infinity values\")\n",
    "            continue\n",
    "        \n",
    "        valid_vars.append(var)\n",
    "    \n",
    "    return valid_vars\n",
    "\n",
    "\n",
    "def load_mode_data(mode, control_sample=None):\n",
    "    \"\"\"\n",
    "    Load signal and generic MC for a given decay mode.\n",
    "    Applies D⁰ mass window cuts from config.\n",
    "    \n",
    "    Parameters:\n",
    "        mode (str): Decay mode (kmpip, km3pi, kmpippi0_eff20_May2020)\n",
    "        control_sample (str): Control sample tag (\"WCh\", \"ReverseID\", etc.) or None for nominal\n",
    "    \n",
    "    Returns:\n",
    "        df_signal, df_generic (DataFrames)\n",
    "    \"\"\"\n",
    "    config = DECAY_CONFIG[mode]\n",
    "    tree_name = config[\"ds_tree\"]\n",
    "    mass_cut = config[\"cut\"]\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Loading data for mode: {mode}\")\n",
    "    if control_sample:\n",
    "        print(f\"Control sample: {control_sample}\")\n",
    "    print(f\"Tree: {tree_name}\")\n",
    "    print(f\"D⁰ mass cut: {mass_cut}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    # Load signal MC\n",
    "    use_control = control_sample is not None\n",
    "    control_tag = control_sample if control_sample else None\n",
    "    \n",
    "    signal_path = get_signal_file(mode, use_control_sample=use_control, control_sample_tag=control_tag)\n",
    "    if isinstance(signal_path, list):\n",
    "        signal_path = signal_path[0]  # Take first if multiple returned\n",
    "    \n",
    "    print(f\"Loading Signal MC from: {signal_path}\")\n",
    "    try:\n",
    "        df_signal = uproot.concatenate(f\"{signal_path}:{tree_name}\", library='pd')\n",
    "        df_signal = df_signal.query(mass_cut)  # Apply D⁰ mass window\n",
    "        print(f\"  → Signal MC: {len(df_signal):,} events after D⁰ mass cut\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ Failed to load signal: {e}\")\n",
    "        df_signal = pd.DataFrame()\n",
    "    \n",
    "    # Load generic MC (all background samples combined)\n",
    "    df_generic_list = []\n",
    "    for sample in tqdm(BACKGROUND_SAMPLES, desc=\"Loading generic MC\"):\n",
    "        generic_path = get_generic_file(sample, mode, use_control_sample=use_control, \n",
    "                                       control_sample_tag=control_tag)\n",
    "        if isinstance(generic_path, list):\n",
    "            generic_path = generic_path[0]\n",
    "        \n",
    "        try:\n",
    "            df = uproot.concatenate(f\"{generic_path}:{tree_name}\", library='pd')\n",
    "            df = df.query(mass_cut)  # Apply D⁰ mass window\n",
    "            df_generic_list.append(df)\n",
    "            print(f\"  → {sample}: {len(df):,} events after D⁰ mass cut\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ Failed to load {sample}: {e}\")\n",
    "    \n",
    "    df_generic = pd.concat(df_generic_list, ignore_index=True) if df_generic_list else pd.DataFrame()\n",
    "    print(f\"\\n  → Total Generic MC: {len(df_generic):,} events\\n\")\n",
    "    \n",
    "    return df_signal, df_generic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pulls(test_data, test_errors, train_data):\n",
    "    \"\"\"\n",
    "    Calculate pulls for overtraining check.\n",
    "    \n",
    "    Parameters:\n",
    "        test_data: Test histogram values\n",
    "        test_errors: Test histogram errors\n",
    "        train_data: Train histogram values\n",
    "    \n",
    "    Returns:\n",
    "        pulls: (test_data - train_data) / test_errors array\n",
    "    \"\"\"\n",
    "    # Avoid division by zero\n",
    "    pulls = np.zeros(len(test_data))\n",
    "    mask = test_errors > 0\n",
    "    pulls[mask] = (test_data[mask] - train_data[mask]) / test_errors[mask]\n",
    "    return pulls\n",
    "\n",
    "\n",
    "def compute_punzi_scan(signal_scores, background_scores, n_sig_events, n_thresholds=100, a=1.96):\n",
    "    \"\"\"\n",
    "    Scan BDT cuts and compute Punzi Figure of Merit (FoM).\n",
    "    \n",
    "    Punzi FoM = n_sig / (a + sqrt(n_bkg))\n",
    "    \n",
    "    Parameters:\n",
    "        signal_scores: Array of BDT scores for signal events\n",
    "        background_scores: Array of BDT scores for background events\n",
    "        n_sig_events: Number of signal events (for FoM calculation)\n",
    "        n_thresholds: Number of threshold points to scan\n",
    "        a: Confidence level parameter (default 1.96 for 95% CL)\n",
    "    \n",
    "    Returns:\n",
    "        thresholds: Array of BDT cut values\n",
    "        foms: Array of FoM values\n",
    "        best_threshold: Optimal BDT cut\n",
    "        best_fom: Maximum FoM value\n",
    "    \"\"\"\n",
    "    thresholds = np.linspace(0, 1, n_thresholds)\n",
    "    foms = []\n",
    "    \n",
    "    for cut in thresholds:\n",
    "        # Count events passing cut\n",
    "        n_sig = np.sum(signal_scores > cut)\n",
    "        n_bkg = np.sum(background_scores > cut)\n",
    "        \n",
    "        # Compute FoM\n",
    "        if n_bkg < 0:\n",
    "            n_bkg = 0\n",
    "        \n",
    "        # Scale FoM to full dataset\n",
    "        if n_sig > 0:\n",
    "            fom = n_sig / np.sqrt(a**2 + n_bkg) if (a**2 + n_bkg) > 0 else 0\n",
    "        else:\n",
    "            fom = 0\n",
    "        \n",
    "        foms.append(fom)\n",
    "    \n",
    "    foms = np.array(foms)\n",
    "    best_idx = np.argmax(foms)\n",
    "    best_threshold = thresholds[best_idx]\n",
    "    best_fom = foms[best_idx]\n",
    "    \n",
    "    return thresholds, foms, best_threshold, best_fom\n",
    "\n",
    "\n",
    "def print_bdt_statistics(model, X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Print BDT training and test statistics.\n",
    "    \n",
    "    Parameters:\n",
    "        model: Trained XGBoost classifier\n",
    "        X_train, y_train: Training features and labels\n",
    "        X_test, y_test: Test features and labels\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"BDT STATISTICS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Get predictions\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    \n",
    "    y_proba_train = model.predict_proba(X_train)[:, 1]\n",
    "    y_proba_test = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Training metrics\n",
    "    train_acc = accuracy_score(y_train, y_pred_train)\n",
    "    train_auc = roc_auc_score(y_train, y_proba_train)\n",
    "    \n",
    "    # Test metrics\n",
    "    test_acc = accuracy_score(y_test, y_pred_test)\n",
    "    test_auc = roc_auc_score(y_test, y_proba_test)\n",
    "    \n",
    "    # Compute precision and recall\n",
    "    precision_train = confusion_matrix(y_train, y_pred_train)[1, 1] / (confusion_matrix(y_train, y_pred_train)[1, 1] + confusion_matrix(y_train, y_pred_train)[0, 1]) if (confusion_matrix(y_train, y_pred_train)[1, 1] + confusion_matrix(y_train, y_pred_train)[0, 1]) > 0 else 0\n",
    "    precision_test = confusion_matrix(y_test, y_pred_test)[1, 1] / (confusion_matrix(y_test, y_pred_test)[1, 1] + confusion_matrix(y_test, y_pred_test)[0, 1]) if (confusion_matrix(y_test, y_pred_test)[1, 1] + confusion_matrix(y_test, y_pred_test)[0, 1]) > 0 else 0\n",
    "    \n",
    "    print(f\"\\nTraining Set Performance:\")\n",
    "    print(f\"  Accuracy: {train_acc:.4f}\")\n",
    "    print(f\"  ROC AUC:  {train_auc:.4f}\")\n",
    "    print(f\"\\nTest Set Performance:\")\n",
    "    print(f\"  Accuracy: {test_acc:.4f}\")\n",
    "    print(f\"  ROC AUC:  {test_auc:.4f}\")\n",
    "    print(f\"\\nOvertraining Check (Test - Train):\")\n",
    "    print(f\"  ΔAccuracy: {test_acc - train_acc:+.4f}\")\n",
    "    print(f\"  ΔAUC:      {test_auc - train_auc:+.4f}\")\n",
    "    \n",
    "    # Check for overtraining\n",
    "    if abs(test_auc - train_auc) < 0.01 and abs(test_acc - train_acc) < 0.01:\n",
    "        print(f\"  ✓ Good agreement (no overtraining detected)\")\n",
    "    elif test_auc < train_auc or test_acc < train_acc:\n",
    "        print(f\"  ⚠ Slight overtraining detected\")\n",
    "    else:\n",
    "        print(f\"  ✓ Test performance better than train (normal for some cases)\")\n",
    "    \n",
    "    print(\"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Set Creation with Corrected Labeling\n",
    "\n",
    "**CORRECTED PROCEDURE**:\n",
    "\n",
    "The key insight is that we want the BDT to learn \"is this a real D⁰?\" rather than \"is this from signal MC?\".\n",
    "\n",
    "- **Real D⁰ (label = 1)**: Events with `abs(D0_mcPDG) == 421` from BOTH:\n",
    "  - Signal MC (Ds → D⁰ e ν samples)\n",
    "  - Generic MC (combinatoric backgrounds that happen to form real D⁰)\n",
    "\n",
    "- **Fake D⁰ (label = 0)**: Events from generic MC with:\n",
    "  - `abs(D0_mcPDG) != 421` (combinatoric background)\n",
    "  - `D0_mcPDG` is NaN (no truth match)\n",
    "\n",
    "This ensures the BDT learns real vs fake D⁰ topology, not signal vs background event characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_set(df_signal, df_generic, mode_variables):\n",
    "    \"\"\"\n",
    "    Create training set with CORRECTED LABELING PROCEDURE.\n",
    "    \n",
    "    Real D⁰: abs(D0_mcPDG) == 421 from BOTH signal MC AND generic MC\n",
    "    Fake D⁰: abs(D0_mcPDG) != 421 or NaN from generic MC\n",
    "    \n",
    "    Parameters:\n",
    "        df_signal: Signal MC DataFrame\n",
    "        df_generic: Generic MC DataFrame\n",
    "        mode_variables: List of variable names to use\n",
    "    \n",
    "    Returns:\n",
    "        X, y, df_train, df_real, df_fake, mode_variables_filtered\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Creating training set with CORRECTED labeling\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Real D⁰ from signal MC\n",
    "    real_signal_mask = (abs(df_signal[\"D0_mcPDG\"]) == 421)\n",
    "    df_real_signal = df_signal[real_signal_mask]\n",
    "    print(f\"Real D⁰ from Signal MC: {len(df_real_signal):,} events\")\n",
    "    \n",
    "    # Real D⁰ from generic MC\n",
    "    real_generic_mask = (abs(df_generic[\"D0_mcPDG\"]) == 421)\n",
    "    df_real_generic = df_generic[real_generic_mask]\n",
    "    print(f\"Real D⁰ from Generic MC: {len(df_real_generic):,} events\")\n",
    "    \n",
    "    # Combine real D⁰\n",
    "    df_real = pd.concat([df_real_signal, df_real_generic], ignore_index=True)\n",
    "    print(f\"Total Real D⁰: {len(df_real):,} events\")\n",
    "    \n",
    "    # Fake D⁰ from generic MC\n",
    "    fake_mask = (abs(df_generic[\"D0_mcPDG\"]) != 421) | df_generic[\"D0_mcPDG\"].isna()\n",
    "    df_fake = df_generic[fake_mask]\n",
    "    print(f\"Fake D⁰ from Generic MC: {len(df_fake):,} events\")\n",
    "    \n",
    "    # Check variable availability\n",
    "    missing_vars = [v for v in mode_variables if v not in df_real.columns]\n",
    "    if missing_vars:\n",
    "        print(f\"\\n⚠ Warning: {len(missing_vars)} variables not available in data:\")\n",
    "        for v in missing_vars[:10]:  # Show first 10\n",
    "            print(f\"  - {v}\")\n",
    "        if len(missing_vars) > 10:\n",
    "            print(f\"  ... and {len(missing_vars)-10} more\")\n",
    "        \n",
    "        # Filter to available variables\n",
    "        mode_variables_filtered = [v for v in mode_variables if v in df_real.columns]\n",
    "        print(f\"\\nProceeding with {len(mode_variables_filtered)} available variables\\n\")\n",
    "    else:\n",
    "        mode_variables_filtered = mode_variables\n",
    "    \n",
    "    # Combine\n",
    "    df_train = pd.concat([df_real, df_fake], axis=0, ignore_index=True)\n",
    "    \n",
    "    # Labels: 1 for real D⁰, 0 for fake D⁰\n",
    "    labels = np.concatenate([\n",
    "        np.ones(len(df_real), dtype=np.int64),\n",
    "        np.zeros(len(df_fake), dtype=np.int64)\n",
    "    ])\n",
    "    \n",
    "    # Features\n",
    "    X = df_train[mode_variables_filtered].to_numpy(dtype=np.float32)\n",
    "    y = labels\n",
    "    \n",
    "    # Clean data: replace inf with NaN\n",
    "    X = np.where(np.isinf(X), np.nan, X)\n",
    "    \n",
    "    # Check for NaN/inf values\n",
    "    n_nan = np.sum(np.isnan(X))\n",
    "    n_inf = np.sum(np.isinf(X))\n",
    "    \n",
    "    if n_nan > 0 or n_inf > 0:\n",
    "        print(f\"\\n⚠ Data cleaning:\")\n",
    "        print(f\"  - NaN values found: {n_nan:,}\")\n",
    "        print(f\"  - Inf values found: {n_inf:,}\")\n",
    "        print(f\"  - NaN/Inf values will be handled as missing by XGBoost\")\n",
    "    \n",
    "    print(f\"\\nTraining set: {len(df_train):,} events\")\n",
    "    print(f\"  - Real D⁰ (label=1): {np.sum(y==1):,}\")\n",
    "    print(f\"  - Fake D⁰ (label=0): {np.sum(y==0):,}\")\n",
    "    print(f\"  - Class ratio (Real/Fake): {np.sum(y==1)/np.sum(y==0):.3f}\")\n",
    "    print(f\"  - Number of features: {len(mode_variables_filtered)}\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    return X, y, df_train, df_real, df_fake, mode_variables_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_bdt(X, y, run_optimization=True):\n",
    "    \"\"\"\n",
    "    Train XGBoost BDT with optional hyperparameter optimization.\n",
    "    \n",
    "    Automatically handles class imbalance using scale_pos_weight parameter.\n",
    "    \n",
    "    Parameters:\n",
    "        X: Feature matrix\n",
    "        y: Labels\n",
    "        run_optimization: Whether to run RandomizedSearchCV\n",
    "    \n",
    "    Returns:\n",
    "        bdt_final, X_train, X_test, y_train, y_test\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TRAINING BDT\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y\n",
    "    )\n",
    "    \n",
    "    print(f\"Training set: {len(X_train):,} events\")\n",
    "    print(f\"Test set: {len(X_test):,} events\")\n",
    "    \n",
    "    # Calculate class weights to handle imbalance\n",
    "    n_real = np.sum(y_train == 1)\n",
    "    n_fake = np.sum(y_train == 0)\n",
    "    scale_pos_weight = n_fake / n_real if n_real > 0 else 1.0\n",
    "    \n",
    "    print(f\"\\nClass balance in training set:\")\n",
    "    print(f\"  Real D⁰ (positive, label=1): {n_real:,} ({100*n_real/len(y_train):.1f}%)\")\n",
    "    print(f\"  Fake D⁰ (negative, label=0): {n_fake:,} ({100*n_fake/len(y_train):.1f}%)\")\n",
    "    print(f\"  Class ratio (Fake/Real): {n_fake/n_real:.2f}\")\n",
    "    print(f\"  XGBoost scale_pos_weight: {scale_pos_weight:.2f} (for balancing)\")\n",
    "    \n",
    "    if run_optimization:\n",
    "        print(f\"\\nRunning hyperparameter optimization with {RANDOM_SEARCH_ITERS} iterations...\")\n",
    "        \n",
    "        # Define base estimator with scale_pos_weight for class imbalance\n",
    "        bdt_base = XGBClassifier(\n",
    "            objective=\"binary:logistic\",\n",
    "            eval_metric=\"logloss\",\n",
    "            max_delta_step=1,\n",
    "            scale_pos_weight=scale_pos_weight,  # Handle class imbalance\n",
    "            random_state=RANDOM_STATE,\n",
    "            missing=np.nan  # Handle NaN values as missing\n",
    "        )\n",
    "        \n",
    "        # Hyperparameter search space\n",
    "        param_dist = {\n",
    "            'learning_rate': uniform(0.01, 0.19),\n",
    "            'max_depth': randint(1, 6),\n",
    "            'n_estimators': randint(100, 201),\n",
    "            'reg_lambda': uniform(1, 4),\n",
    "            'gamma': uniform(0, 4),\n",
    "            'subsample': uniform(0.5, 0.5),\n",
    "            'min_child_weight': randint(1, 7),\n",
    "            'colsample_bytree': uniform(0.3, 0.7)\n",
    "        }\n",
    "        \n",
    "        # Random search with cross-validation\n",
    "        random_search = RandomizedSearchCV(\n",
    "            bdt_base,\n",
    "            param_distributions=param_dist,\n",
    "            n_iter=RANDOM_SEARCH_ITERS,\n",
    "            cv=5,\n",
    "            scoring='roc_auc',\n",
    "            n_jobs=-1,\n",
    "            random_state=RANDOM_STATE,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        random_search.fit(X_train, y_train)\n",
    "        \n",
    "        print(\"\\nBest hyperparameters:\")\n",
    "        for param, value in random_search.best_params_.items():\n",
    "            print(f\"  {param}: {value}\")\n",
    "        \n",
    "        print(f\"\\nBest CV score (ROC AUC): {random_search.best_score_:.4f}\")\n",
    "        \n",
    "        bdt_final = random_search.best_estimator_\n",
    "    else:\n",
    "        print(\"\\nTraining with default parameters (no optimization)...\")\n",
    "        print(f\"Using scale_pos_weight={scale_pos_weight:.2f} to handle class imbalance\")\n",
    "        \n",
    "        bdt_final = XGBClassifier(\n",
    "            objective=\"binary:logistic\",\n",
    "            eval_metric=\"logloss\",\n",
    "            max_delta_step=1,\n",
    "            scale_pos_weight=scale_pos_weight,  # Handle class imbalance\n",
    "            random_state=RANDOM_STATE,\n",
    "            n_estimators=N_ESTIMATORS,\n",
    "            missing=np.nan  # Handle NaN values as missing\n",
    "        )\n",
    "        \n",
    "        bdt_final.fit(X_train, y_train)\n",
    "    \n",
    "    print(\"\\n✓ BDT training complete\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    return bdt_final, X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_bdt_and_save(bdt, mode_variables, mode, df_signal, df_generic):\n",
    "    \"\"\"\n",
    "    Apply trained BDT to signal and generic MC, and save to ROOT files.\n",
    "    \n",
    "    Parameters:\n",
    "        bdt: Trained XGBoost classifier\n",
    "        mode_variables: List of variable names used in training\n",
    "        mode: Decay mode\n",
    "        df_signal: Signal MC DataFrame\n",
    "        df_generic: Generic MC DataFrame\n",
    "    \"\"\"\n",
    "    import uproot\n",
    "    \n",
    "    if not SAVE_OUTPUT:\n",
    "        print(\"\\nSAVE_OUTPUT is False. Skipping file saving.\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"SAVING OUTPUT FILES\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Create output directory\n",
    "    output_dir = os.path.join(OUTPUT_BASE_DIR, \"FakeD0_BDT\", mode)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Save signal with BDT\n",
    "    signal_path = os.path.join(output_dir, f\"Ds2D0enu-Signal_{mode}_withBDT.root\")\n",
    "    print(f\"\\nSaving signal MC to: {signal_path}\")\n",
    "    \n",
    "    config = DECAY_CONFIG[mode]\n",
    "    tree_name = config[\"ds_tree\"]\n",
    "    \n",
    "    with uproot.recreate(signal_path) as f:\n",
    "        f[tree_name] = df_signal\n",
    "    \n",
    "    print(f\"  ✓ Saved {len(df_signal):,} signal events\")\n",
    "    \n",
    "    # Save generic with BDT\n",
    "    generic_path = os.path.join(output_dir, f\"Ds2D0e-Generic_{mode}_withBDT.root\")\n",
    "    print(f\"\\nSaving generic MC to: {generic_path}\")\n",
    "    \n",
    "    with uproot.recreate(generic_path) as f:\n",
    "        f[tree_name] = df_generic\n",
    "    \n",
    "    print(f\"  ✓ Saved {len(df_generic):,} generic events\")\n",
    "    print(\"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training Function\n",
    "\n",
    "Train XGBoost BDT with optional hyperparameter optimization.\n",
    "\n",
    "**Hyperparameter search space** (when `RUN_OPTIMIZATION = True`):\n",
    "- `learning_rate`: [0.01, 0.2]\n",
    "- `max_depth`: [1, 5]\n",
    "- `n_estimators`: [100, 200]\n",
    "- `reg_lambda`: [1, 5] (L2 regularization)\n",
    "- `gamma`: [0, 4] (min loss reduction for split)\n",
    "- `subsample`: [0.5, 1.0]\n",
    "- `min_child_weight`: [1, 6]\n",
    "- `colsample_bytree`: [0.3, 1.0]\n",
    "\n",
    "Uses `RandomizedSearchCV` with 5-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_mode(mode):\n",
    "    \"\"\"\n",
    "    Complete BDT training pipeline for a single mode.\n",
    "    \n",
    "    Parameters:\n",
    "        mode: Decay mode to process\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"█\"*80)\n",
    "    print(f\"PROCESSING MODE: {mode}\")\n",
    "    print(\"█\"*80 + \"\\n\")\n",
    "    \n",
    "    # Get proper title\n",
    "    mode_title = MODE_TITLES.get(mode, mode)\n",
    "    print(f\"Decay chain: {mode_title}\")\n",
    "    \n",
    "    # 1. Get variable list for this mode\n",
    "    if mode not in VARIABLES:\n",
    "        print(f\"ERROR: No variables defined for mode {mode}\")\n",
    "        return\n",
    "    \n",
    "    mode_variables = VARIABLES[mode][\"all_vars\"]\n",
    "    print(f\"Using {len(mode_variables)} variables from final_variables.py\")\n",
    "    \n",
    "    # 2. Load data\n",
    "    df_signal, df_generic = load_mode_data(mode)\n",
    "    \n",
    "    if df_signal.empty or df_generic.empty:\n",
    "        print(f\"ERROR: Failed to load data for {mode}\")\n",
    "        return\n",
    "    \n",
    "    # 3. Create training set\n",
    "    X, y, df_train, df_real, df_fake, mode_variables_filtered = create_training_set(\n",
    "        df_signal, df_generic, mode_variables\n",
    "    )\n",
    "    \n",
    "    # 4. Train BDT\n",
    "    bdt_final, X_train, X_test, y_train, y_test = train_bdt(X, y, run_optimization=RUN_OPTIMIZATION)\n",
    "    \n",
    "    # 5. BDT Statistics and overtraining metrics\n",
    "    print_bdt_statistics(bdt_final, X_train, y_train, X_test, y_test)\n",
    "    \n",
    "    # 6. Feature Importance\n",
    "    if SAVE_IMAGES:\n",
    "        save_dir = os.path.join(OUTPUT_DIR, mode)\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "    else:\n",
    "        save_dir = None\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"FEATURE IMPORTANCE\")\n",
    "    print(f\"\\nTotal variables being used for training: {len(mode_variables_filtered)}\")\n",
    "    print(\"\\nVerifying all variables from final_variables.py are included...\")\n",
    "    \n",
    "    # Check if all expected variables are present\n",
    "    expected_vars = set(VARIABLES[mode][\"all_vars\"])\n",
    "    actual_vars = set(mode_variables_filtered)\n",
    "    \n",
    "    missing_vars = expected_vars - actual_vars\n",
    "    if missing_vars:\n",
    "        print(f\"  ⚠ {len(missing_vars)} variables from final_variables.py not found in data:\")\n",
    "        for v in list(missing_vars)[:10]:\n",
    "            print(f\"    - {v}\")\n",
    "        if len(missing_vars) > 10:\n",
    "            print(f\"    ... and {len(missing_vars)-10} more\")\n",
    "    else:\n",
    "        print(f\"  ✓ All {len(expected_vars)} variables from final_variables.py are being used\")\n",
    "    print(\"=\"*80)\n",
    "    feature_imp_df = plot_feature_importance(\n",
    "        bdt_final, mode_variables_filtered, mode, num=20, save_dir=save_dir\n",
    "    )\n",
    "    print(\"\\nTop 10 Most Important Features:\")\n",
    "    print(feature_imp_df.sort_values(by=\"Value\", ascending=False).head(10))\n",
    "    \n",
    "    # 7. BDT Output Comparison with Pull Plots\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"BDT OUTPUT COMPARISON (Train vs Test with Pull Plots)\")\n",
    "    print(\"=\"*80)\n",
    "    decisions = compare_train_test(bdt_final, X_train, y_train, X_test, y_test, mode)\n",
    "    \n",
    "    if save_dir:\n",
    "        plt.savefig(os.path.join(save_dir, \"bdt_output_comparison.png\"), dpi=150, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()\n",
    "    \n",
    "    # 8. ROC Curve (keep existing Belle II style)\n",
    "    print(\"\\nPlotting ROC curve...\")\n",
    "    y_score_test = bdt_final.predict_proba(X_test)[:, 1]\n",
    "    fpr_test, tpr_test, _ = roc_curve(y_test, y_score_test)\n",
    "    area_test = auc(fpr_test, tpr_test)\n",
    "    \n",
    "    y_score_train = bdt_final.predict_proba(X_train)[:, 1]\n",
    "    fpr_train, tpr_train, _ = roc_curve(y_train, y_score_train)\n",
    "    area_train = auc(fpr_train, tpr_train)\n",
    "    \n",
    "    # Compute background rejection vs signal efficiency\n",
    "    bdt_cuts = np.linspace(0, 1, 100)\n",
    "    sig_train = y_score_train[y_train == 1]\n",
    "    bkg_train = y_score_train[y_train == 0]\n",
    "    sig_test = y_score_test[y_test == 1]\n",
    "    bkg_test = y_score_test[y_test == 0]\n",
    "    \n",
    "    sig_eff_train = []\n",
    "    bkg_rej_train = []\n",
    "    sig_eff_test = []\n",
    "    bkg_rej_test = []\n",
    "    \n",
    "    for cut in bdt_cuts:\n",
    "        sig_eff_train.append(np.sum(sig_train > cut) / len(sig_train))\n",
    "        bkg_rej_train.append(1 - (np.sum(bkg_train > cut) / len(bkg_train)))\n",
    "        sig_eff_test.append(np.sum(sig_test > cut) / len(sig_test))\n",
    "        bkg_rej_test.append(1 - (np.sum(bkg_test > cut) / len(bkg_test)))\n",
    "    \n",
    "    fig, axs = plt.subplots(1, 1, figsize=(7, 6))\n",
    "    \n",
    "    axs.plot(bkg_rej_train, sig_eff_train, color='tab:blue', linewidth=2,\n",
    "            label=f'Train (AUC = {area_train:.2f})')\n",
    "    axs.plot(bkg_rej_test, sig_eff_test, color='tab:red', linestyle='--', linewidth=2,\n",
    "            label=f'Test (AUC = {area_test:.2f})')\n",
    "    \n",
    "    axs.fill_between(bkg_rej_test, sig_eff_train, sig_eff_test,\n",
    "                     where=(np.array(sig_eff_train) > np.array(sig_eff_test)),\n",
    "                     color='gray', alpha=0.2, label='Overfit Gap')\n",
    "    \n",
    "    axs.set_title(mode_title, loc='left')\n",
    "    axs.set_ylim(0, 1.05)\n",
    "    axs.set_xlim(0, 1.05)\n",
    "    axs.set_xlabel('Background rejection')\n",
    "    axs.set_ylabel('Signal efficiency')\n",
    "    axs.legend(loc='lower left')\n",
    "    axs.grid(True)\n",
    "    \n",
    "    if save_dir:\n",
    "        plt.savefig(os.path.join(save_dir, \"roc_curve_belle2.png\"), dpi=150, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()\n",
    "    \n",
    "    # 9. Optimize cut using Punzi FoM\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"OPTIMIZING BDT CUT USING PUNZI FoM\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Apply BDT to signal and generic\n",
    "    df_signal[\"Ds_FakeD0BDT\"] = bdt_final.predict_proba(\n",
    "        df_signal[mode_variables_filtered]\n",
    "    )[:, 1].astype(np.float32)\n",
    "    \n",
    "    df_generic[\"Ds_FakeD0BDT\"] = bdt_final.predict_proba(\n",
    "        df_generic[mode_variables_filtered]\n",
    "    )[:, 1].astype(np.float32)\n",
    "    \n",
    "    # Get real D0 from signal for Punzi FoM calculation\n",
    "    sig_real_mask = (abs(df_signal[\"D0_mcPDG\"]) == 421)\n",
    "    bdt_scores_sig = df_signal.loc[sig_real_mask, \"Ds_FakeD0BDT\"].values\n",
    "    bdt_scores_bkg = df_generic[\"Ds_FakeD0BDT\"].values\n",
    "    \n",
    "    # Compute Punzi FoM for different confidence levels\n",
    "    print(\"\\nOptimizing BDT cut for different confidence levels:\\n\")\n",
    "    \n",
    "    best_cuts = {}\n",
    "    best_foms = {}\n",
    "    \n",
    "    for a_val in PUNZI_A_VALUES:\n",
    "        thresholds, foms, best_thresh, best_fom = compute_punzi_scan(\n",
    "            bdt_scores_sig, bdt_scores_bkg, N_SIGNAL_EVENTS, n_thresholds=200, a=a_val\n",
    "        )\n",
    "        \n",
    "        cl_name = {1.64: \"90% CL\", 1.96: \"95% CL\", 3.0: \"3σ\"}.get(a_val, f\"a={a_val}\")\n",
    "        best_cuts[cl_name] = best_thresh\n",
    "        best_foms[cl_name] = best_fom\n",
    "        print(f\"  {cl_name:8s}: Optimal cut = {best_thresh:.3f}, Punzi FoM = {best_fom:.6g}\")\n",
    "    \n",
    "    # Plot Punzi FoM curves for all confidence levels\n",
    "    if save_dir:\n",
    "        fig, ax = plt.subplots(figsize=(10, 7))\n",
    "        \n",
    "        # Colors: purple for 95% CL (main), black for others\n",
    "        colors = {\"90% CL\": \"black\", \"95% CL\": \"purple\", \"3σ\": \"black\"}\n",
    "        linestyles = {\"90% CL\": \"--\", \"95% CL\": \"-\", \"3σ\": \"-.\"}\n",
    "        linewidths = {\"90% CL\": 2, \"95% CL\": 3, \"3σ\": 2}\n",
    "        \n",
    "        for a_val in PUNZI_A_VALUES:\n",
    "            thresholds, foms, best_thresh, best_fom = compute_punzi_scan(\n",
    "                bdt_scores_sig, bdt_scores_bkg, N_SIGNAL_EVENTS, n_thresholds=200, a=a_val\n",
    "            )\n",
    "            \n",
    "            cl_name = {1.64: \"90% CL\", 1.96: \"95% CL\", 3.0: \"3σ\"}.get(a_val, f\"a={a_val}\")\n",
    "            \n",
    "            ax.plot(\n",
    "                thresholds, foms,\n",
    "                color=colors[cl_name],\n",
    "                linestyle=linestyles[cl_name],\n",
    "                linewidth=linewidths[cl_name],\n",
    "                label=f\"{cl_name} (cut={best_thresh:.3f})\"\n",
    "            )\n",
    "            \n",
    "            # Mark optimal point\n",
    "            ax.plot(best_thresh, best_fom, \"o\", color=colors[cl_name], markersize=8)\n",
    "        \n",
    "        ax.set_xlabel(\"BDT Cut\", color=\"black\")\n",
    "        ax.set_ylabel(\"Punzi FoM\", color=\"black\")\n",
    "        ax.set_title(mode_title, loc=\"left\")\n",
    "        ax.legend(loc=\"best\")\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Make sure tick labels are black\n",
    "        ax.tick_params(axis=\"x\", colors=\"black\")\n",
    "        ax.tick_params(axis=\"y\", colors=\"black\")\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(save_dir, \"punzi_fom_optimization.png\"), dpi=150, bbox_inches=\"tight\")\n",
    "        plt.close()  # Close to avoid display\n",
    "        \n",
    "        print(f\"\\n✓ Saved Punzi FoM plot to: {os.path.join(save_dir, 'punzi_fom_optimization.png')}\")\n",
    "    \n",
    "    print(\"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Training\n",
    "\n",
    "Execute the training pipeline for the selected mode(s).\n",
    "\n",
    "Set `TRAIN_MODE` in the configuration section to:\n",
    "- `\"kmpip\"` - Train only on K⁻ π⁺ mode\n",
    "- `\"km3pi\"` - Train only on K⁻ 3π mode\n",
    "- `\"kmpippi0_eff20_May2020\"` - Train only on K⁻ π⁺ π⁰ mode\n",
    "- `\"all\"` - Train on all modes sequentially"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "████████████████████████████████████████████████████████████████████████████████\n",
      "PROCESSING MODE: kmpip\n",
      "████████████████████████████████████████████████████████████████████████████████\n",
      "\n",
      "Decay chain: $D_s^{+} \\rightarrow [D^{0} \\rightarrow K^{-} \\pi^{+}] e^{+} \\nu_{e}$\n",
      "Using 18 variables from final_variables.py\n",
      "\n",
      "================================================================================\n",
      "Loading data for mode: kmpip\n",
      "Tree: DstreeCh1\n",
      "D⁰ mass cut: (-0.014291 <= D0_dM) & (D0_dM <= 0.014152)\n",
      "================================================================================\n",
      "\n",
      "Loading Signal MC from: /home/belle2/amubarak/C01-Simulated_Events/Signal/Ds2D0e-Signal_1_kmpip.root\n",
      "  → Signal MC: 16,530 events after D⁰ mass cut\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading generic MC:  17%|█▋        | 1/6 [01:18<06:30, 78.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  → ccbar: 311,413 events after D⁰ mass cut\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading generic MC:  33%|███▎      | 2/6 [01:30<02:37, 39.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  → charged: 2,167 events after D⁰ mass cut\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading generic MC:  50%|█████     | 3/6 [01:43<01:22, 27.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  → ddbar: 4,867 events after D⁰ mass cut\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading generic MC:  67%|██████▋   | 4/6 [01:53<00:41, 20.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  → mixed: 928 events after D⁰ mass cut\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading generic MC:  83%|████████▎ | 5/6 [02:06<00:17, 17.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  → ssbar: 9,847 events after D⁰ mass cut\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading generic MC: 100%|██████████| 6/6 [02:39<00:00, 26.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  → uubar: 22,558 events after D⁰ mass cut\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  → Total Generic MC: 351,780 events\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Creating training set with CORRECTED labeling\n",
      "================================================================================\n",
      "Real D⁰ from Signal MC: 16,408 events\n",
      "Real D⁰ from Generic MC: 293,106 events\n",
      "Total Real D⁰: 309,514 events\n",
      "Fake D⁰ from Generic MC: 58,674 events\n",
      "\n",
      "Training set: 368,188 events\n",
      "  - Real D⁰ (label=1): 309,514\n",
      "  - Fake D⁰ (label=0): 58,674\n",
      "  - Class ratio (Real/Fake): 5.275\n",
      "  - Number of features: 18\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "TRAINING BDT\n",
      "================================================================================\n",
      "Training set: 257,731 events\n",
      "Test set: 110,457 events\n",
      "\n",
      "Class balance in training set:\n",
      "  Real D⁰ (positive, label=1): 216,659 (84.1%)\n",
      "  Fake D⁰ (negative, label=0): 41,072 (15.9%)\n",
      "  Class ratio (Fake/Real): 0.19\n",
      "  XGBoost scale_pos_weight: 0.19 (for balancing)\n",
      "\n",
      "Training with default parameters (no optimization)...\n",
      "Using scale_pos_weight=0.19 to handle class imbalance\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TRAIN_MODE \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m# Train on all modes sequentially\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m DECAY_CONFIG\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m----> 4\u001b[0m         \u001b[43mprocess_mode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# Train on single mode\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m TRAIN_MODE \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m DECAY_CONFIG:\n",
      "Cell \u001b[0;32mIn[37], line 37\u001b[0m, in \u001b[0;36mprocess_mode\u001b[0;34m(mode)\u001b[0m\n\u001b[1;32m     32\u001b[0m X, y, df_train, df_real, df_fake, mode_variables_filtered \u001b[38;5;241m=\u001b[39m create_training_set(\n\u001b[1;32m     33\u001b[0m     df_signal, df_generic, mode_variables\n\u001b[1;32m     34\u001b[0m )\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# 4. Train BDT\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m bdt_final, X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_bdt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_optimization\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mRUN_OPTIMIZATION\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# 5. BDT Statistics and overtraining metrics\u001b[39;00m\n\u001b[1;32m     40\u001b[0m print_bdt_statistics(bdt_final, X_train, y_train, X_test, y_test)\n",
      "Cell \u001b[0;32mIn[35], line 98\u001b[0m, in \u001b[0;36mtrain_bdt\u001b[0;34m(X, y, run_optimization)\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing scale_pos_weight=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscale_pos_weight\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to handle class imbalance\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     88\u001b[0m     bdt_final \u001b[38;5;241m=\u001b[39m XGBClassifier(\n\u001b[1;32m     89\u001b[0m         objective\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary:logistic\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     90\u001b[0m         eval_metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogloss\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     95\u001b[0m         missing\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mnan  \u001b[38;5;66;03m# Handle NaN values as missing\u001b[39;00m\n\u001b[1;32m     96\u001b[0m     )\n\u001b[0;32m---> 98\u001b[0m     \u001b[43mbdt_final\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m✓ BDT training complete\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m80\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/env/lib64/python3.9/site-packages/xgboost/core.py:726\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    724\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[1;32m    725\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m--> 726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/env/lib64/python3.9/site-packages/xgboost/sklearn.py:1599\u001b[0m, in \u001b[0;36mXGBClassifier.fit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001b[0m\n\u001b[1;32m   1579\u001b[0m model, metric, params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_configure_fit(xgb_model, params)\n\u001b[1;32m   1580\u001b[0m train_dmatrix, evals \u001b[38;5;241m=\u001b[39m _wrap_evaluation_matrices(\n\u001b[1;32m   1581\u001b[0m     missing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmissing,\n\u001b[1;32m   1582\u001b[0m     X\u001b[38;5;241m=\u001b[39mX,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1596\u001b[0m     feature_types\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_types,\n\u001b[1;32m   1597\u001b[0m )\n\u001b[0;32m-> 1599\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1600\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1601\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dmatrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1602\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_num_boosting_rounds\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1603\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1604\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1605\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals_result\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals_result\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1606\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1607\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1608\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1609\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxgb_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1610\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1611\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective):\n\u001b[1;32m   1614\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective \u001b[38;5;241m=\u001b[39m params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobjective\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/env/lib64/python3.9/site-packages/xgboost/core.py:726\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    724\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[1;32m    725\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m--> 726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/env/lib64/python3.9/site-packages/xgboost/training.py:181\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mbefore_iteration(bst, i, dtrain, evals):\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m--> 181\u001b[0m \u001b[43mbst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miteration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mafter_iteration(bst, i, dtrain, evals):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/env/lib64/python3.9/site-packages/xgboost/core.py:2101\u001b[0m, in \u001b[0;36mBooster.update\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   2097\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_dmatrix_features(dtrain)\n\u001b[1;32m   2099\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2100\u001b[0m     _check_call(\n\u001b[0;32m-> 2101\u001b[0m         \u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mXGBoosterUpdateOneIter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2102\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43miteration\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\n\u001b[1;32m   2103\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2104\u001b[0m     )\n\u001b[1;32m   2105\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2106\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(dtrain, output_margin\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if TRAIN_MODE == \"all\":\n",
    "    # Train on all modes sequentially\n",
    "    for mode in DECAY_CONFIG.keys():\n",
    "        process_mode(mode)\n",
    "else:\n",
    "    # Train on single mode\n",
    "    if TRAIN_MODE not in DECAY_CONFIG:\n",
    "        raise ValueError(f\"Unknown mode: {TRAIN_MODE}. Choose from {list(DECAY_CONFIG.keys())} or 'all'\")\n",
    "    process_mode(TRAIN_MODE)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✓ ALL PROCESSING COMPLETE\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Control Sample Processing (Optional)\n",
    "\n",
    "Load and apply BDT to control samples for systematic studies.\n",
    "\n",
    "**Available control samples**:\n",
    "- `WCh`: Wrong charge (opposite sign combinations)\n",
    "- `ReverseID`: Reversed particle ID (e.g., π identified as K)\n",
    "- `ReverseID_WCh`: Combination of both\n",
    "\n",
    "**Note**: This section is currently disabled. Set `LOAD_CONTROL_SAMPLES = True` in the configuration to enable.\n",
    "\n",
    "Control samples are useful for:\n",
    "- Validating BDT performance on data-driven backgrounds\n",
    "- Estimating systematic uncertainties\n",
    "- Cross-checking signal/background discrimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOAD_CONTROL_SAMPLES:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"PROCESSING CONTROL SAMPLES\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Get the trained BDT (need to reload or keep from previous cell)\n",
    "    # For now, we'll process control samples separately per mode\n",
    "    \n",
    "    for control_sample in CONTROL_SAMPLES:\n",
    "        print(f\"\\nProcessing control sample: {control_sample}\")\n",
    "        \n",
    "        # Load data\n",
    "        df_signal_ctrl, df_generic_ctrl = load_mode_data(TRAIN_MODE, control_sample=control_sample)\n",
    "        \n",
    "        if df_signal_ctrl.empty or df_generic_ctrl.empty:\n",
    "            print(f\"  ✗ No data for control sample {control_sample}\")\n",
    "            continue\n",
    "        \n",
    "        # Get variables for this mode\n",
    "        mode_variables = VARIABLES[TRAIN_MODE][\"all_vars\"]\n",
    "        mode_variables_filtered = [v for v in mode_variables if v in df_signal_ctrl.columns]\n",
    "        \n",
    "        # Apply BDT (would need to reload trained model from saved file)\n",
    "        # For demonstration, assuming we have bdt_final from main training\n",
    "        # In practice, you'd save the model and reload it here\n",
    "        \n",
    "        print(f\"  Loaded {len(df_signal_ctrl):,} signal events\")\n",
    "        print(f\"  Loaded {len(df_generic_ctrl):,} generic events\")\n",
    "        \n",
    "        # Save outputs\n",
    "        if SAVE_OUTPUT:\n",
    "            output_dir_ctrl = os.path.join(OUTPUT_BASE_DIR, f\"FakeD0_{control_sample}\", TRAIN_MODE)\n",
    "            os.makedirs(output_dir_ctrl, exist_ok=True)\n",
    "            \n",
    "            # Save signal\n",
    "            signal_path = os.path.join(output_dir_ctrl, f\"Ds2D0enu-Signal_{TRAIN_MODE}_withBDT.root\")\n",
    "            with uproot.recreate(signal_path) as f:\n",
    "                f[\"Dstree\"] = df_signal_ctrl\n",
    "            print(f\"  Saved: {signal_path}\")\n",
    "            \n",
    "            # Save generic\n",
    "            generic_path = os.path.join(output_dir_ctrl, f\"Ds2D0e-Generic_{TRAIN_MODE}_withBDT.root\")\n",
    "            with uproot.recreate(generic_path) as f:\n",
    "                f[\"Dstree\"] = df_generic_ctrl\n",
    "            print(f\"  Saved: {generic_path}\")\n",
    "        \n",
    "        # Clear memory\n",
    "        del df_signal_ctrl, df_generic_ctrl\n",
    "        gc.collect()\n",
    "        \n",
    "    print(\"\\n✓ Control sample processing complete\")\n",
    "else:\n",
    "    print(\"\\nControl sample processing is disabled (LOAD_CONTROL_SAMPLES = False)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Control Sample / Data-MC Comparison (Optional)\n",
    "\n",
    "This section is for validating the BDT on control samples or sideband data.\n",
    "\n",
    "**Use cases**:\n",
    "1. **Control Samples**: Load control samples (WCh, ReverseID, etc.) and apply the trained BDT\n",
    "2. **Sideband Data**: Load signal region sideband data for data-MC comparison\n",
    "3. **BDT Output Comparison**: Compare BDT distributions between MC and data/control samples\n",
    "4. **Variable Comparison**: Compare input variables between MC and control samples\n",
    "\n",
    "**Available control samples**:\n",
    "- `WCh`: Wrong charge (opposite sign combinations)\n",
    "- `ReverseID`: Reversed particle ID (e.g., π identified as K)\n",
    "- `ReverseID_WCh`: Combination of both\n",
    "\n",
    "**Note**: Set `LOAD_CONTROL_SAMPLES = True` in the configuration to enable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell provides a template for data-MC comparison using control samples\n",
    "# Modify as needed for your specific analysis\n",
    "\n",
    "if LOAD_CONTROL_SAMPLES:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"DATA-MC COMPARISON WITH CONTROL SAMPLES\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # TODO: Load the trained BDT model for the mode you want to validate\n",
    "    # You may need to save/load the model from the training step\n",
    "    # For now, assuming you have bdt_final and mode_variables_filtered from training\n",
    "    \n",
    "    for control_sample in CONTROL_SAMPLES:\n",
    "        print(f\"\\n{'-'*80}\")\n",
    "        print(f\"Processing control sample: {control_sample}\")\n",
    "        print(f\"{'-'*80}\")\n",
    "        \n",
    "        # Load control sample data\n",
    "        df_signal_ctrl, df_generic_ctrl = load_mode_data(TRAIN_MODE, control_sample=control_sample)\n",
    "        \n",
    "        if df_signal_ctrl.empty or df_generic_ctrl.empty:\n",
    "            print(f\"  ✗ No data for control sample {control_sample}\")\n",
    "            continue\n",
    "        \n",
    "        # Filter variables (same as training)\n",
    "        mode_variables_ctrl = filter_valid_variables(\n",
    "            pd.concat([df_signal_ctrl, df_generic_ctrl], ignore_index=True),\n",
    "            VARIABLES[TRAIN_MODE][\"all_vars\"],\n",
    "            nan_threshold=0.5\n",
    "        )\n",
    "        \n",
    "        print(f\"  Valid variables: {len(mode_variables_ctrl)}\")\n",
    "        \n",
    "        # Apply BDT to control samples (requires trained model)\n",
    "        # df_signal_ctrl[\"Ds_FakeD0BDT\"] = bdt_final.predict_proba(\n",
    "        #     df_signal_ctrl[mode_variables_ctrl]\n",
    "        # )[:, 1].astype(np.float32)\n",
    "        \n",
    "        # df_generic_ctrl[\"Ds_FakeD0BDT\"] = bdt_final.predict_proba(\n",
    "        #     df_generic_ctrl[mode_variables_ctrl]\n",
    "        # )[:, 1].astype(np.float32)\n",
    "        \n",
    "        # Example: Compare variable distributions (MC vs control sample)\n",
    "        # Select a few important variables to compare\n",
    "        # vars_to_compare = mode_variables_ctrl[:5]  # First 5 variables\n",
    "        \n",
    "        # for var in vars_to_compare:\n",
    "        #     plt.figure(figsize=(10, 6))\n",
    "        #     \n",
    "        #     # Plot MC\n",
    "        #     plt.hist(df_signal[var].dropna(), bins=50, alpha=0.5, \n",
    "        #              label='MC', density=True, histtype='step', linewidth=2)\n",
    "        #     \n",
    "        #     # Plot control sample\n",
    "        #     plt.hist(df_signal_ctrl[var].dropna(), bins=50, alpha=0.5,\n",
    "        #              label=f'Control ({control_sample})', density=True, \n",
    "        #              histtype='step', linewidth=2)\n",
    "        #     \n",
    "        #     plt.xlabel(var)\n",
    "        #     plt.ylabel('Event Density')\n",
    "        #     plt.legend()\n",
    "        #     plt.title(f'{MODE_TITLES[TRAIN_MODE]} - {var}', loc='left')\n",
    "        #     plt.show()\n",
    "        \n",
    "        # Example: Compare BDT output distributions\n",
    "        # plt.figure(figsize=(10, 6))\n",
    "        # plt.hist(df_signal[\"Ds_FakeD0BDT\"], bins=50, alpha=0.5,\n",
    "        #          label='MC', density=True, range=(0, 1), histtype='step', linewidth=2)\n",
    "        # plt.hist(df_signal_ctrl[\"Ds_FakeD0BDT\"], bins=50, alpha=0.5,\n",
    "        #          label=f'Control ({control_sample})', density=True, range=(0, 1),\n",
    "        #          histtype='step', linewidth=2)\n",
    "        # plt.xlabel('Fake D⁰ BDT Output')\n",
    "        # plt.ylabel('Event Density')\n",
    "        # plt.legend()\n",
    "        # plt.title(f'{MODE_TITLES[TRAIN_MODE]} - BDT Output Comparison', loc='left')\n",
    "        # plt.show()\n",
    "        \n",
    "        print(f\"  Loaded {len(df_signal_ctrl):,} signal events\")\n",
    "        print(f\"  Loaded {len(df_generic_ctrl):,} generic events\")\n",
    "        \n",
    "        # Save outputs if requested\n",
    "        if SAVE_OUTPUT:\n",
    "            output_dir_ctrl = os.path.join(OUTPUT_BASE_DIR, f\"FakeD0_{control_sample}\", TRAIN_MODE)\n",
    "            os.makedirs(output_dir_ctrl, exist_ok=True)\n",
    "            \n",
    "            # Save signal\n",
    "            signal_path = os.path.join(output_dir_ctrl, f\"Ds2D0enu-Signal_{TRAIN_MODE}_withBDT.root\")\n",
    "            with uproot.recreate(signal_path) as f:\n",
    "                f[\"Dstree\"] = df_signal_ctrl\n",
    "            print(f\"  Saved: {signal_path}\")\n",
    "            \n",
    "            # Save generic\n",
    "            generic_path = os.path.join(output_dir_ctrl, f\"Ds2D0e-Generic_{TRAIN_MODE}_withBDT.root\")\n",
    "            with uproot.recreate(generic_path) as f:\n",
    "                f[\"Dstree\"] = df_generic_ctrl\n",
    "            print(f\"  Saved: {generic_path}\")\n",
    "        \n",
    "        # Clear memory\n",
    "        del df_signal_ctrl, df_generic_ctrl\n",
    "        gc.collect()\n",
    "        \n",
    "    print(\"\\n✓ Control sample processing complete\")\n",
    "else:\n",
    "    print(\"\\nControl sample processing is disabled (LOAD_CONTROL_SAMPLES = False)\")\n",
    "    print(\"Set LOAD_CONTROL_SAMPLES = True in the configuration to enable data-MC comparison.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Belle II PyROOT (ROOT 6.36.02, Py 3.9) [env]",
   "language": "python",
   "name": "belle2-pyroot-py39-root63602"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
