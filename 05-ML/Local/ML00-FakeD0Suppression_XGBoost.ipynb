{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fake D⁰ BDT Training - Multi-Mode Implementation\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook trains an XGBoost BDT to suppress fake D⁰ backgrounds using the **corrected labeling procedure**:\n",
    "\n",
    "- **Real D⁰**: `abs(D0_mcPDG) == 421` from BOTH signal MC AND generic MC\n",
    "- **Fake D⁰**: `abs(D0_mcPDG) != 421` or NaN from generic MC\n",
    "\n",
    "**Key Features**:\n",
    "- Multi-mode support: kmpip, km3pi, kmpippi0_eff20_May2020\n",
    "- Uses ALL variables from `final_variables.py` (278, 478, 788 vars per mode)\n",
    "- Sequential processing to avoid memory bloat\n",
    "- Punzi FoM for cut optimization (90% CL, 95% CL, 3σ)\n",
    "- Configurable hyperparameter optimization toggle\n",
    "- Control sample support with toggle\n",
    "- Comprehensive plotting and diagnostics\n",
    "- Automatic saving to `/group/belle/users/amubarak/04-ML/`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System Information\n",
    "\n",
    "Check available CPU cores and RAM before processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T07:14:38.093991Z",
     "iopub.status.busy": "2025-12-12T07:14:38.093862Z",
     "iopub.status.idle": "2025-12-12T07:14:38.100034Z",
     "shell.execute_reply": "2025-12-12T07:14:38.099707Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T07:14:38.122091Z",
     "iopub.status.busy": "2025-12-12T07:14:38.121911Z",
     "iopub.status.idle": "2025-12-12T07:14:38.124777Z",
     "shell.execute_reply": "2025-12-12T07:14:38.124489Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total RAM: 810.42 GB\n",
      "Available: 263.07 GB\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "\n",
    "mem = psutil.virtual_memory()\n",
    "print(f\"Total RAM: {mem.total / 1e9:.2f} GB\")\n",
    "print(f\"Available: {mem.available / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T07:14:38.125879Z",
     "iopub.status.busy": "2025-12-12T07:14:38.125768Z",
     "iopub.status.idle": "2025-12-12T07:14:48.274626Z",
     "shell.execute_reply": "2025-12-12T07:14:48.273991Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import gc\n",
    "import seaborn as sns\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import uproot\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import auc, roc_curve, confusion_matrix, classification_report\n",
    "from sklearn.metrics import precision_recall_curve, mean_squared_error, accuracy_score, roc_auc_score\n",
    "from sklearn.model_selection import GridSearchCV, cross_validate, validation_curve\n",
    "from sklearn.model_selection import train_test_split, KFold, learning_curve, cross_val_score\n",
    "from sklearn.utils import compute_sample_weight\n",
    "from scipy.stats import ks_2samp, randint, uniform\n",
    "\n",
    "import xgboost\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T07:14:48.276760Z",
     "iopub.status.busy": "2025-12-12T07:14:48.276463Z",
     "iopub.status.idle": "2025-12-12T07:14:48.279084Z",
     "shell.execute_reply": "2025-12-12T07:14:48.278687Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.rcParams.update({\n",
    "    \"axes.labelsize\": 16,\n",
    "    \"xtick.labelsize\": 12,\n",
    "    \"ytick.labelsize\": 12,\n",
    "    \"legend.fontsize\": 14,\n",
    "    \"figure.titlesize\": 20\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T07:14:48.280687Z",
     "iopub.status.busy": "2025-12-12T07:14:48.280523Z",
     "iopub.status.idle": "2025-12-12T07:14:48.284506Z",
     "shell.execute_reply": "2025-12-12T07:14:48.284118Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 200000)\n",
    "pd.set_option('display.max_columns', 200000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T07:14:48.285913Z",
     "iopub.status.busy": "2025-12-12T07:14:48.285769Z",
     "iopub.status.idle": "2025-12-12T07:14:48.381284Z",
     "shell.execute_reply": "2025-12-12T07:14:48.380705Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported custom functions successfully\n"
     ]
    }
   ],
   "source": [
    "sys.path.append(\"/home/belle2/amubarak/Ds2D0enue_Analysis/08-Python_Functions/\")\n",
    "sys.path.append(\"/home/belle2/amubarak/Ds2D0enue_Analysis/05-ML/Variables/\")\n",
    "\n",
    "# Import custom functions\n",
    "from Functions import optimize_cut, plot_save\n",
    "from Ds2D0e_config import DECAY_CONFIG, BACKGROUND_SAMPLES, get_signal_file, get_generic_file\n",
    "from final_variables import VARIABLES\n",
    "\n",
    "print(\"Imported custom functions successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Set the parameters for BDT training, data loading, and output saving.\n",
    "\n",
    "**Important toggles**:\n",
    "- `LOAD_CONTROL_SAMPLES`: Enable/disable control sample processing (WCh, ReverseID, etc.)\n",
    "- `RUN_OPTIMIZATION`: Enable/disable hyperparameter optimization via RandomizedSearchCV\n",
    "- `SAVE_OUTPUT`: Enable/disable ROOT file output saving\n",
    "- `SAVE_IMAGES`: Enable/disable plot saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T07:14:48.383012Z",
     "iopub.status.busy": "2025-12-12T07:14:48.382830Z",
     "iopub.status.idle": "2025-12-12T07:14:48.388177Z",
     "shell.execute_reply": "2025-12-12T07:14:48.387713Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded:\n",
      "  Train mode: all\n",
      "  Load control samples: False\n",
      "  Save images: True\n",
      "  Save output: False\n",
      "  Run hyperparameter optimization: True\n",
      "  N signal events: 100,000\n",
      "  Luminosity: 200 fb^-1\n"
     ]
    }
   ],
   "source": [
    "# ========================================================================\n",
    "# CONFIGURATION SECTION\n",
    "# ========================================================================\n",
    "\n",
    "# === Mode Selection ===\n",
    "# Set to specific mode: \"kmpip\", \"km3pi\", \"kmpippi0_eff20_May2020\"\n",
    "# Set to \"all\" to train on all modes sequentially\n",
    "TRAIN_MODE = \"all\"  # Change this to train different modes\n",
    "\n",
    "# === Control Samples ===\n",
    "# Toggle for loading control samples (noEID, wrongCharge, reverseID)\n",
    "LOAD_CONTROL_SAMPLES = False  # Set to True to load control samples\n",
    "CONTROL_SAMPLES = [\"WCh\", \"ReverseID\", \"ReverseID_WCh\"]  # Which control samples to load\n",
    "\n",
    "# === Image Saving ===\n",
    "SAVE_IMAGES = True  # Set to True to save all plots\n",
    "OUTPUT_DIR = \"/home/belle2/amubarak/Ds2D0enue_Analysis/05-ML/Figures/FakeD0_BDT\"\n",
    "\n",
    "# === ROOT File Output Saving ===\n",
    "SAVE_OUTPUT = False  # Set to True to save DataFrames with BDT to ROOT files\n",
    "OUTPUT_BASE_DIR = \"/group/belle/users/amubarak/04-ML/\"\n",
    "\n",
    "# === Normalization Parameters for Punzi FoM ===\n",
    "N_SIGNAL_EVENTS = 100_000  # Number of signal MC events generated\n",
    "LUMINOSITY_FB = 200  # Integrated luminosity in fb^-1\n",
    "PUNZI_A_VALUES = [1.64, 1.96, 3.0]  # 90% CL, 95% CL, 3σ discovery\n",
    "\n",
    "# === BDT Training Parameters ===\n",
    "RANDOM_STATE = 42\n",
    "TEST_SIZE = 0.30\n",
    "N_ESTIMATORS = 100\n",
    "\n",
    "# === Hyperparameter Optimization Toggle ===\n",
    "RUN_OPTIMIZATION = True  # Set to False to skip hyperparameter optimization\n",
    "RANDOM_SEARCH_ITERS = 50  # Number of random search iterations (only if RUN_OPTIMIZATION=True)\n",
    "\n",
    "print(f\"Configuration loaded:\")\n",
    "print(f\"  Train mode: {TRAIN_MODE}\")\n",
    "print(f\"  Load control samples: {LOAD_CONTROL_SAMPLES}\")\n",
    "print(f\"  Save images: {SAVE_IMAGES}\")\n",
    "print(f\"  Save output: {SAVE_OUTPUT}\")\n",
    "print(f\"  Run hyperparameter optimization: {RUN_OPTIMIZATION}\")\n",
    "print(f\"  N signal events: {N_SIGNAL_EVENTS:,}\")\n",
    "print(f\"  Luminosity: {LUMINOSITY_FB} fb^-1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data Configuration and Variables\n",
    "\n",
    "Load decay mode configurations from `Ds2D0e_config.py` and the final variable lists from `final_variables.py`.\n",
    "\n",
    "**Variable counts per mode**:\n",
    "- kmpip: 278 variables\n",
    "- km3pi: 478 variables  \n",
    "- kmpippi0_eff20_May2020: 788 variables\n",
    "\n",
    "We use ALL variables from `final_variables.py` without reduction at this stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T07:14:48.389726Z",
     "iopub.status.busy": "2025-12-12T07:14:48.389545Z",
     "iopub.status.idle": "2025-12-12T07:14:48.429048Z",
     "shell.execute_reply": "2025-12-12T07:14:48.428483Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available decay modes:\n",
      "  - kmpip\n",
      "  - km3pi\n",
      "  - kmpippi0_eff20_May2020\n",
      "\n",
      "Variable counts per mode:\n",
      "  kmpip: 12 variables\n",
      "  km3pi: 25 variables\n",
      "  kmpippi0_eff20_May2020: 18 variables\n"
     ]
    }
   ],
   "source": [
    "from Ds2D0e_config import DECAY_CONFIG, BACKGROUND_SAMPLES, get_signal_file, get_generic_file\n",
    "from final_variables import VARIABLES\n",
    "\n",
    "print(\"Available decay modes:\")\n",
    "for mode in DECAY_CONFIG.keys():\n",
    "    print(f\"  - {mode}\")\n",
    "\n",
    "print(\"\\nVariable counts per mode:\")\n",
    "for mode in VARIABLES.keys():\n",
    "    print(f\"  {mode}: {len(VARIABLES[mode]['all_vars'])} variables\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions\n",
    "\n",
    "### Punzi Figure of Merit\n",
    "\n",
    "The Punzi FoM is used for cut optimization in the presence of systematic uncertainties:\n",
    "\n",
    "$$\\text{Punzi FoM} = \\frac{\\epsilon_S}{\\frac{a}{2} + \\sqrt{B}}$$\n",
    "\n",
    "where:\n",
    "- $\\epsilon_S$ is the signal efficiency\n",
    "- $B$ is the number of background events\n",
    "- $a$ is the confidence level parameter:\n",
    "  - $a = 1.64$ for 90% CL\n",
    "  - $a = 1.96$ for 95% CL\n",
    "  - $a = 3.0$ for 3σ discovery\n",
    "\n",
    "This is preferred over the simple $S/\\sqrt{S+B}$ FoM when systematic uncertainties are significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T07:14:48.430624Z",
     "iopub.status.busy": "2025-12-12T07:14:48.430468Z",
     "iopub.status.idle": "2025-12-12T07:14:48.435878Z",
     "shell.execute_reply": "2025-12-12T07:14:48.435378Z"
    }
   },
   "outputs": [],
   "source": [
    "def punzi_fom(epsS, B, a=1.96):\n",
    "    \"\"\"\n",
    "    Compute Punzi figure of merit.\n",
    "    \n",
    "    Parameters:\n",
    "        epsS (float): Signal efficiency\n",
    "        B (float): Number of background events\n",
    "        a (float): Confidence level parameter (default 1.96 for 95% CL)\n",
    "    \n",
    "    Returns:\n",
    "        float: Punzi FoM = ε_S / (a/2 + √B)\n",
    "    \"\"\"\n",
    "    denom = (a / 2.0) + np.sqrt(max(B, 0.0))\n",
    "    return 0.0 if denom <= 0.0 else epsS / denom\n",
    "\n",
    "\n",
    "def compute_punzi_scan(bdt_scores_sig, bdt_scores_bkg, n_sig_total, n_thresholds=200, a=1.96):\n",
    "    \"\"\"\n",
    "    Scan BDT thresholds and compute Punzi FoM.\n",
    "    \n",
    "    Parameters:\n",
    "        bdt_scores_sig (array): BDT scores for signal events\n",
    "        bdt_scores_bkg (array): BDT scores for background events\n",
    "        n_sig_total (int): Total number of signal events (for efficiency calculation)\n",
    "        n_thresholds (int): Number of thresholds to scan\n",
    "        a (float): Confidence level parameter\n",
    "    \n",
    "    Returns:\n",
    "        thresholds, foms, best_threshold, best_fom\n",
    "    \"\"\"\n",
    "    thresholds = np.linspace(0, 1, n_thresholds)\n",
    "    foms = []\n",
    "    \n",
    "    for t in thresholds:\n",
    "        n_sig_pass = np.sum(bdt_scores_sig > t)\n",
    "        n_bkg_pass = np.sum(bdt_scores_bkg > t)\n",
    "        \n",
    "        epsS = n_sig_pass / n_sig_total if n_sig_total > 0 else 0.0\n",
    "        fom = punzi_fom(epsS, n_bkg_pass, a)\n",
    "        foms.append(fom)\n",
    "    \n",
    "    foms = np.array(foms)\n",
    "    best_idx = np.argmax(foms)\n",
    "    return thresholds, foms, thresholds[best_idx], foms[best_idx]\n",
    "\n",
    "\n",
    "def get_pulls(counts, errors, pdf):\n",
    "    \"\"\"\n",
    "    Compute pull values for comparing test vs train distributions.\n",
    "    \n",
    "    Parameters:\n",
    "        counts (array): Test counts\n",
    "        errors (array): Test errors (uncertainties)\n",
    "        pdf (array): Train probability density (normalized)\n",
    "    \n",
    "    Returns:\n",
    "        array: Pull values = (counts - pdf) / errors\n",
    "    \"\"\"\n",
    "    pull = (counts - pdf) / errors\n",
    "    return pull"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading Functions\n",
    "\n",
    "Functions to load signal and generic MC for each decay mode, applying D⁰ mass window cuts from the configuration.\n",
    "\n",
    "**Data sources**:\n",
    "- Signal MC: Ds → D⁰ e ν samples\n",
    "- Generic MC: Combined generic BB̄ backgrounds (charged, mixed, uubar, ddbar, ssbar, ccbar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T07:14:48.437710Z",
     "iopub.status.busy": "2025-12-12T07:14:48.437540Z",
     "iopub.status.idle": "2025-12-12T07:14:48.442346Z",
     "shell.execute_reply": "2025-12-12T07:14:48.441848Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mode titles loaded:\n",
      "  kmpip: $D_s^{+} \\rightarrow [D^{0} \\rightarrow K^{-} \\pi^{+}] e^{+} \\nu_{e}$\n",
      "  kmpippi0_eff20_May2020: $D_s^{+} \\rightarrow [D^{0} \\rightarrow K^{-} \\pi^{+} \\pi^{0}] e^{+} \\nu_{e}$\n",
      "  km3pi: $D_s^{+} \\rightarrow [D^{0} \\rightarrow K^{-} 3\\pi] e^{+} \\nu_{e}$\n"
     ]
    }
   ],
   "source": [
    "# Mode titles with proper LaTeX formatting for decay chains\n",
    "MODE_TITLES = {\n",
    "    \"kmpip\": r\"$D_s^{+} \\rightarrow [D^{0} \\rightarrow K^{-} \\pi^{+}] e^{+} \\nu_{e}$\",\n",
    "    \"kmpippi0_eff20_May2020\": r\"$D_s^{+} \\rightarrow [D^{0} \\rightarrow K^{-} \\pi^{+} \\pi^{0}] e^{+} \\nu_{e}$\",\n",
    "    \"km3pi\": r\"$D_s^{+} \\rightarrow [D^{0} \\rightarrow K^{-} 3\\pi] e^{+} \\nu_{e}$\",\n",
    "}\n",
    "\n",
    "print(\"Mode titles loaded:\")\n",
    "for mode, title in MODE_TITLES.items():\n",
    "    print(f\"  {mode}: {title}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T07:14:48.443799Z",
     "iopub.status.busy": "2025-12-12T07:14:48.443660Z",
     "iopub.status.idle": "2025-12-12T07:14:48.449234Z",
     "shell.execute_reply": "2025-12-12T07:14:48.448761Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_feature_importance(model, features, mode, num=None, save_dir=None):\n",
    "    \"\"\"\n",
    "    Plot feature importance from trained XGBoost model and save all importances to CSV.\n",
    "    \n",
    "    Parameters:\n",
    "        model: Trained XGBoost classifier\n",
    "        features: List of feature names\n",
    "        mode: Decay mode (for title)\n",
    "        num: Number of top features to show in plot (None = show all)\n",
    "        save_dir: Directory to save plot and CSV (None = show plot)\n",
    "    \"\"\"\n",
    "    feature_imp = pd.DataFrame({\n",
    "        'Value': model.feature_importances_,\n",
    "        'Feature': features\n",
    "    })\n",
    "    \n",
    "    # Sort by importance\n",
    "    feature_imp_sorted = feature_imp.sort_values(by=\"Value\", ascending=False)\n",
    "    \n",
    "    # Save complete table to CSV\n",
    "    if save_dir:\n",
    "        csv_path = os.path.join(save_dir, \"feature_importance_all.csv\")\n",
    "        feature_imp_sorted.to_csv(csv_path, index=False)\n",
    "        print(f\"\\n✓ Saved complete feature importance table to: {csv_path}\")\n",
    "    \n",
    "    # If num is specified, take top N for plotting, otherwise take all\n",
    "    if num is not None:\n",
    "        feature_imp_plot = feature_imp_sorted.head(num)\n",
    "        title_suffix = f\"Top {num} Feature Importances\"\n",
    "    else:\n",
    "        feature_imp_plot = feature_imp_sorted\n",
    "        title_suffix = f\"Feature Importances (All {len(features)} variables)\"\n",
    "    \n",
    "    # Adjust figure size based on number of features\n",
    "    n_features = len(feature_imp_plot)\n",
    "    fig_height = max(8, min(100, n_features * 0.3))  # At least 8, at most 100\n",
    "    \n",
    "    plt.figure(figsize=(16, fig_height))\n",
    "    sns.barplot(\n",
    "        x=\"Value\", y=\"Feature\",\n",
    "        data=feature_imp_plot\n",
    "    )\n",
    "    \n",
    "    title = MODE_TITLES.get(mode, mode)\n",
    "    plt.title(f'{title}\\n{title_suffix}', loc='left')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_dir:\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        if num is not None:\n",
    "            plt.savefig(os.path.join(save_dir, f\"feature_importance_top{num}.png\"), dpi=150, bbox_inches='tight')\n",
    "        else:\n",
    "            plt.savefig(os.path.join(save_dir, \"feature_importance_all.png\"), dpi=150, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()\n",
    "    \n",
    "    return feature_imp_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T07:14:48.450710Z",
     "iopub.status.busy": "2025-12-12T07:14:48.450571Z",
     "iopub.status.idle": "2025-12-12T07:14:48.460891Z",
     "shell.execute_reply": "2025-12-12T07:14:48.460390Z"
    }
   },
   "outputs": [],
   "source": [
    "def compare_train_test(clf, X_train, y_train, X_test, y_test, mode):\n",
    "    \"\"\"\n",
    "    Plot BDT output comparing train vs test with pull plots.\n",
    "    \n",
    "    Parameters:\n",
    "        clf: Trained classifier\n",
    "        X_train, y_train: Training data and labels\n",
    "        X_test, y_test: Test data and labels\n",
    "        mode: Decay mode (for title)\n",
    "    \n",
    "    Returns:\n",
    "        decisions: List of [train_bkg, train_sig, test_bkg, test_sig] BDT outputs\n",
    "    \"\"\"\n",
    "    decisions = []  # list to hold decisions of classifier\n",
    "    for X, y in ((X_train, y_train), (X_test, y_test)):  # train and test\n",
    "        if hasattr(clf, \"predict_proba\"):  # if predict_proba function exists\n",
    "            d1 = clf.predict_proba(X[y<0.5])[:, 1]  # background\n",
    "            d2 = clf.predict_proba(X[y>0.5])[:, 1]  # signal\n",
    "        else:  # predict_proba function doesn't exist\n",
    "            X_tensor = torch.as_tensor(X, dtype=torch.float)\n",
    "            y_tensor = torch.as_tensor(y, dtype=torch.long)\n",
    "            X_var, y_var = Variable(X_tensor), Variable(y_tensor)\n",
    "            d1 = clf(X_var[y_var<0.5])[1][:, 1].cpu().detach().numpy()  # background\n",
    "            d2 = clf(X_var[y_var>0.5])[1][:, 1].cpu().detach().numpy()  # signal\n",
    "        decisions += [d1, d2]  # add to list of classifier decision\n",
    "\n",
    "    lw = 3\n",
    "    fig, axs = plt.subplots(3, 1, figsize=(10, 10), gridspec_kw={'height_ratios':[1, 0.2, 0.2]})\n",
    "\n",
    "    bins = 50\n",
    "    bin_edges = np.linspace(0, 1, bins)\n",
    "    \n",
    "    test_bkg_count_weight = bins / len(decisions[2])\n",
    "    test_sig_count_weight = bins / len(decisions[3])\n",
    "    test_bkg_counts, test_bkg_bins = np.histogram(decisions[2], bins=bins, range=(0, 1))\n",
    "    test_sig_counts, test_sig_bins = np.histogram(decisions[3], bins=bins, range=(0, 1))\n",
    "\n",
    "    train_bkg_counts, train_bkg_bins, _etc = axs[0].hist(\n",
    "        decisions[0], color='tab:blue', histtype='step', bins=bins,\n",
    "        density=True, range=(0, 1), linewidth=lw, label='Train Background'\n",
    "    )\n",
    "    train_sig_counts, train_sig_bins, _etc = axs[0].hist(\n",
    "        decisions[1], color='tab:red', histtype='step', bins=bins,\n",
    "        density=True, range=(0, 1), linewidth=lw, label=r'Train Signal'\n",
    "    )\n",
    "    axs[0].hist(decisions[0], color='tab:blue', histtype='stepfilled',\n",
    "                alpha=0.4, bins=bins, density=True, range=(0, 1))\n",
    "    axs[0].hist(decisions[1], color='tab:red', histtype='stepfilled',\n",
    "                alpha=0.4, bins=bins, density=True, range=(0, 1))\n",
    "    \n",
    "    bin_width = test_bkg_bins[1] - test_bkg_bins[0]\n",
    "    bin_centers = [el + (bin_width/2) for el in test_bkg_bins[:-1]]\n",
    "\n",
    "    axs[0].errorbar(\n",
    "        bin_centers, test_bkg_count_weight * test_bkg_counts,\n",
    "        yerr=test_bkg_count_weight * np.sqrt(test_bkg_counts),\n",
    "        label='Test Background', color='tab:blue', marker='o', linewidth=lw, ls=''\n",
    "    )\n",
    "    axs[0].errorbar(\n",
    "        bin_centers, test_sig_count_weight * test_sig_counts,\n",
    "        yerr=test_sig_count_weight * np.sqrt(test_sig_counts),\n",
    "        label='Test Signal', color='tab:red', marker='o', linewidth=lw, ls=''\n",
    "    )\n",
    "    \n",
    "    # Title with proper LaTeX decay chain\n",
    "    title = MODE_TITLES.get(mode, mode)\n",
    "    axs[0].set_title(title, loc='left')\n",
    "    axs[0].set_xlim(0, 1)\n",
    "    axs[0].set_ylim(0)\n",
    "    axs[0].set_ylabel('Event Density')\n",
    "\n",
    "    # K-S test scores\n",
    "    ks_p_value_sig = ks_2samp(decisions[1], decisions[3])[1]\n",
    "    ks_p_value_bkg = ks_2samp(decisions[0], decisions[2])[1]\n",
    "\n",
    "    leg = axs[0].legend(\n",
    "        loc='upper center',\n",
    "        title=f\"Sig K-S test score: {ks_p_value_sig:0.3f}\\nBkg K-S test score: {ks_p_value_bkg:0.3f}\"\n",
    "    )\n",
    "    leg._legend_box.align = \"left\"\n",
    "\n",
    "    # Background pulls\n",
    "    pulls = get_pulls(\n",
    "        test_bkg_count_weight * test_bkg_counts,\n",
    "        test_bkg_count_weight * np.sqrt(test_bkg_counts),\n",
    "        np.array(train_bkg_counts)\n",
    "    )\n",
    "    axs[1].bar(bin_centers, pulls, width=bin_width)\n",
    "    axs[1].set_xlim(0, 1)\n",
    "    axs[1].set_ylabel('Pulls')\n",
    "    axs[1].set_ylim(-5, 5)\n",
    "\n",
    "    # Signal pulls\n",
    "    pulls = get_pulls(\n",
    "        test_sig_count_weight * test_sig_counts,\n",
    "        test_sig_count_weight * np.sqrt(test_sig_counts),\n",
    "        np.array(train_sig_counts)\n",
    "    )\n",
    "    axs[2].bar(bin_centers, pulls, width=bin_width, color='tab:red')\n",
    "    axs[2].set_xlim(0, 1)\n",
    "    axs[2].set_ylabel('Pulls')\n",
    "    axs[2].set_ylim(-5, 5)\n",
    "    axs[2].set_xlabel(r'BDT output')\n",
    "\n",
    "    return decisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T07:14:48.462407Z",
     "iopub.status.busy": "2025-12-12T07:14:48.462253Z",
     "iopub.status.idle": "2025-12-12T07:14:48.469630Z",
     "shell.execute_reply": "2025-12-12T07:14:48.469153Z"
    }
   },
   "outputs": [],
   "source": [
    "def filter_valid_variables(df, variable_list, nan_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Filter variable list to only include valid columns from dataframe.\n",
    "    \n",
    "    Removes variables that:\n",
    "    - Don't exist in the dataframe\n",
    "    - Are not numeric\n",
    "    - Have >50% NaN values (by default)\n",
    "    - Have any infinity values\n",
    "    \n",
    "    Parameters:\n",
    "        df: DataFrame to check\n",
    "        variable_list: List of variable names to filter\n",
    "        nan_threshold: Maximum fraction of NaN values allowed (0.5 = 50%)\n",
    "    \n",
    "    Returns:\n",
    "        List of valid variable names\n",
    "    \"\"\"\n",
    "    valid_vars = []\n",
    "    \n",
    "    for var in variable_list:\n",
    "        # Check if variable exists\n",
    "        if var not in df.columns:\n",
    "            continue\n",
    "        \n",
    "        # Check if numeric\n",
    "        if not pd.api.types.is_numeric_dtype(df[var]):\n",
    "            continue\n",
    "        \n",
    "        col_data = df[var]\n",
    "        \n",
    "        # Check NaN fraction\n",
    "        nan_fraction = col_data.isna().sum() / len(col_data)\n",
    "        if nan_fraction > nan_threshold:\n",
    "            print(f\"  ⚠ Skipping {var}: {nan_fraction*100:.1f}% NaN values\")\n",
    "            continue\n",
    "        \n",
    "        # Check for infinity\n",
    "        if np.isinf(col_data.dropna()).any():\n",
    "            print(f\"  ⚠ Skipping {var}: contains infinity values\")\n",
    "            continue\n",
    "        \n",
    "        valid_vars.append(var)\n",
    "    \n",
    "    return valid_vars\n",
    "\n",
    "\n",
    "def load_mode_data(mode, control_sample=None):\n",
    "    \"\"\"\n",
    "    Load signal and generic MC for a given decay mode.\n",
    "    Applies D⁰ mass window cuts from config.\n",
    "    \n",
    "    Parameters:\n",
    "        mode (str): Decay mode (kmpip, km3pi, kmpippi0_eff20_May2020)\n",
    "        control_sample (str): Control sample tag (\"WCh\", \"ReverseID\", etc.) or None for nominal\n",
    "    \n",
    "    Returns:\n",
    "        df_signal, df_generic (DataFrames)\n",
    "    \"\"\"\n",
    "    config = DECAY_CONFIG[mode]\n",
    "    tree_name = config[\"ds_tree\"]\n",
    "    mass_cut = config[\"cut\"]\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Loading data for mode: {mode}\")\n",
    "    if control_sample:\n",
    "        print(f\"Control sample: {control_sample}\")\n",
    "    print(f\"Tree: {tree_name}\")\n",
    "    print(f\"D⁰ mass cut: {mass_cut}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    # Load signal MC\n",
    "    use_control = control_sample is not None\n",
    "    control_tag = control_sample if control_sample else None\n",
    "    \n",
    "    signal_path = get_signal_file(mode, use_control_sample=use_control, control_sample_tag=control_tag)\n",
    "    if isinstance(signal_path, list):\n",
    "        signal_path = signal_path[0]  # Take first if multiple returned\n",
    "    \n",
    "    print(f\"Loading Signal MC from: {signal_path}\")\n",
    "    try:\n",
    "        df_signal = uproot.concatenate(f\"{signal_path}:{tree_name}\", library='pd')\n",
    "        df_signal = df_signal.query(mass_cut)  # Apply D⁰ mass window\n",
    "        print(f\"  → Signal MC: {len(df_signal):,} events after D⁰ mass cut\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ Failed to load signal: {e}\")\n",
    "        df_signal = pd.DataFrame()\n",
    "    \n",
    "    # Load generic MC (all background samples combined)\n",
    "    df_generic_list = []\n",
    "    for sample in tqdm(BACKGROUND_SAMPLES, desc=\"Loading generic MC\"):\n",
    "        generic_path = get_generic_file(sample, mode, use_control_sample=use_control, \n",
    "                                       control_sample_tag=control_tag)\n",
    "        if isinstance(generic_path, list):\n",
    "            generic_path = generic_path[0]\n",
    "        \n",
    "        try:\n",
    "            df = uproot.concatenate(f\"{generic_path}:{tree_name}\", library='pd')\n",
    "            df = df.query(mass_cut)  # Apply D⁰ mass window\n",
    "            df_generic_list.append(df)\n",
    "            print(f\"  → {sample}: {len(df):,} events after D⁰ mass cut\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ Failed to load {sample}: {e}\")\n",
    "    \n",
    "    df_generic = pd.concat(df_generic_list, ignore_index=True) if df_generic_list else pd.DataFrame()\n",
    "    print(f\"\\n  → Total Generic MC: {len(df_generic):,} events\\n\")\n",
    "    \n",
    "    return df_signal, df_generic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T07:14:48.471354Z",
     "iopub.status.busy": "2025-12-12T07:14:48.471176Z",
     "iopub.status.idle": "2025-12-12T07:14:48.523975Z",
     "shell.execute_reply": "2025-12-12T07:14:48.523481Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_pulls(test_data, test_errors, train_data):\n",
    "    \"\"\"\n",
    "    Calculate pulls for overtraining check.\n",
    "    \n",
    "    Parameters:\n",
    "        test_data: Test histogram values\n",
    "        test_errors: Test histogram errors\n",
    "        train_data: Train histogram values\n",
    "    \n",
    "    Returns:\n",
    "        pulls: (test_data - train_data) / test_errors array\n",
    "    \"\"\"\n",
    "    # Avoid division by zero\n",
    "    pulls = np.zeros(len(test_data))\n",
    "    mask = test_errors > 0\n",
    "    pulls[mask] = (test_data[mask] - train_data[mask]) / test_errors[mask]\n",
    "    return pulls\n",
    "\n",
    "\n",
    "def compute_punzi_scan(signal_scores, background_scores, n_sig_events, n_thresholds=100, a=1.96):\n",
    "    \"\"\"\n",
    "    Scan BDT cuts and compute Punzi Figure of Merit (FoM).\n",
    "    \n",
    "    Punzi FoM = n_sig / (a + sqrt(n_bkg))\n",
    "    \n",
    "    Parameters:\n",
    "        signal_scores: Array of BDT scores for signal events\n",
    "        background_scores: Array of BDT scores for background events\n",
    "        n_sig_events: Number of signal events (for FoM calculation)\n",
    "        n_thresholds: Number of threshold points to scan\n",
    "        a: Confidence level parameter (default 1.96 for 95% CL)\n",
    "    \n",
    "    Returns:\n",
    "        thresholds: Array of BDT cut values\n",
    "        foms: Array of FoM values\n",
    "        best_threshold: Optimal BDT cut\n",
    "        best_fom: Maximum FoM value\n",
    "    \"\"\"\n",
    "    thresholds = np.linspace(0, 1, n_thresholds)\n",
    "    foms = []\n",
    "    \n",
    "    for cut in thresholds:\n",
    "        # Count events passing cut\n",
    "        n_sig = np.sum(signal_scores > cut)\n",
    "        n_bkg = np.sum(background_scores > cut)\n",
    "        \n",
    "        # Compute FoM\n",
    "        if n_bkg < 0:\n",
    "            n_bkg = 0\n",
    "        \n",
    "        # Scale FoM to full dataset\n",
    "        if n_sig > 0:\n",
    "            fom = n_sig / np.sqrt(a**2 + n_bkg) if (a**2 + n_bkg) > 0 else 0\n",
    "        else:\n",
    "            fom = 0\n",
    "        \n",
    "        foms.append(fom)\n",
    "    \n",
    "    foms = np.array(foms)\n",
    "    best_idx = np.argmax(foms)\n",
    "    best_threshold = thresholds[best_idx]\n",
    "    best_fom = foms[best_idx]\n",
    "    \n",
    "    return thresholds, foms, best_threshold, best_fom\n",
    "\n",
    "\n",
    "def print_bdt_statistics(model, X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Print BDT training and test statistics.\n",
    "    \n",
    "    Parameters:\n",
    "        model: Trained XGBoost classifier\n",
    "        X_train, y_train: Training features and labels\n",
    "        X_test, y_test: Test features and labels\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"BDT STATISTICS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Get predictions\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    \n",
    "    y_proba_train = model.predict_proba(X_train)[:, 1]\n",
    "    y_proba_test = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Training metrics\n",
    "    train_acc = accuracy_score(y_train, y_pred_train)\n",
    "    train_auc = roc_auc_score(y_train, y_proba_train)\n",
    "    \n",
    "    # Test metrics\n",
    "    test_acc = accuracy_score(y_test, y_pred_test)\n",
    "    test_auc = roc_auc_score(y_test, y_proba_test)\n",
    "    \n",
    "    # Compute precision and recall\n",
    "    precision_train = confusion_matrix(y_train, y_pred_train)[1, 1] / (confusion_matrix(y_train, y_pred_train)[1, 1] + confusion_matrix(y_train, y_pred_train)[0, 1]) if (confusion_matrix(y_train, y_pred_train)[1, 1] + confusion_matrix(y_train, y_pred_train)[0, 1]) > 0 else 0\n",
    "    precision_test = confusion_matrix(y_test, y_pred_test)[1, 1] / (confusion_matrix(y_test, y_pred_test)[1, 1] + confusion_matrix(y_test, y_pred_test)[0, 1]) if (confusion_matrix(y_test, y_pred_test)[1, 1] + confusion_matrix(y_test, y_pred_test)[0, 1]) > 0 else 0\n",
    "    \n",
    "    print(f\"\\nTraining Set Performance:\")\n",
    "    print(f\"  Accuracy: {train_acc:.4f}\")\n",
    "    print(f\"  ROC AUC:  {train_auc:.4f}\")\n",
    "    print(f\"\\nTest Set Performance:\")\n",
    "    print(f\"  Accuracy: {test_acc:.4f}\")\n",
    "    print(f\"  ROC AUC:  {test_auc:.4f}\")\n",
    "    print(f\"\\nOvertraining Check (Test - Train):\")\n",
    "    print(f\"  ΔAccuracy: {test_acc - train_acc:+.4f}\")\n",
    "    print(f\"  ΔAUC:      {test_auc - train_auc:+.4f}\")\n",
    "    \n",
    "    # Check for overtraining\n",
    "    if abs(test_auc - train_auc) < 0.01 and abs(test_acc - train_acc) < 0.01:\n",
    "        print(f\"  ✓ Good agreement (no overtraining detected)\")\n",
    "    elif test_auc < train_auc or test_acc < train_acc:\n",
    "        print(f\"  ⚠ Slight overtraining detected\")\n",
    "    else:\n",
    "        print(f\"  ✓ Test performance better than train (normal for some cases)\")\n",
    "    \n",
    "    print(\"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Set Creation with Corrected Labeling\n",
    "\n",
    "**CORRECTED PROCEDURE**:\n",
    "\n",
    "The key insight is that we want the BDT to learn \"is this a real D⁰?\" rather than \"is this from signal MC?\".\n",
    "\n",
    "- **Real D⁰ (label = 1)**: Events with `abs(D0_mcPDG) == 421` from BOTH:\n",
    "  - Signal MC (Ds → D⁰ e ν samples)\n",
    "  - Generic MC (combinatoric backgrounds that happen to form real D⁰)\n",
    "\n",
    "- **Fake D⁰ (label = 0)**: Events from generic MC with:\n",
    "  - `abs(D0_mcPDG) != 421` (combinatoric background)\n",
    "  - `D0_mcPDG` is NaN (no truth match)\n",
    "\n",
    "This ensures the BDT learns real vs fake D⁰ topology, not signal vs background event characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T07:14:48.526024Z",
     "iopub.status.busy": "2025-12-12T07:14:48.525850Z",
     "iopub.status.idle": "2025-12-12T07:14:48.533193Z",
     "shell.execute_reply": "2025-12-12T07:14:48.532731Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_training_set(df_signal, df_generic, mode_variables):\n",
    "    \"\"\"\n",
    "    Create training set with CORRECTED LABELING PROCEDURE.\n",
    "    \n",
    "    Real D⁰: abs(D0_mcPDG) == 421 from BOTH signal MC AND generic MC\n",
    "    Fake D⁰: abs(D0_mcPDG) != 421 or NaN from generic MC\n",
    "    \n",
    "    Parameters:\n",
    "        df_signal: Signal MC DataFrame\n",
    "        df_generic: Generic MC DataFrame\n",
    "        mode_variables: List of variable names to use\n",
    "    \n",
    "    Returns:\n",
    "        X, y, df_train, df_real, df_fake, mode_variables_filtered\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Creating training set with CORRECTED labeling\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Real D⁰ from signal MC\n",
    "    real_signal_mask = (abs(df_signal[\"D0_mcPDG\"]) == 421)\n",
    "    df_real_signal = df_signal[real_signal_mask]\n",
    "    print(f\"Real D⁰ from Signal MC: {len(df_real_signal):,} events\")\n",
    "    \n",
    "    # Real D⁰ from generic MC\n",
    "    real_generic_mask = (abs(df_generic[\"D0_mcPDG\"]) == 421)\n",
    "    df_real_generic = df_generic[real_generic_mask]\n",
    "    print(f\"Real D⁰ from Generic MC: {len(df_real_generic):,} events\")\n",
    "    \n",
    "    # Combine real D⁰\n",
    "    df_real = pd.concat([df_real_signal, df_real_generic], ignore_index=True)\n",
    "    print(f\"Total Real D⁰: {len(df_real):,} events\")\n",
    "    \n",
    "    # Fake D⁰ from generic MC\n",
    "    fake_mask = (abs(df_generic[\"D0_mcPDG\"]) != 421) | df_generic[\"D0_mcPDG\"].isna()\n",
    "    df_fake = df_generic[fake_mask]\n",
    "    print(f\"Fake D⁰ from Generic MC: {len(df_fake):,} events\")\n",
    "    \n",
    "    # Check variable availability\n",
    "    missing_vars = [v for v in mode_variables if v not in df_real.columns]\n",
    "    if missing_vars:\n",
    "        print(f\"\\n⚠ Warning: {len(missing_vars)} variables not available in data:\")\n",
    "        for v in missing_vars[:10]:  # Show first 10\n",
    "            print(f\"  - {v}\")\n",
    "        if len(missing_vars) > 10:\n",
    "            print(f\"  ... and {len(missing_vars)-10} more\")\n",
    "        \n",
    "        # Filter to available variables\n",
    "        mode_variables_filtered = [v for v in mode_variables if v in df_real.columns]\n",
    "        print(f\"\\nProceeding with {len(mode_variables_filtered)} available variables\\n\")\n",
    "    else:\n",
    "        mode_variables_filtered = mode_variables\n",
    "    \n",
    "    # Combine\n",
    "    df_train = pd.concat([df_real, df_fake], axis=0, ignore_index=True)\n",
    "    \n",
    "    # Labels: 1 for real D⁰, 0 for fake D⁰\n",
    "    labels = np.concatenate([\n",
    "        np.ones(len(df_real), dtype=np.int64),\n",
    "        np.zeros(len(df_fake), dtype=np.int64)\n",
    "    ])\n",
    "    \n",
    "    # Features\n",
    "    X = df_train[mode_variables_filtered].to_numpy(dtype=np.float32)\n",
    "    y = labels\n",
    "    \n",
    "    # Clean data: replace inf with NaN\n",
    "    X = np.where(np.isinf(X), np.nan, X)\n",
    "    \n",
    "    # Check for NaN/inf values\n",
    "    n_nan = np.sum(np.isnan(X))\n",
    "    n_inf = np.sum(np.isinf(X))\n",
    "    \n",
    "    if n_nan > 0 or n_inf > 0:\n",
    "        print(f\"\\n⚠ Data cleaning:\")\n",
    "        print(f\"  - NaN values found: {n_nan:,}\")\n",
    "        print(f\"  - Inf values found: {n_inf:,}\")\n",
    "        print(f\"  - NaN/Inf values will be handled as missing by XGBoost\")\n",
    "    \n",
    "    print(f\"\\nTraining set: {len(df_train):,} events\")\n",
    "    print(f\"  - Real D⁰ (label=1): {np.sum(y==1):,}\")\n",
    "    print(f\"  - Fake D⁰ (label=0): {np.sum(y==0):,}\")\n",
    "    print(f\"  - Class ratio (Real/Fake): {np.sum(y==1)/np.sum(y==0):.3f}\")\n",
    "    print(f\"  - Number of features: {len(mode_variables_filtered)}\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    return X, y, df_train, df_real, df_fake, mode_variables_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T07:14:48.535671Z",
     "iopub.status.busy": "2025-12-12T07:14:48.535433Z",
     "iopub.status.idle": "2025-12-12T07:14:48.559224Z",
     "shell.execute_reply": "2025-12-12T07:14:48.558710Z"
    }
   },
   "outputs": [],
   "source": [
    "def check_boundary_violations(best_params, param_dist, boundary_threshold=0.15):\n",
    "    \"\"\"\n",
    "    Check if optimized parameters are too close to search boundaries.\n",
    "    \n",
    "    Respects hard physical constraints for XGBoost parameters.\n",
    "    \n",
    "    Parameters:\n",
    "        best_params: Dictionary of best parameters found\n",
    "        param_dist: Dictionary of parameter distributions used in search\n",
    "        boundary_threshold: Fraction from boundary to consider a violation (default 0.15 = 15%)\n",
    "    \n",
    "    Returns:\n",
    "        violations: Dictionary of parameters that violate boundaries\n",
    "        expanded_dist: New parameter distribution with expanded boundaries\n",
    "    \"\"\"\n",
    "    violations = {}\n",
    "    expanded_dist = {}\n",
    "    \n",
    "    # Define hard constraints for XGBoost parameters (min, max)\n",
    "    HARD_CONSTRAINTS = {\n",
    "        'subsample': (0.001, 1.0),         # MUST be in (0, 1]\n",
    "        'colsample_bytree': (0.001, 1.0),  # MUST be in (0, 1]\n",
    "        'colsample_bylevel': (0.001, 1.0),\n",
    "        'colsample_bynode': (0.001, 1.0),\n",
    "        'gamma': (0.0, None),               # >= 0, no upper limit\n",
    "        'reg_lambda': (0.0, None),          # >= 0, no upper limit  \n",
    "        'reg_alpha': (0.0, None),\n",
    "        'learning_rate': (0.0001, 1.0),     # Typically in (0, 1]\n",
    "        'max_delta_step': (0.0, None),\n",
    "        'n_estimators': (10, None),         # At least 10 trees\n",
    "        'max_depth': (1, None),             # At least depth 1\n",
    "        'min_child_weight': (0, None),\n",
    "    }\n",
    "    \n",
    "    for param_name, best_value in best_params.items():\n",
    "        if param_name not in param_dist:\n",
    "            expanded_dist[param_name] = param_dist[param_name]\n",
    "            continue\n",
    "        \n",
    "        dist = param_dist[param_name]\n",
    "        hard_min, hard_max = HARD_CONSTRAINTS.get(param_name, (None, None))\n",
    "        \n",
    "        # Handle uniform distributions (continuous parameters)\n",
    "        if hasattr(dist, 'kwds') and 'loc' in dist.kwds and 'scale' in dist.kwds:\n",
    "            # scipy.stats.uniform(loc, scale) -> range is [loc, loc+scale]\n",
    "            low = dist.kwds['loc']\n",
    "            scale = dist.kwds['scale']\n",
    "            high = low + scale\n",
    "            param_range = high - low\n",
    "            \n",
    "            # Check lower boundary\n",
    "            if best_value < (low + boundary_threshold * param_range):\n",
    "                violations[param_name] = {\n",
    "                    'value': best_value,\n",
    "                    'boundary': 'lower',\n",
    "                    'old_range': (low, high)\n",
    "                }\n",
    "                \n",
    "                # Expand downward, respecting hard minimum\n",
    "                expansion_amount = param_range  # Double the range\n",
    "                new_low = low - expansion_amount\n",
    "                \n",
    "                if hard_min is not None and new_low < hard_min:\n",
    "                    new_low = hard_min\n",
    "                    if new_low >= low:\n",
    "                        # Can't expand, already at limit\n",
    "                        print(f\"    ⚠ {param_name} at hard minimum ({hard_min}), cannot expand lower\")\n",
    "                        expanded_dist[param_name] = dist\n",
    "                        continue\n",
    "                \n",
    "                new_scale = high - new_low\n",
    "                expanded_dist[param_name] = uniform(new_low, new_scale)\n",
    "            \n",
    "            # Check upper boundary\n",
    "            elif best_value > (high - boundary_threshold * param_range):\n",
    "                violations[param_name] = {\n",
    "                    'value': best_value,\n",
    "                    'boundary': 'upper',\n",
    "                    'old_range': (low, high)\n",
    "                }\n",
    "                \n",
    "                # Expand upward, respecting hard maximum\n",
    "                expansion_amount = param_range\n",
    "                new_high = high + expansion_amount\n",
    "                \n",
    "                if hard_max is not None and new_high > hard_max:\n",
    "                    new_high = hard_max\n",
    "                    if new_high <= high:\n",
    "                        # Can't expand, already at limit\n",
    "                        print(f\"    ⚠ {param_name} at hard maximum ({hard_max}), cannot expand higher\")\n",
    "                        expanded_dist[param_name] = dist\n",
    "                        continue\n",
    "                \n",
    "                new_scale = new_high - low\n",
    "                expanded_dist[param_name] = uniform(low, new_scale)\n",
    "            else:\n",
    "                # No violation\n",
    "                expanded_dist[param_name] = dist\n",
    "        \n",
    "        # Handle randint distributions (integer parameters like n_estimators, max_depth)\n",
    "        elif hasattr(dist, 'kwds') and 'low' in dist.kwds and 'high' in dist.kwds:\n",
    "            # scipy.stats.randint(low, high) -> range is [low, high)\n",
    "            low = dist.kwds['low']\n",
    "            high = dist.kwds['high']\n",
    "            param_range = high - low\n",
    "            \n",
    "            # Check lower boundary\n",
    "            if best_value < (low + boundary_threshold * param_range):\n",
    "                violations[param_name] = {\n",
    "                    'value': best_value,\n",
    "                    'boundary': 'lower',\n",
    "                    'old_range': (low, high)\n",
    "                }\n",
    "                \n",
    "                # Expand downward\n",
    "                expansion_amount = param_range\n",
    "                new_low = int(low - expansion_amount)\n",
    "                \n",
    "                if hard_min is not None and new_low < hard_min:\n",
    "                    new_low = int(hard_min)\n",
    "                    if new_low >= low:\n",
    "                        print(f\"    ⚠ {param_name} at hard minimum ({hard_min}), cannot expand lower\")\n",
    "                        expanded_dist[param_name] = dist\n",
    "                        continue\n",
    "                \n",
    "                expanded_dist[param_name] = randint(new_low, high)\n",
    "            \n",
    "            # Check upper boundary  \n",
    "            elif best_value > (high - boundary_threshold * param_range):\n",
    "                violations[param_name] = {\n",
    "                    'value': best_value,\n",
    "                    'boundary': 'upper',\n",
    "                    'old_range': (low, high)\n",
    "                }\n",
    "                \n",
    "                # Expand upward\n",
    "                expansion_amount = param_range\n",
    "                new_high = int(high + expansion_amount)\n",
    "                \n",
    "                if hard_max is not None and new_high > hard_max:\n",
    "                    new_high = int(hard_max)\n",
    "                    if new_high <= high:\n",
    "                        print(f\"    ⚠ {param_name} at hard maximum ({hard_max}), cannot expand higher\")\n",
    "                        expanded_dist[param_name] = dist\n",
    "                        continue\n",
    "                \n",
    "                expanded_dist[param_name] = randint(low, new_high)\n",
    "            else:\n",
    "                # No violation\n",
    "                expanded_dist[param_name] = dist\n",
    "        else:\n",
    "            # Unknown distribution type, keep original\n",
    "            expanded_dist[param_name] = dist\n",
    "    \n",
    "    return violations, expanded_dist\n",
    "\n",
    "\n",
    "def train_bdt(X, y, run_optimization=True):\n",
    "    \"\"\"\n",
    "    Train XGBoost BDT with optional hyperparameter optimization.\n",
    "    \n",
    "    FIXED VERSION with early stopping and conservative hyperparameters to prevent overtraining.\n",
    "    \n",
    "    Parameters:\n",
    "        X: Feature matrix\n",
    "        y: Labels\n",
    "        run_optimization: Whether to run RandomizedSearchCV\n",
    "    \n",
    "    Returns:\n",
    "        bdt_final, X_train, X_test, y_train, y_test\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TRAINING BDT (WITH ANTI-OVERTRAINING FIXES)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Split data into train and test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y\n",
    "    )\n",
    "    \n",
    "    print(f\"Training set: {len(X_train):,} events\")\n",
    "    print(f\"Test set: {len(X_test):,} events\")\n",
    "    \n",
    "    # Calculate class weights to handle imbalance\n",
    "    n_real = np.sum(y_train == 1)\n",
    "    n_fake = np.sum(y_train == 0)\n",
    "    scale_pos_weight = n_fake / n_real if n_real > 0 else 1.0\n",
    "    \n",
    "    print(f\"\\nClass balance in training set:\")\n",
    "    print(f\"  Real D⁰ (positive, label=1): {n_real:,} ({100*n_real/len(y_train):.1f}%)\")\n",
    "    print(f\"  Fake D⁰ (negative, label=0): {n_fake:,} ({100*n_fake/len(y_train):.1f}%)\")\n",
    "    print(f\"  Class ratio (Fake/Real): {n_fake/n_real:.2f}\")\n",
    "    print(f\"  XGBoost scale_pos_weight: {scale_pos_weight:.2f} (for balancing)\")\n",
    "    \n",
    "    if run_optimization:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"HYPERPARAMETER OPTIMIZATION WITH EARLY STOPPING\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"\\nRunning RandomizedSearchCV with {RANDOM_SEARCH_ITERS} iterations...\")\n",
    "        print(\"Using CONSERVATIVE parameter ranges to prevent overfitting\")\n",
    "        \n",
    "        # Split training set into train_fit and validation for early stopping\n",
    "        X_train_fit, X_val, y_train_fit, y_val = train_test_split(\n",
    "            X_train, y_train, test_size=0.2, random_state=RANDOM_STATE, stratify=y_train\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n  Train (for fitting): {len(X_train_fit):,} events\")\n",
    "        print(f\"  Validation (for early stopping): {len(X_val):,} events\")\n",
    "        \n",
    "        # Define base estimator with early stopping\n",
    "        # FIXED: Use verbosity=0 instead of verbose=0 to suppress output\n",
    "        bdt_base = XGBClassifier(\n",
    "            objective=\"binary:logistic\",\n",
    "            eval_metric=\"logloss\",\n",
    "            max_delta_step=1,\n",
    "            scale_pos_weight=scale_pos_weight,\n",
    "            random_state=RANDOM_STATE,\n",
    "            missing=np.nan,\n",
    "            n_jobs=1,\n",
    "            early_stopping_rounds=20,  # CRITICAL FIX: Stop if no improvement for 20 rounds\n",
    "            verbosity=0  # FIXED: Correct parameter name to suppress warnings and logs\n",
    "        )\n",
    "        \n",
    "        # FIXED: More conservative hyperparameter search space to prevent overfitting\n",
    "        param_dist = {\n",
    "            'learning_rate': uniform(0.01, 0.09),      # [0.01, 0.10] - slower learning\n",
    "            'max_depth': randint(2, 5),                # [2, 6] - shallower trees\n",
    "            'n_estimators': randint(50, 151),          # [50, 200] - fewer trees\n",
    "            'reg_lambda': uniform(1, 9),               # [1, 10] - stronger L2 regularization\n",
    "            'gamma': uniform(1, 4),                    # [1, 5] - require more gain to split\n",
    "            'subsample': uniform(0.6, 0.3),            # [0.6, 0.9] - higher subsampling\n",
    "            'min_child_weight': randint(3, 8),         # [3, 10] - require more samples per leaf\n",
    "            'colsample_bytree': uniform(0.5, 0.4)      # [0.5, 0.9] - feature subsampling\n",
    "        }\n",
    "        \n",
    "        print(\"\\nConservative hyperparameter ranges:\")\n",
    "        print(\"  learning_rate: [0.01, 0.10]\")\n",
    "        print(\"  max_depth: [2, 6] (shallower trees)\")\n",
    "        print(\"  n_estimators: [50, 200] (fewer trees)\")\n",
    "        print(\"  reg_lambda: [1, 10] (stronger L2 regularization)\")\n",
    "        print(\"  gamma: [1, 5] (require more gain)\")\n",
    "        print(\"  subsample: [0.6, 0.9]\")\n",
    "        print(\"  min_child_weight: [3, 10] (more samples per leaf)\")\n",
    "        print(\"  colsample_bytree: [0.5, 0.9]\\n\")\n",
    "        \n",
    "        # Run optimization with boundary expansion\n",
    "        max_expansions = 2\n",
    "        expansion_count = 0\n",
    "        current_param_dist = param_dist.copy()\n",
    "        \n",
    "        while expansion_count <= max_expansions:\n",
    "            if expansion_count > 0:\n",
    "                print(f\"\\n{'='*80}\")\n",
    "                print(f\"BOUNDARY EXPANSION #{expansion_count}\")\n",
    "                print(f\"{'='*80}\")\n",
    "            \n",
    "            # Random search with cross-validation\n",
    "            # FIXED: Added return_train_score=True to monitor overfitting during CV\n",
    "            random_search = RandomizedSearchCV(\n",
    "                bdt_base,\n",
    "                param_distributions=current_param_dist,\n",
    "                n_iter=RANDOM_SEARCH_ITERS,\n",
    "                cv=5,\n",
    "                scoring='roc_auc',\n",
    "                n_jobs=8,\n",
    "                random_state=RANDOM_STATE + expansion_count,\n",
    "                verbose=1,\n",
    "                return_train_score=True  # CRITICAL FIX: Monitor train vs val performance\n",
    "            )\n",
    "            \n",
    "            # Fit with validation set for early stopping\n",
    "            random_search.fit(\n",
    "                X_train_fit, y_train_fit,\n",
    "                eval_set=[(X_val, y_val)],  # CRITICAL FIX: Validation set for early stopping\n",
    "                verbose=False  # FIXED: Suppress iteration-by-iteration output\n",
    "            )\n",
    "            \n",
    "            print(\"\\n\" + \"=\"*80)\n",
    "            print(\"OPTIMIZATION RESULTS\")\n",
    "            print(\"=\"*80)\n",
    "            print(\"\\nBest hyperparameters:\")\n",
    "            for param, value in random_search.best_params_.items():\n",
    "                print(f\"  {param}: {value}\")\n",
    "            \n",
    "            print(f\"\\nBest CV score (ROC AUC): {random_search.best_score_:.4f}\")\n",
    "            \n",
    "            # Check train vs test performance in CV\n",
    "            cv_results = random_search.cv_results_\n",
    "            best_idx = random_search.best_index_\n",
    "            mean_train_score = cv_results['mean_train_score'][best_idx]\n",
    "            mean_test_score = cv_results['mean_test_score'][best_idx]\n",
    "            \n",
    "            print(f\"\\nCross-validation performance for best model:\")\n",
    "            print(f\"  Mean train score: {mean_train_score:.4f}\")\n",
    "            print(f\"  Mean test score:  {mean_test_score:.4f}\")\n",
    "            print(f\"  Difference:       {mean_train_score - mean_test_score:+.4f}\")\n",
    "            \n",
    "            if mean_train_score - mean_test_score > 0.05:\n",
    "                print(\"  ⚠ Warning: Possible overfitting detected in CV (train > test by >0.05)\")\n",
    "            else:\n",
    "                print(\"  ✓ Good train/test agreement in CV\")\n",
    "            \n",
    "            # Check for boundary violations\n",
    "            violations, expanded_dist = check_boundary_violations(\n",
    "                random_search.best_params_, \n",
    "                current_param_dist,\n",
    "                boundary_threshold=0.15\n",
    "            )\n",
    "            \n",
    "            if violations:\n",
    "                print(\"\\n\" + \"⚠\"*40)\n",
    "                print(\"BOUNDARY VIOLATION DETECTED!\")\n",
    "                print(\"⚠\"*40)\n",
    "                print(\"\\nThe following parameters are too close to search boundaries:\")\n",
    "                \n",
    "                can_expand = False\n",
    "                \n",
    "                for param, info in violations.items():\n",
    "                    old_low, old_high = info['old_range']\n",
    "                    print(f\"\\n  {param}:\")\n",
    "                    print(f\"    Optimal value: {info['value']:.6g}\")\n",
    "                    print(f\"    Boundary hit: {info['boundary']}\")\n",
    "                    print(f\"    Original range: [{old_low}, {old_high}]\")\n",
    "                    \n",
    "                    # Check if expansion happened\n",
    "                    if param in expanded_dist and param in current_param_dist:\n",
    "                        new_dist = expanded_dist[param]\n",
    "                        old_dist = current_param_dist[param]\n",
    "                        \n",
    "                        # Get new range\n",
    "                        if hasattr(new_dist, 'kwds'):\n",
    "                            if 'loc' in new_dist.kwds and 'scale' in new_dist.kwds:\n",
    "                                # uniform distribution\n",
    "                                new_low = new_dist.kwds['loc']\n",
    "                                new_high = new_low + new_dist.kwds['scale']\n",
    "                                old_low_check = old_dist.kwds['loc']\n",
    "                                old_high_check = old_low_check + old_dist.kwds['scale']\n",
    "                                \n",
    "                                if abs(new_low - old_low_check) > 1e-9 or abs(new_high - old_high_check) > 1e-9:\n",
    "                                    print(f\"    Expanded range: [{new_low:.6g}, {new_high:.6g}]\")\n",
    "                                    can_expand = True\n",
    "                                    \n",
    "                            elif 'low' in new_dist.kwds and 'high' in new_dist.kwds:\n",
    "                                # randint distribution\n",
    "                                new_low = new_dist.kwds['low']\n",
    "                                new_high = new_dist.kwds['high']\n",
    "                                old_low_check = old_dist.kwds['low']\n",
    "                                old_high_check = old_dist.kwds['high']\n",
    "                                \n",
    "                                if new_low != old_low_check or new_high != old_high_check:\n",
    "                                    print(f\"    Expanded range: [{new_low}, {new_high})\")\n",
    "                                    can_expand = True\n",
    "                \n",
    "                if can_expand and expansion_count < max_expansions:\n",
    "                    print(f\"\\n→ Re-running optimization with expanded boundaries...\")\n",
    "                    current_param_dist = expanded_dist\n",
    "                    expansion_count += 1\n",
    "                else:\n",
    "                    if not can_expand:\n",
    "                        print(f\"\\n→ All parameters at hard constraints - this is NORMAL and acceptable.\")\n",
    "                    else:\n",
    "                        print(f\"\\n→ Maximum expansions ({max_expansions}) reached.\")\n",
    "                    break\n",
    "            else:\n",
    "                print(\"\\n✓ No boundary violations detected\")\n",
    "                break\n",
    "        \n",
    "        bdt_final = random_search.best_estimator_\n",
    "        \n",
    "        if expansion_count > 0:\n",
    "            print(f\"\\n{'='*80}\")\n",
    "            print(f\"OPTIMIZATION COMPLETE AFTER {expansion_count} EXPANSION(S)\")\n",
    "            print(f\"{'='*80}\")\n",
    "    \n",
    "    else:\n",
    "        print(\"\\nTraining with CONSERVATIVE default parameters (no optimization)...\")\n",
    "        print(f\"Using scale_pos_weight={scale_pos_weight:.2f}\")\n",
    "        \n",
    "        # FIXED: More conservative default parameters\n",
    "        X_train_fit, X_val, y_train_fit, y_val = train_test_split(\n",
    "            X_train, y_train, test_size=0.2, random_state=RANDOM_STATE, stratify=y_train\n",
    "        )\n",
    "        \n",
    "        bdt_final = XGBClassifier(\n",
    "            objective=\"binary:logistic\",\n",
    "            eval_metric=\"logloss\",\n",
    "            max_delta_step=1,\n",
    "            scale_pos_weight=scale_pos_weight,\n",
    "            random_state=RANDOM_STATE,\n",
    "            n_estimators=100,\n",
    "            max_depth=3,              # FIXED: Shallow trees\n",
    "            learning_rate=0.05,       # FIXED: Slower learning\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            reg_lambda=2.0,           # FIXED: L2 regularization\n",
    "            gamma=1.0,                # FIXED: Require gain to split\n",
    "            min_child_weight=5,       # FIXED: More samples per leaf\n",
    "            missing=np.nan,\n",
    "            early_stopping_rounds=20,  # CRITICAL FIX: Early stopping\n",
    "            verbosity=0  # FIXED: Correct parameter name to suppress warnings and logs\n",
    "        )\n",
    "        \n",
    "        print(\"\\nConservative default parameters:\")\n",
    "        print(\"  n_estimators: 100\")\n",
    "        print(\"  max_depth: 3 (shallow)\")\n",
    "        print(\"  learning_rate: 0.05 (slow)\")\n",
    "        print(\"  reg_lambda: 2.0 (L2 regularization)\")\n",
    "        print(\"  gamma: 1.0 (min gain to split)\")\n",
    "        print(\"  min_child_weight: 5 (samples per leaf)\")\n",
    "        print(\"  early_stopping_rounds: 20\")\n",
    "        \n",
    "        bdt_final.fit(\n",
    "            X_train_fit, y_train_fit,\n",
    "            eval_set=[(X_val, y_val)],\n",
    "            verbose=False  # FIXED: Suppress iteration-by-iteration output\n",
    "        )\n",
    "    \n",
    "    print(\"\\n✓ BDT training complete\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    return bdt_final, X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T07:14:48.560579Z",
     "iopub.status.busy": "2025-12-12T07:14:48.560449Z",
     "iopub.status.idle": "2025-12-12T07:14:48.564604Z",
     "shell.execute_reply": "2025-12-12T07:14:48.564188Z"
    }
   },
   "outputs": [],
   "source": [
    "def apply_bdt_and_save(bdt, mode_variables, mode, df_signal, df_generic):\n",
    "    \"\"\"\n",
    "    Apply trained BDT to signal and generic MC, and save to ROOT files.\n",
    "    \n",
    "    Parameters:\n",
    "        bdt: Trained XGBoost classifier\n",
    "        mode_variables: List of variable names used in training\n",
    "        mode: Decay mode\n",
    "        df_signal: Signal MC DataFrame\n",
    "        df_generic: Generic MC DataFrame\n",
    "    \"\"\"\n",
    "    import uproot\n",
    "    \n",
    "    if not SAVE_OUTPUT:\n",
    "        print(\"\\nSAVE_OUTPUT is False. Skipping file saving.\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"SAVING OUTPUT FILES\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Create output directory\n",
    "    output_dir = os.path.join(OUTPUT_BASE_DIR, \"FakeD0_BDT\", mode)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Save signal with BDT\n",
    "    signal_path = os.path.join(output_dir, f\"Ds2D0enu-Signal_{mode}_withBDT.root\")\n",
    "    print(f\"\\nSaving signal MC to: {signal_path}\")\n",
    "    \n",
    "    config = DECAY_CONFIG[mode]\n",
    "    tree_name = config[\"ds_tree\"]\n",
    "    \n",
    "    with uproot.recreate(signal_path) as f:\n",
    "        f[tree_name] = df_signal\n",
    "    \n",
    "    print(f\"  ✓ Saved {len(df_signal):,} signal events\")\n",
    "    \n",
    "    # Save generic with BDT\n",
    "    generic_path = os.path.join(output_dir, f\"Ds2D0e-Generic_{mode}_withBDT.root\")\n",
    "    print(f\"\\nSaving generic MC to: {generic_path}\")\n",
    "    \n",
    "    with uproot.recreate(generic_path) as f:\n",
    "        f[tree_name] = df_generic\n",
    "    \n",
    "    print(f\"  ✓ Saved {len(df_generic):,} generic events\")\n",
    "    print(\"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training Function\n",
    "\n",
    "Train XGBoost BDT with optional hyperparameter optimization.\n",
    "\n",
    "**Hyperparameter search space** (when `RUN_OPTIMIZATION = True`):\n",
    "- `learning_rate`: [0.01, 0.2]\n",
    "- `max_depth`: [1, 5]\n",
    "- `n_estimators`: [100, 200]\n",
    "- `reg_lambda`: [1, 5] (L2 regularization)\n",
    "- `gamma`: [0, 4] (min loss reduction for split)\n",
    "- `subsample`: [0.5, 1.0]\n",
    "- `min_child_weight`: [1, 6]\n",
    "- `colsample_bytree`: [0.3, 1.0]\n",
    "\n",
    "Uses `RandomizedSearchCV` with 5-fold cross-validation.\n",
    "\n",
    "### Intelligent Boundary Expansion\n",
    "\n",
    "**IMPORTANT FEATURE**: The optimization now includes automatic boundary checking:\n",
    "\n",
    "1. **Boundary Detection**: After optimization, checks if any parameters are within 15% of search boundaries\n",
    "2. **Automatic Expansion**: If a parameter hits a boundary, expands that boundary by 100% (2x range)\n",
    "3. **Re-optimization**: Automatically re-runs optimization with expanded boundaries\n",
    "4. **Maximum Attempts**: Allows up to 2 boundary expansions to prevent infinite loops\n",
    "5. **Warning System**: Alerts you if boundaries are still being hit after maximum expansions\n",
    "\n",
    "**Why This Matters**: If optimal parameters are pressed against boundaries, it suggests we haven't explored enough parameter space. This can lead to **suboptimal models** and **invalid scientific conclusions**. The automatic expansion ensures your results are scientifically sound.\n",
    "\n",
    "**Example Output**:\n",
    "```\n",
    "⚠ BOUNDARY VIOLATION DETECTED!\n",
    "The following parameters are too close to search boundaries:\n",
    "\n",
    "  learning_rate:\n",
    "    Optimal value: 0.0123\n",
    "    Boundary hit: lower\n",
    "    Original range: [0.01, 0.20]\n",
    "    Expanded range: [0.001, 0.20] (2x expansion)\n",
    "\n",
    "→ Re-running optimization with expanded boundaries...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T07:14:48.566111Z",
     "iopub.status.busy": "2025-12-12T07:14:48.565968Z",
     "iopub.status.idle": "2025-12-12T07:14:48.582347Z",
     "shell.execute_reply": "2025-12-12T07:14:48.581847Z"
    }
   },
   "outputs": [],
   "source": [
    "def process_mode(mode):\n",
    "    \"\"\"\n",
    "    Complete BDT training pipeline for a single mode.\n",
    "    \n",
    "    Parameters:\n",
    "        mode: Decay mode to process\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"█\"*80)\n",
    "    print(f\"PROCESSING MODE: {mode}\")\n",
    "    print(\"█\"*80 + \"\\n\")\n",
    "    \n",
    "    # Get proper title\n",
    "    mode_title = MODE_TITLES.get(mode, mode)\n",
    "    print(f\"Decay chain: {mode_title}\")\n",
    "    \n",
    "    # 1. Get variable list for this mode\n",
    "    if mode not in VARIABLES:\n",
    "        print(f\"ERROR: No variables defined for mode {mode}\")\n",
    "        return\n",
    "    \n",
    "    mode_variables = VARIABLES[mode][\"all_vars\"]\n",
    "    print(f\"Using {len(mode_variables)} variables from final_variables.py\")\n",
    "    \n",
    "    # 2. Load data\n",
    "    df_signal, df_generic = load_mode_data(mode)\n",
    "    \n",
    "    if df_signal.empty or df_generic.empty:\n",
    "        print(f\"ERROR: Failed to load data for {mode}\")\n",
    "        return\n",
    "    \n",
    "    # 3. Create training set\n",
    "    X, y, df_train, df_real, df_fake, mode_variables_filtered = create_training_set(\n",
    "        df_signal, df_generic, mode_variables\n",
    "    )\n",
    "    \n",
    "    # 4. Train BDT\n",
    "    bdt_final, X_train, X_test, y_train, y_test = train_bdt(X, y, run_optimization=RUN_OPTIMIZATION)\n",
    "    \n",
    "    # 5. BDT Statistics and overtraining metrics\n",
    "    print_bdt_statistics(bdt_final, X_train, y_train, X_test, y_test)\n",
    "    \n",
    "    # 6. Feature Importance\n",
    "    if SAVE_IMAGES:\n",
    "        save_dir = os.path.join(OUTPUT_DIR, mode)\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "    else:\n",
    "        save_dir = None\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"FEATURE IMPORTANCE\")\n",
    "    print(f\"\\nTotal variables being used for training: {len(mode_variables_filtered)}\")\n",
    "    print(\"\\nVerifying all variables from final_variables.py are included...\")\n",
    "    \n",
    "    # Check if all expected variables are present\n",
    "    expected_vars = set(VARIABLES[mode][\"all_vars\"])\n",
    "    actual_vars = set(mode_variables_filtered)\n",
    "    \n",
    "    missing_vars = expected_vars - actual_vars\n",
    "    if missing_vars:\n",
    "        print(f\"  ⚠ {len(missing_vars)} variables from final_variables.py not found in data:\")\n",
    "        for v in list(missing_vars)[:10]:\n",
    "            print(f\"    - {v}\")\n",
    "        if len(missing_vars) > 10:\n",
    "            print(f\"    ... and {len(missing_vars)-10} more\")\n",
    "    else:\n",
    "        print(f\"  ✓ All {len(expected_vars)} variables from final_variables.py are being used\")\n",
    "    print(\"=\"*80)\n",
    "    feature_imp_df = plot_feature_importance(\n",
    "        bdt_final, mode_variables_filtered, mode, num=20, save_dir=save_dir\n",
    "    )\n",
    "    print(\"\\nTop 10 Most Important Features:\")\n",
    "    print(feature_imp_df.sort_values(by=\"Value\", ascending=False).head(10))\n",
    "    \n",
    "    # 7. BDT Output Comparison with Pull Plots\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"BDT OUTPUT COMPARISON (Train vs Test with Pull Plots)\")\n",
    "    print(\"=\"*80)\n",
    "    decisions = compare_train_test(bdt_final, X_train, y_train, X_test, y_test, mode)\n",
    "    \n",
    "    if save_dir:\n",
    "        plt.savefig(os.path.join(save_dir, \"bdt_output_comparison.png\"), dpi=150, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()\n",
    "    \n",
    "    # 8. ROC Curve (keep existing Belle II style)\n",
    "    print(\"\\nPlotting ROC curve...\")\n",
    "    y_score_test = bdt_final.predict_proba(X_test)[:, 1]\n",
    "    fpr_test, tpr_test, _ = roc_curve(y_test, y_score_test)\n",
    "    area_test = auc(fpr_test, tpr_test)\n",
    "    \n",
    "    y_score_train = bdt_final.predict_proba(X_train)[:, 1]\n",
    "    fpr_train, tpr_train, _ = roc_curve(y_train, y_score_train)\n",
    "    area_train = auc(fpr_train, tpr_train)\n",
    "    \n",
    "    # Compute background rejection vs signal efficiency\n",
    "    bdt_cuts = np.linspace(0, 1, 100)\n",
    "    sig_train = y_score_train[y_train == 1]\n",
    "    bkg_train = y_score_train[y_train == 0]\n",
    "    sig_test = y_score_test[y_test == 1]\n",
    "    bkg_test = y_score_test[y_test == 0]\n",
    "    \n",
    "    sig_eff_train = []\n",
    "    bkg_rej_train = []\n",
    "    sig_eff_test = []\n",
    "    bkg_rej_test = []\n",
    "    \n",
    "    for cut in bdt_cuts:\n",
    "        sig_eff_train.append(np.sum(sig_train > cut) / len(sig_train))\n",
    "        bkg_rej_train.append(1 - (np.sum(bkg_train > cut) / len(bkg_train)))\n",
    "        sig_eff_test.append(np.sum(sig_test > cut) / len(sig_test))\n",
    "        bkg_rej_test.append(1 - (np.sum(bkg_test > cut) / len(bkg_test)))\n",
    "    \n",
    "    fig, axs = plt.subplots(1, 1, figsize=(7, 6))\n",
    "    \n",
    "    axs.plot(bkg_rej_train, sig_eff_train, color='tab:blue', linewidth=2,\n",
    "            label=f'Train (AUC = {area_train:.2f})')\n",
    "    axs.plot(bkg_rej_test, sig_eff_test, color='tab:red', linestyle='--', linewidth=2,\n",
    "            label=f'Test (AUC = {area_test:.2f})')\n",
    "    \n",
    "    axs.fill_between(bkg_rej_test, sig_eff_train, sig_eff_test,\n",
    "                     where=(np.array(sig_eff_train) > np.array(sig_eff_test)),\n",
    "                     color='gray', alpha=0.2, label='Overfit Gap')\n",
    "    \n",
    "    axs.set_title(mode_title, loc='left')\n",
    "    axs.set_ylim(0, 1.05)\n",
    "    axs.set_xlim(0, 1.05)\n",
    "    axs.set_xlabel('Background rejection')\n",
    "    axs.set_ylabel('Signal efficiency')\n",
    "    axs.legend(loc='lower left')\n",
    "    axs.grid(True)\n",
    "    \n",
    "    if save_dir:\n",
    "        plt.savefig(os.path.join(save_dir, \"roc_curve_belle2.png\"), dpi=150, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()\n",
    "    \n",
    "    # 9. Optimize cut using Punzi FoM\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"OPTIMIZING BDT CUT USING PUNZI FoM\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Apply BDT to signal and generic\n",
    "    df_signal[\"Ds_FakeD0BDT\"] = bdt_final.predict_proba(\n",
    "        df_signal[mode_variables_filtered]\n",
    "    )[:, 1].astype(np.float32)\n",
    "    \n",
    "    df_generic[\"Ds_FakeD0BDT\"] = bdt_final.predict_proba(\n",
    "        df_generic[mode_variables_filtered]\n",
    "    )[:, 1].astype(np.float32)\n",
    "    \n",
    "    # Get real D0 from signal for Punzi FoM calculation\n",
    "    sig_real_mask = (abs(df_signal[\"D0_mcPDG\"]) == 421)\n",
    "    bdt_scores_sig = df_signal.loc[sig_real_mask, \"Ds_FakeD0BDT\"].values\n",
    "    bdt_scores_bkg = df_generic[\"Ds_FakeD0BDT\"].values\n",
    "    \n",
    "    # Compute Punzi FoM for different confidence levels\n",
    "    print(\"\\nOptimizing BDT cut for different confidence levels:\\n\")\n",
    "    \n",
    "    best_cuts = {}\n",
    "    best_foms = {}\n",
    "    \n",
    "    for a_val in PUNZI_A_VALUES:\n",
    "        thresholds, foms, best_thresh, best_fom = compute_punzi_scan(\n",
    "            bdt_scores_sig, bdt_scores_bkg, N_SIGNAL_EVENTS, n_thresholds=200, a=a_val\n",
    "        )\n",
    "        \n",
    "        cl_name = {1.64: \"90% CL\", 1.96: \"95% CL\", 3.0: \"3σ\"}.get(a_val, f\"a={a_val}\")\n",
    "        best_cuts[cl_name] = best_thresh\n",
    "        best_foms[cl_name] = best_fom\n",
    "        print(f\"  {cl_name:8s}: Optimal cut = {best_thresh:.3f}, Punzi FoM = {best_fom:.6g}\")\n",
    "    \n",
    "    # Plot Punzi FoM curves for all confidence levels\n",
    "    if save_dir:\n",
    "        fig, ax = plt.subplots(figsize=(10, 7))\n",
    "        \n",
    "        # Colors: purple for 95% CL (main), black for others\n",
    "        colors = {\"90% CL\": \"black\", \"95% CL\": \"purple\", \"3σ\": \"black\"}\n",
    "        linestyles = {\"90% CL\": \"--\", \"95% CL\": \"-\", \"3σ\": \"-.\"}\n",
    "        linewidths = {\"90% CL\": 2, \"95% CL\": 3, \"3σ\": 2}\n",
    "        \n",
    "        for a_val in PUNZI_A_VALUES:\n",
    "            thresholds, foms, best_thresh, best_fom = compute_punzi_scan(\n",
    "                bdt_scores_sig, bdt_scores_bkg, N_SIGNAL_EVENTS, n_thresholds=200, a=a_val\n",
    "            )\n",
    "            \n",
    "            cl_name = {1.64: \"90% CL\", 1.96: \"95% CL\", 3.0: \"3σ\"}.get(a_val, f\"a={a_val}\")\n",
    "            \n",
    "            ax.plot(\n",
    "                thresholds, foms,\n",
    "                color=colors[cl_name],\n",
    "                linestyle=linestyles[cl_name],\n",
    "                linewidth=linewidths[cl_name],\n",
    "                label=f\"{cl_name} (cut={best_thresh:.3f})\"\n",
    "            )\n",
    "            \n",
    "            # Mark optimal point\n",
    "            ax.plot(best_thresh, best_fom, \"o\", color=colors[cl_name], markersize=8)\n",
    "        \n",
    "        ax.set_xlabel(\"BDT Cut\", color=\"black\")\n",
    "        ax.set_ylabel(\"Punzi FoM\", color=\"black\")\n",
    "        ax.set_title(mode_title, loc=\"left\")\n",
    "        ax.legend(loc=\"best\")\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Make sure tick labels are black\n",
    "        ax.tick_params(axis=\"x\", colors=\"black\")\n",
    "        ax.tick_params(axis=\"y\", colors=\"black\")\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(save_dir, \"punzi_fom_optimization.png\"), dpi=150, bbox_inches=\"tight\")\n",
    "        plt.close()  # Close to avoid display\n",
    "        \n",
    "        print(f\"\\n✓ Saved Punzi FoM plot to: {os.path.join(save_dir, 'punzi_fom_optimization.png')}\")\n",
    "    \n",
    "    # 10. Generate comprehensive BDT summary\n",
    "    if save_dir:\n",
    "        # Get hyperparameters from the trained model\n",
    "        hyperparams = bdt_final.get_params()\n",
    "        \n",
    "        # Create summary\n",
    "        summary = create_bdt_summary(\n",
    "            mode=mode,\n",
    "            bdt_model=bdt_final,\n",
    "            X_train=X_train,\n",
    "            y_train=y_train,\n",
    "            X_test=X_test,\n",
    "            y_test=y_test,\n",
    "            feature_names=mode_variables_filtered,\n",
    "            best_cuts=best_cuts,\n",
    "            hyperparameters=hyperparams,\n",
    "            save_dir=save_dir\n",
    "        )\n",
    "    \n",
    "    print(\"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T07:14:48.583921Z",
     "iopub.status.busy": "2025-12-12T07:14:48.583785Z",
     "iopub.status.idle": "2025-12-12T07:14:48.602231Z",
     "shell.execute_reply": "2025-12-12T07:14:48.601732Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_additional_metrics(train_bkg, train_sig, test_bkg, test_sig):\n",
    "    \"\"\"\n",
    "    Compute additional robustness metrics beyond KS test.\n",
    "    \n",
    "    Parameters:\n",
    "        train_bkg, train_sig: Training BDT outputs for background and signal\n",
    "        test_bkg, test_sig: Test BDT outputs for background and signal\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary of additional metrics\n",
    "    \"\"\"\n",
    "    from scipy.spatial.distance import jensenshannon\n",
    "    from scipy.stats import wasserstein_distance\n",
    "    \n",
    "    # Jensen-Shannon divergence (0 = identical, 1 = completely different)\n",
    "    bins = np.linspace(0, 1, 51)\n",
    "    hist_train_bkg, _ = np.histogram(train_bkg, bins=bins, density=True)\n",
    "    hist_test_bkg, _ = np.histogram(test_bkg, bins=bins, density=True)\n",
    "    hist_train_sig, _ = np.histogram(train_sig, bins=bins, density=True)\n",
    "    hist_test_sig, _ = np.histogram(test_sig, bins=bins, density=True)\n",
    "    \n",
    "    # Normalize to probability distributions\n",
    "    hist_train_bkg = hist_train_bkg / (hist_train_bkg.sum() + 1e-10)\n",
    "    hist_test_bkg = hist_test_bkg / (hist_test_bkg.sum() + 1e-10)\n",
    "    hist_train_sig = hist_train_sig / (hist_train_sig.sum() + 1e-10)\n",
    "    hist_test_sig = hist_test_sig / (hist_test_sig.sum() + 1e-10)\n",
    "    \n",
    "    js_bkg = jensenshannon(hist_train_bkg, hist_test_bkg)\n",
    "    js_sig = jensenshannon(hist_train_sig, hist_test_sig)\n",
    "    \n",
    "    # Wasserstein distance (Earth Mover's Distance)\n",
    "    wasserstein_bkg = wasserstein_distance(train_bkg, test_bkg)\n",
    "    wasserstein_sig = wasserstein_distance(train_sig, test_sig)\n",
    "    \n",
    "    return {\n",
    "        \"jensen_shannon_divergence_bkg\": float(js_bkg),\n",
    "        \"jensen_shannon_divergence_sig\": float(js_sig),\n",
    "        \"wasserstein_distance_bkg\": float(wasserstein_bkg),\n",
    "        \"wasserstein_distance_sig\": float(wasserstein_sig)\n",
    "    }\n",
    "\n",
    "\n",
    "def assess_model_quality(metrics):\n",
    "    \"\"\"\n",
    "    Assess overall BDT model quality based on metrics.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with assessment and recommendations\n",
    "    \"\"\"\n",
    "    assessment = {\n",
    "        \"overall_quality\": \"UNKNOWN\",\n",
    "        \"concerns\": [],\n",
    "        \"strengths\": [],\n",
    "        \"recommendations\": []\n",
    "    }\n",
    "    \n",
    "    # Check AUC agreement\n",
    "    auc_diff = abs(metrics[\"test_auc\"] - metrics[\"train_auc\"])\n",
    "    if auc_diff < 0.02:\n",
    "        assessment[\"strengths\"].append(\"Excellent train/test AUC agreement (no overtraining)\")\n",
    "    elif auc_diff < 0.05:\n",
    "        assessment[\"strengths\"].append(\"Good train/test AUC agreement\")\n",
    "    else:\n",
    "        assessment[\"concerns\"].append(f\"Significant AUC difference: {auc_diff:.3f}\")\n",
    "        assessment[\"recommendations\"].append(\"Consider stronger regularization or early stopping\")\n",
    "    \n",
    "    # Check KS test scores\n",
    "    ks_sig = metrics[\"ks_test_signal\"]\n",
    "    ks_bkg = metrics[\"ks_test_background\"]\n",
    "    \n",
    "    if ks_sig > 0.05:\n",
    "        assessment[\"strengths\"].append(f\"Signal KS test p-value = {ks_sig:.3f} (good agreement)\")\n",
    "    else:\n",
    "        assessment[\"concerns\"].append(f\"Low signal KS test p-value = {ks_sig:.3f}\")\n",
    "    \n",
    "    if ks_bkg > 0.05:\n",
    "        assessment[\"strengths\"].append(f\"Background KS test p-value = {ks_bkg:.3f} (good agreement)\")\n",
    "    else:\n",
    "        assessment[\"concerns\"].append(f\"Low background KS test p-value = {ks_bkg:.3f} (statistical difference detected)\")\n",
    "        assessment[\"recommendations\"].append(\"Check background train/test distribution shape - KS test is sensitive even if practical performance is good\")\n",
    "    \n",
    "    # Check additional metrics if available\n",
    "    if \"additional_metrics\" in metrics:\n",
    "        js_bkg = metrics[\"additional_metrics\"][\"jensen_shannon_divergence_bkg\"]\n",
    "        if js_bkg < 0.1:\n",
    "            assessment[\"strengths\"].append(f\"Low Jensen-Shannon divergence for background ({js_bkg:.3f})\")\n",
    "        elif js_bkg > 0.3:\n",
    "            assessment[\"concerns\"].append(f\"High Jensen-Shannon divergence for background ({js_bkg:.3f})\")\n",
    "    \n",
    "    # Overall assessment\n",
    "    n_concerns = len(assessment[\"concerns\"])\n",
    "    n_strengths = len(assessment[\"strengths\"])\n",
    "    \n",
    "    if n_concerns == 0:\n",
    "        assessment[\"overall_quality\"] = \"EXCELLENT\"\n",
    "    elif n_concerns <= 1 and auc_diff < 0.02:\n",
    "        assessment[\"overall_quality\"] = \"GOOD\"\n",
    "        assessment[\"recommendations\"].append(\"Model is usable. KS test is sensitive to shape differences even if discrimination power (AUC) is identical.\")\n",
    "    elif n_concerns <= 2 and auc_diff < 0.05:\n",
    "        assessment[\"overall_quality\"] = \"ACCEPTABLE\"\n",
    "        assessment[\"recommendations\"].append(\"Model has some issues but may be usable. Monitor performance on validation data.\")\n",
    "    else:\n",
    "        assessment[\"overall_quality\"] = \"POOR\"\n",
    "        assessment[\"recommendations\"].append(\"Consider retraining with different hyperparameters or more data.\")\n",
    "    \n",
    "    return assessment\n",
    "\n",
    "\n",
    "def create_bdt_summary(\n",
    "    mode,\n",
    "    bdt_model,\n",
    "    X_train, y_train,\n",
    "    X_test, y_test,\n",
    "    feature_names,\n",
    "    best_cuts,\n",
    "    hyperparameters,\n",
    "    save_dir\n",
    "):\n",
    "    \"\"\"\n",
    "    Create comprehensive BDT summary with JSON and text outputs.\n",
    "    \n",
    "    Parameters:\n",
    "        mode: Decay mode\n",
    "        bdt_model: Trained XGBoost model\n",
    "        X_train, y_train: Training data\n",
    "        X_test, y_test: Test data\n",
    "        feature_names: List of feature names\n",
    "        best_cuts: Dictionary of optimal cuts from Punzi FoM\n",
    "        hyperparameters: Dictionary of model hyperparameters\n",
    "        save_dir: Directory to save summary files\n",
    "    \n",
    "    Returns:\n",
    "        summary: Dictionary containing all metrics\n",
    "    \"\"\"\n",
    "    import json\n",
    "    from datetime import datetime\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"GENERATING BDT SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Get BDT predictions\n",
    "    y_pred_train = bdt_model.predict_proba(X_train)[:, 1]\n",
    "    y_pred_test = bdt_model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Split by class\n",
    "    train_sig = y_pred_train[y_train == 1]\n",
    "    train_bkg = y_pred_train[y_train == 0]\n",
    "    test_sig = y_pred_test[y_test == 1]\n",
    "    test_bkg = y_pred_test[y_test == 0]\n",
    "    \n",
    "    # Compute metrics\n",
    "    train_auc = roc_auc_score(y_train, y_pred_train)\n",
    "    test_auc = roc_auc_score(y_test, y_pred_test)\n",
    "    \n",
    "    ks_sig_stat, ks_sig_pvalue = ks_2samp(train_sig, test_sig)\n",
    "    ks_bkg_stat, ks_bkg_pvalue = ks_2samp(train_bkg, test_bkg)\n",
    "    \n",
    "    # Additional metrics\n",
    "    additional_metrics = compute_additional_metrics(train_bkg, train_sig, test_bkg, test_sig)\n",
    "    \n",
    "    # Feature importances\n",
    "    feature_imp = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance': bdt_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    top_features = feature_imp.head(10).to_dict('records')\n",
    "    \n",
    "    # Pull statistics\n",
    "    bins = 50\n",
    "    hist_train_bkg, _ = np.histogram(train_bkg, bins=bins, range=(0, 1), density=True)\n",
    "    hist_test_bkg, bin_edges = np.histogram(test_bkg, bins=bins, range=(0, 1))\n",
    "    bin_width = bin_edges[1] - bin_edges[0]\n",
    "    test_bkg_density = (bins / len(test_bkg)) * hist_test_bkg\n",
    "    test_bkg_errors = (bins / len(test_bkg)) * np.sqrt(hist_test_bkg)\n",
    "    \n",
    "    pulls_bkg = np.zeros(len(hist_test_bkg))\n",
    "    mask = test_bkg_errors > 0\n",
    "    pulls_bkg[mask] = (test_bkg_density[mask] - hist_train_bkg[mask]) / test_bkg_errors[mask]\n",
    "    \n",
    "    pull_mean = np.mean(pulls_bkg)\n",
    "    pull_std = np.std(pulls_bkg)\n",
    "    pull_max = np.max(np.abs(pulls_bkg))\n",
    "    \n",
    "    # Create summary dictionary\n",
    "    summary = {\n",
    "        \"metadata\": {\n",
    "            \"mode\": mode,\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"n_features\": len(feature_names),\n",
    "            \"n_train\": len(y_train),\n",
    "            \"n_test\": len(y_test),\n",
    "            \"n_train_signal\": int(np.sum(y_train == 1)),\n",
    "            \"n_train_background\": int(np.sum(y_train == 0)),\n",
    "            \"n_test_signal\": int(np.sum(y_test == 1)),\n",
    "            \"n_test_background\": int(np.sum(y_test == 0))\n",
    "        },\n",
    "        \"performance\": {\n",
    "            \"train_auc\": float(train_auc),\n",
    "            \"test_auc\": float(test_auc),\n",
    "            \"auc_difference\": float(test_auc - train_auc),\n",
    "            \"ks_test_signal\": float(ks_sig_pvalue),\n",
    "            \"ks_test_background\": float(ks_bkg_pvalue),\n",
    "            \"ks_statistic_signal\": float(ks_sig_stat),\n",
    "            \"ks_statistic_background\": float(ks_bkg_stat)\n",
    "        },\n",
    "        \"additional_metrics\": additional_metrics,\n",
    "        \"pull_statistics\": {\n",
    "            \"mean\": float(pull_mean),\n",
    "            \"std\": float(pull_std),\n",
    "            \"max_abs\": float(pull_max)\n",
    "        },\n",
    "        \"optimal_cuts\": best_cuts,\n",
    "        \"hyperparameters\": hyperparameters,\n",
    "        \"top_features\": top_features\n",
    "    }\n",
    "    \n",
    "    # Add assessment\n",
    "    summary[\"assessment\"] = assess_model_quality(summary[\"performance\"] | {\"additional_metrics\": additional_metrics})\n",
    "    \n",
    "    # Save JSON\n",
    "    json_path = os.path.join(save_dir, \"bdt_summary.json\")\n",
    "    with open(json_path, 'w') as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "    print(f\"\\n✓ Saved JSON summary to: {json_path}\")\n",
    "    \n",
    "    # Create human-readable text summary\n",
    "    text_summary = f\"\"\"\n",
    "================================================================================\n",
    "FAKE D⁰ BDT TRAINING SUMMARY - {mode.upper()}\n",
    "================================================================================\n",
    "\n",
    "Generated: {summary['metadata']['timestamp']}\n",
    "\n",
    "DATASET INFORMATION\n",
    "────────────────────────────────────────────────────────────────────────────────\n",
    "Training set:   {summary['metadata']['n_train']:,} events\n",
    "  - Signal:     {summary['metadata']['n_train_signal']:,} events\n",
    "  - Background: {summary['metadata']['n_train_background']:,} events\n",
    "\n",
    "Test set:       {summary['metadata']['n_test']:,} events\n",
    "  - Signal:     {summary['metadata']['n_test_signal']:,} events\n",
    "  - Background: {summary['metadata']['n_test_background']:,} events\n",
    "\n",
    "Features:       {summary['metadata']['n_features']} variables\n",
    "\n",
    "PERFORMANCE METRICS\n",
    "────────────────────────────────────────────────────────────────────────────────\n",
    "ROC AUC:\n",
    "  Training:     {summary['performance']['train_auc']:.4f}\n",
    "  Test:         {summary['performance']['test_auc']:.4f}\n",
    "  Difference:   {summary['performance']['auc_difference']:+.4f}\n",
    "\n",
    "Kolmogorov-Smirnov Test (p-values):\n",
    "  Signal:       {summary['performance']['ks_test_signal']:.4f}\n",
    "  Background:   {summary['performance']['ks_test_background']:.4f}\n",
    "\n",
    "Additional Distribution Metrics:\n",
    "  Jensen-Shannon Divergence (Background): {summary['additional_metrics']['jensen_shannon_divergence_bkg']:.4f}\n",
    "  Jensen-Shannon Divergence (Signal):     {summary['additional_metrics']['jensen_shannon_divergence_sig']:.4f}\n",
    "  Wasserstein Distance (Background):      {summary['additional_metrics']['wasserstein_distance_bkg']:.4f}\n",
    "  Wasserstein Distance (Signal):          {summary['additional_metrics']['wasserstein_distance_sig']:.4f}\n",
    "\n",
    "Pull Statistics (Background):\n",
    "  Mean:         {summary['pull_statistics']['mean']:+.3f}\n",
    "  Std Dev:      {summary['pull_statistics']['std']:.3f}\n",
    "  Max |Pull|:   {summary['pull_statistics']['max_abs']:.3f}\n",
    "\n",
    "OPTIMAL BDT CUTS (Punzi FoM)\n",
    "────────────────────────────────────────────────────────────────────────────────\n",
    "\"\"\"\n",
    "    \n",
    "    for cl_name, cut_value in best_cuts.items():\n",
    "        text_summary += f\"  {cl_name:8s}: {cut_value:.4f}\\n\"\n",
    "    \n",
    "    text_summary += f\"\"\"\n",
    "TOP 10 MOST IMPORTANT FEATURES\n",
    "────────────────────────────────────────────────────────────────────────────────\n",
    "\"\"\"\n",
    "    \n",
    "    for i, feat in enumerate(top_features, 1):\n",
    "        text_summary += f\"  {i:2d}. {feat['feature']:40s} {feat['importance']:.4f}\\n\"\n",
    "    \n",
    "    text_summary += f\"\"\"\n",
    "HYPERPARAMETERS\n",
    "────────────────────────────────────────────────────────────────────────────────\n",
    "\"\"\"\n",
    "    \n",
    "    for param, value in hyperparameters.items():\n",
    "        if isinstance(value, float):\n",
    "            text_summary += f\"  {param:25s}: {value:.6f}\\n\"\n",
    "        else:\n",
    "            text_summary += f\"  {param:25s}: {value}\\n\"\n",
    "    \n",
    "    text_summary += f\"\"\"\n",
    "MODEL QUALITY ASSESSMENT\n",
    "────────────────────────────────────────────────────────────────────────────────\n",
    "Overall Quality: {summary['assessment']['overall_quality']}\n",
    "\n",
    "Strengths:\n",
    "\"\"\"\n",
    "    \n",
    "    for strength in summary['assessment']['strengths']:\n",
    "        text_summary += f\"  ✓ {strength}\\n\"\n",
    "    \n",
    "    if summary['assessment']['concerns']:\n",
    "        text_summary += \"\\nConcerns:\\n\"\n",
    "        for concern in summary['assessment']['concerns']:\n",
    "            text_summary += f\"  ⚠ {concern}\\n\"\n",
    "    \n",
    "    if summary['assessment']['recommendations']:\n",
    "        text_summary += \"\\nRecommendations:\\n\"\n",
    "        for rec in summary['assessment']['recommendations']:\n",
    "            text_summary += f\"  → {rec}\\n\"\n",
    "    \n",
    "    text_summary += \"\"\"\n",
    "================================================================================\n",
    "USAGE NOTES FOR OVERALL BACKGROUND BDT\n",
    "────────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "This Fake D⁰ suppression BDT can be applied in two ways:\n",
    "\n",
    "1. HARD CUT: Apply recommended Punzi FoM cut as a selection requirement\n",
    "   - Use the optimal cut from the confidence level appropriate for your analysis\n",
    "   - Recommended for simple event selection\n",
    "\n",
    "2. BDT VARIABLE: Include Fake D⁰ BDT output as input to overall background BDT\n",
    "   - Allows overall BDT to learn optimal combination with other discriminants\n",
    "   - Recommended for maximum performance\n",
    "\n",
    "Recommendation: Test both approaches and compare final significance/FoM.\n",
    "\n",
    "================================================================================\n",
    "\"\"\"\n",
    "    \n",
    "    # Save text summary\n",
    "    text_path = os.path.join(save_dir, \"bdt_summary.txt\")\n",
    "    with open(text_path, 'w') as f:\n",
    "        f.write(text_summary)\n",
    "    print(f\"✓ Saved text summary to: {text_path}\")\n",
    "    \n",
    "    # Print summary to console\n",
    "    print(text_summary)\n",
    "    \n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Training\n",
    "\n",
    "Execute the training pipeline for the selected mode(s).\n",
    "\n",
    "Set `TRAIN_MODE` in the configuration section to:\n",
    "- `\"kmpip\"` - Train only on K⁻ π⁺ mode\n",
    "- `\"km3pi\"` - Train only on K⁻ 3π mode\n",
    "- `\"kmpippi0_eff20_May2020\"` - Train only on K⁻ π⁺ π⁰ mode\n",
    "- `\"all\"` - Train on all modes sequentially"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T07:14:48.603673Z",
     "iopub.status.busy": "2025-12-12T07:14:48.603532Z",
     "iopub.status.idle": "2025-12-12T07:53:47.760095Z",
     "shell.execute_reply": "2025-12-12T07:53:47.759540Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "████████████████████████████████████████████████████████████████████████████████\n",
      "PROCESSING MODE: kmpip\n",
      "████████████████████████████████████████████████████████████████████████████████\n",
      "\n",
      "Decay chain: $D_s^{+} \\rightarrow [D^{0} \\rightarrow K^{-} \\pi^{+}] e^{+} \\nu_{e}$\n",
      "Using 12 variables from final_variables.py\n",
      "\n",
      "================================================================================\n",
      "Loading data for mode: kmpip\n",
      "Tree: DstreeCh1\n",
      "D⁰ mass cut: (-0.014291 <= D0_dM) & (D0_dM <= 0.014152)\n",
      "================================================================================\n",
      "\n",
      "Loading Signal MC from: /home/belle2/amubarak/C01-Simulated_Events/Signal/Ds2D0e-Signal_1_kmpip.root\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  → Signal MC: 16,530 events after D⁰ mass cut\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Loading generic MC:   0%|                                                                                                                                                                       | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Loading generic MC:  17%|██████████████████████████▌                                                                                                                                    | 1/6 [01:08<05:43, 68.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  → ccbar: 311,413 events after D⁰ mass cut\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Loading generic MC:  33%|█████████████████████████████████████████████████████                                                                                                          | 2/6 [01:24<02:30, 37.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  → charged: 2,167 events after D⁰ mass cut\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Loading generic MC:  50%|███████████████████████████████████████████████████████████████████████████████▌                                                                               | 3/6 [01:33<01:13, 24.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  → ddbar: 4,867 events after D⁰ mass cut\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Loading generic MC:  67%|██████████████████████████████████████████████████████████████████████████████████████████████████████████                                                     | 4/6 [01:46<00:40, 20.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  → mixed: 928 events after D⁰ mass cut\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Loading generic MC:  83%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                          | 5/6 [01:55<00:15, 15.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  → ssbar: 9,847 events after D⁰ mass cut\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Loading generic MC: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [02:29<00:00, 22.14s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Loading generic MC: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [02:29<00:00, 24.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  → uubar: 22,558 events after D⁰ mass cut\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  → Total Generic MC: 351,780 events\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Creating training set with CORRECTED labeling\n",
      "================================================================================\n",
      "Real D⁰ from Signal MC: 16,408 events\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real D⁰ from Generic MC: 293,106 events\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Real D⁰: 309,514 events\n",
      "Fake D⁰ from Generic MC: 58,674 events\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training set: 368,188 events\n",
      "  - Real D⁰ (label=1): 309,514\n",
      "  - Fake D⁰ (label=0): 58,674\n",
      "  - Class ratio (Real/Fake): 5.275\n",
      "  - Number of features: 12\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "TRAINING BDT (WITH ANTI-OVERTRAINING FIXES)\n",
      "================================================================================\n",
      "Training set: 257,731 events\n",
      "Test set: 110,457 events\n",
      "\n",
      "Class balance in training set:\n",
      "  Real D⁰ (positive, label=1): 216,659 (84.1%)\n",
      "  Fake D⁰ (negative, label=0): 41,072 (15.9%)\n",
      "  Class ratio (Fake/Real): 0.19\n",
      "  XGBoost scale_pos_weight: 0.19 (for balancing)\n",
      "\n",
      "================================================================================\n",
      "HYPERPARAMETER OPTIMIZATION WITH EARLY STOPPING\n",
      "================================================================================\n",
      "\n",
      "Running RandomizedSearchCV with 50 iterations...\n",
      "Using CONSERVATIVE parameter ranges to prevent overfitting\n",
      "\n",
      "  Train (for fitting): 206,184 events\n",
      "  Validation (for early stopping): 51,547 events\n",
      "\n",
      "Conservative hyperparameter ranges:\n",
      "  learning_rate: [0.01, 0.10]\n",
      "  max_depth: [2, 6] (shallower trees)\n",
      "  n_estimators: [50, 200] (fewer trees)\n",
      "  reg_lambda: [1, 10] (stronger L2 regularization)\n",
      "  gamma: [1, 5] (require more gain)\n",
      "  subsample: [0.6, 0.9]\n",
      "  min_child_weight: [3, 10] (more samples per leaf)\n",
      "  colsample_bytree: [0.5, 0.9]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "OPTIMIZATION RESULTS\n",
      "================================================================================\n",
      "\n",
      "Best hyperparameters:\n",
      "  colsample_bytree: 0.6587135308855554\n",
      "  gamma: 1.2030741241575877\n",
      "  learning_rate: 0.08979554340555938\n",
      "  max_depth: 4\n",
      "  min_child_weight: 6\n",
      "  n_estimators: 147\n",
      "  reg_lambda: 1.845837458567821\n",
      "  subsample: 0.7734840422988521\n",
      "\n",
      "Best CV score (ROC AUC): 0.8996\n",
      "\n",
      "Cross-validation performance for best model:\n",
      "  Mean train score: 0.9053\n",
      "  Mean test score:  0.8996\n",
      "  Difference:       +0.0058\n",
      "  ✓ Good train/test agreement in CV\n",
      "\n",
      "✓ No boundary violations detected\n",
      "\n",
      "✓ BDT training complete\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "BDT STATISTICS\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Set Performance:\n",
      "  Accuracy: 0.8053\n",
      "  ROC AUC:  0.9037\n",
      "\n",
      "Test Set Performance:\n",
      "  Accuracy: 0.8029\n",
      "  ROC AUC:  0.8995\n",
      "\n",
      "Overtraining Check (Test - Train):\n",
      "  ΔAccuracy: -0.0024\n",
      "  ΔAUC:      -0.0042\n",
      "  ✓ Good agreement (no overtraining detected)\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "FEATURE IMPORTANCE\n",
      "\n",
      "Total variables being used for training: 12\n",
      "\n",
      "Verifying all variables from final_variables.py are included...\n",
      "  ✓ All 12 variables from final_variables.py are being used\n",
      "================================================================================\n",
      "\n",
      "✓ Saved complete feature importance table to: /home/belle2/amubarak/Ds2D0enue_Analysis/05-ML/Figures/FakeD0_BDT/kmpip/feature_importance_all.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 Most Important Features:\n",
      "       Value               Feature\n",
      "2   0.340248  D0_daughterAngle_0_1\n",
      "11  0.114610             pi_Ch1_dr\n",
      "3   0.095330   D0_cos_decayAngle_0\n",
      "4   0.075201     D0_flightDistance\n",
      "5   0.069591  D0_flightDistanceErr\n",
      "1   0.059211            D0_chiProb\n",
      "8   0.055531              K_Ch1_dr\n",
      "7   0.048727              K_Ch1_pt\n",
      "10  0.046838             pi_Ch1_pt\n",
      "6   0.040253          K_Ch1_kaonID\n",
      "\n",
      "================================================================================\n",
      "BDT OUTPUT COMPARISON (Train vs Test with Pull Plots)\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Plotting ROC curve...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "OPTIMIZING BDT CUT USING PUNZI FoM\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optimizing BDT cut for different confidence levels:\n",
      "\n",
      "  90% CL  : Optimal cut = 0.899, Punzi FoM = 32.006\n",
      "  95% CL  : Optimal cut = 0.899, Punzi FoM = 32.0059\n",
      "  3σ      : Optimal cut = 0.899, Punzi FoM = 32.0051\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Saved Punzi FoM plot to: /home/belle2/amubarak/Ds2D0enue_Analysis/05-ML/Figures/FakeD0_BDT/kmpip/punzi_fom_optimization.png\n",
      "\n",
      "================================================================================\n",
      "GENERATING BDT SUMMARY\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Saved JSON summary to: /home/belle2/amubarak/Ds2D0enue_Analysis/05-ML/Figures/FakeD0_BDT/kmpip/bdt_summary.json\n",
      "✓ Saved text summary to: /home/belle2/amubarak/Ds2D0enue_Analysis/05-ML/Figures/FakeD0_BDT/kmpip/bdt_summary.txt\n",
      "\n",
      "================================================================================\n",
      "FAKE D⁰ BDT TRAINING SUMMARY - KMPIP\n",
      "================================================================================\n",
      "\n",
      "Generated: 2025-12-12T16:20:32.819923\n",
      "\n",
      "DATASET INFORMATION\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Training set:   257,731 events\n",
      "  - Signal:     216,659 events\n",
      "  - Background: 41,072 events\n",
      "\n",
      "Test set:       110,457 events\n",
      "  - Signal:     92,855 events\n",
      "  - Background: 17,602 events\n",
      "\n",
      "Features:       12 variables\n",
      "\n",
      "PERFORMANCE METRICS\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "ROC AUC:\n",
      "  Training:     0.9037\n",
      "  Test:         0.8995\n",
      "  Difference:   -0.0042\n",
      "\n",
      "Kolmogorov-Smirnov Test (p-values):\n",
      "  Signal:       0.8332\n",
      "  Background:   0.0056\n",
      "\n",
      "Additional Distribution Metrics:\n",
      "  Jensen-Shannon Divergence (Background): 0.0262\n",
      "  Jensen-Shannon Divergence (Signal):     0.0112\n",
      "  Wasserstein Distance (Background):      0.0068\n",
      "  Wasserstein Distance (Signal):          0.0007\n",
      "\n",
      "Pull Statistics (Background):\n",
      "  Mean:         +0.192\n",
      "  Std Dev:      1.351\n",
      "  Max |Pull|:   3.075\n",
      "\n",
      "OPTIMAL BDT CUTS (Punzi FoM)\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "  90% CL  : 0.8995\n",
      "  95% CL  : 0.8995\n",
      "  3σ      : 0.8995\n",
      "\n",
      "TOP 10 MOST IMPORTANT FEATURES\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "   1. D0_daughterAngle_0_1                     0.3402\n",
      "   2. pi_Ch1_dr                                0.1146\n",
      "   3. D0_cos_decayAngle_0                      0.0953\n",
      "   4. D0_flightDistance                        0.0752\n",
      "   5. D0_flightDistanceErr                     0.0696\n",
      "   6. D0_chiProb                               0.0592\n",
      "   7. K_Ch1_dr                                 0.0555\n",
      "   8. K_Ch1_pt                                 0.0487\n",
      "   9. pi_Ch1_pt                                0.0468\n",
      "  10. K_Ch1_kaonID                             0.0403\n",
      "\n",
      "HYPERPARAMETERS\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "  objective                : binary:logistic\n",
      "  base_score               : None\n",
      "  booster                  : None\n",
      "  callbacks                : None\n",
      "  colsample_bylevel        : None\n",
      "  colsample_bynode         : None\n",
      "  colsample_bytree         : 0.658714\n",
      "  device                   : None\n",
      "  early_stopping_rounds    : 20\n",
      "  enable_categorical       : False\n",
      "  eval_metric              : logloss\n",
      "  feature_types            : None\n",
      "  gamma                    : 1.203074\n",
      "  grow_policy              : None\n",
      "  importance_type          : None\n",
      "  interaction_constraints  : None\n",
      "  learning_rate            : 0.089796\n",
      "  max_bin                  : None\n",
      "  max_cat_threshold        : None\n",
      "  max_cat_to_onehot        : None\n",
      "  max_delta_step           : 1\n",
      "  max_depth                : 4\n",
      "  max_leaves               : None\n",
      "  min_child_weight         : 6\n",
      "  missing                  : nan\n",
      "  monotone_constraints     : None\n",
      "  multi_strategy           : None\n",
      "  n_estimators             : 147\n",
      "  n_jobs                   : 1\n",
      "  num_parallel_tree        : None\n",
      "  random_state             : 42\n",
      "  reg_alpha                : None\n",
      "  reg_lambda               : 1.845837\n",
      "  sampling_method          : None\n",
      "  scale_pos_weight         : 0.189570\n",
      "  subsample                : 0.773484\n",
      "  tree_method              : None\n",
      "  validate_parameters      : None\n",
      "  verbosity                : 0\n",
      "\n",
      "MODEL QUALITY ASSESSMENT\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Overall Quality: GOOD\n",
      "\n",
      "Strengths:\n",
      "  ✓ Excellent train/test AUC agreement (no overtraining)\n",
      "  ✓ Signal KS test p-value = 0.833 (good agreement)\n",
      "  ✓ Low Jensen-Shannon divergence for background (0.026)\n",
      "\n",
      "Concerns:\n",
      "  ⚠ Low background KS test p-value = 0.006 (statistical difference detected)\n",
      "\n",
      "Recommendations:\n",
      "  → Check background train/test distribution shape - KS test is sensitive even if practical performance is good\n",
      "  → Model is usable. KS test is sensitive to shape differences even if discrimination power (AUC) is identical.\n",
      "\n",
      "================================================================================\n",
      "USAGE NOTES FOR OVERALL BACKGROUND BDT\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "\n",
      "This Fake D⁰ suppression BDT can be applied in two ways:\n",
      "\n",
      "1. HARD CUT: Apply recommended Punzi FoM cut as a selection requirement\n",
      "   - Use the optimal cut from the confidence level appropriate for your analysis\n",
      "   - Recommended for simple event selection\n",
      "\n",
      "2. BDT VARIABLE: Include Fake D⁰ BDT output as input to overall background BDT\n",
      "   - Allows overall BDT to learn optimal combination with other discriminants\n",
      "   - Recommended for maximum performance\n",
      "\n",
      "Recommendation: Test both approaches and compare final significance/FoM.\n",
      "\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "████████████████████████████████████████████████████████████████████████████████\n",
      "PROCESSING MODE: km3pi\n",
      "████████████████████████████████████████████████████████████████████████████████\n",
      "\n",
      "Decay chain: $D_s^{+} \\rightarrow [D^{0} \\rightarrow K^{-} 3\\pi] e^{+} \\nu_{e}$\n",
      "Using 25 variables from final_variables.py\n",
      "\n",
      "================================================================================\n",
      "Loading data for mode: km3pi\n",
      "Tree: DstreeCh3\n",
      "D⁰ mass cut: (-0.013093 <= D0_dM) & (D0_dM <= 0.012520)\n",
      "================================================================================\n",
      "\n",
      "Loading Signal MC from: /home/belle2/amubarak/C01-Simulated_Events/Signal/Ds2D0e-Signal_1_km3pi.root\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  → Signal MC: 12,250 events after D⁰ mass cut\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Loading generic MC:   0%|                                                                                                                                                                       | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Loading generic MC:  17%|██████████████████████████▎                                                                                                                                   | 1/6 [03:33<17:49, 213.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  → ccbar: 641,201 events after D⁰ mass cut\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Loading generic MC:  33%|█████████████████████████████████████████████████████                                                                                                          | 2/6 [03:50<06:31, 97.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  → charged: 20,919 events after D⁰ mass cut\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Loading generic MC:  50%|███████████████████████████████████████████████████████████████████████████████▌                                                                               | 3/6 [04:02<02:55, 58.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  → ddbar: 20,248 events after D⁰ mass cut\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Loading generic MC:  67%|██████████████████████████████████████████████████████████████████████████████████████████████████████████                                                     | 4/6 [04:21<01:25, 42.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  → mixed: 17,847 events after D⁰ mass cut\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Loading generic MC:  83%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                          | 5/6 [04:43<00:35, 35.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  → ssbar: 31,653 events after D⁰ mass cut\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Loading generic MC: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [05:29<00:00, 38.91s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Loading generic MC: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [05:29<00:00, 54.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  → uubar: 77,743 events after D⁰ mass cut\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  → Total Generic MC: 809,611 events\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Creating training set with CORRECTED labeling\n",
      "================================================================================\n",
      "Real D⁰ from Signal MC: 11,144 events\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real D⁰ from Generic MC: 424,156 events\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Real D⁰: 435,300 events\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fake D⁰ from Generic MC: 385,455 events\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training set: 820,755 events\n",
      "  - Real D⁰ (label=1): 435,300\n",
      "  - Fake D⁰ (label=0): 385,455\n",
      "  - Class ratio (Real/Fake): 1.129\n",
      "  - Number of features: 25\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "TRAINING BDT (WITH ANTI-OVERTRAINING FIXES)\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 574,528 events\n",
      "Test set: 246,227 events\n",
      "\n",
      "Class balance in training set:\n",
      "  Real D⁰ (positive, label=1): 304,710 (53.0%)\n",
      "  Fake D⁰ (negative, label=0): 269,818 (47.0%)\n",
      "  Class ratio (Fake/Real): 0.89\n",
      "  XGBoost scale_pos_weight: 0.89 (for balancing)\n",
      "\n",
      "================================================================================\n",
      "HYPERPARAMETER OPTIMIZATION WITH EARLY STOPPING\n",
      "================================================================================\n",
      "\n",
      "Running RandomizedSearchCV with 50 iterations...\n",
      "Using CONSERVATIVE parameter ranges to prevent overfitting\n",
      "\n",
      "  Train (for fitting): 459,622 events\n",
      "  Validation (for early stopping): 114,906 events\n",
      "\n",
      "Conservative hyperparameter ranges:\n",
      "  learning_rate: [0.01, 0.10]\n",
      "  max_depth: [2, 6] (shallower trees)\n",
      "  n_estimators: [50, 200] (fewer trees)\n",
      "  reg_lambda: [1, 10] (stronger L2 regularization)\n",
      "  gamma: [1, 5] (require more gain)\n",
      "  subsample: [0.6, 0.9]\n",
      "  min_child_weight: [3, 10] (more samples per leaf)\n",
      "  colsample_bytree: [0.5, 0.9]\n",
      "\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "OPTIMIZATION RESULTS\n",
      "================================================================================\n",
      "\n",
      "Best hyperparameters:\n",
      "  colsample_bytree: 0.6587135308855554\n",
      "  gamma: 1.2030741241575877\n",
      "  learning_rate: 0.08979554340555938\n",
      "  max_depth: 4\n",
      "  min_child_weight: 6\n",
      "  n_estimators: 147\n",
      "  reg_lambda: 1.845837458567821\n",
      "  subsample: 0.7734840422988521\n",
      "\n",
      "Best CV score (ROC AUC): 0.8506\n",
      "\n",
      "Cross-validation performance for best model:\n",
      "  Mean train score: 0.8544\n",
      "  Mean test score:  0.8506\n",
      "  Difference:       +0.0038\n",
      "  ✓ Good train/test agreement in CV\n",
      "\n",
      "✓ No boundary violations detected\n",
      "\n",
      "✓ BDT training complete\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "BDT STATISTICS\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Set Performance:\n",
      "  Accuracy: 0.7685\n",
      "  ROC AUC:  0.8535\n",
      "\n",
      "Test Set Performance:\n",
      "  Accuracy: 0.7659\n",
      "  ROC AUC:  0.8507\n",
      "\n",
      "Overtraining Check (Test - Train):\n",
      "  ΔAccuracy: -0.0026\n",
      "  ΔAUC:      -0.0028\n",
      "  ✓ Good agreement (no overtraining detected)\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "FEATURE IMPORTANCE\n",
      "\n",
      "Total variables being used for training: 25\n",
      "\n",
      "Verifying all variables from final_variables.py are included...\n",
      "  ✓ All 25 variables from final_variables.py are being used\n",
      "================================================================================\n",
      "\n",
      "✓ Saved complete feature importance table to: /home/belle2/amubarak/Ds2D0enue_Analysis/05-ML/Figures/FakeD0_BDT/km3pi/feature_importance_all.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 Most Important Features:\n",
      "       Value               Feature\n",
      "6   0.169263      D0_useCMSFrame_p\n",
      "7   0.147589            D0_chiProb\n",
      "11  0.093432     D0_flightDistance\n",
      "20  0.067217            pi2_Ch3_pt\n",
      "12  0.064747  D0_flightDistanceErr\n",
      "13  0.052014          K_Ch3_kaonID\n",
      "23  0.044309            pi3_Ch3_pt\n",
      "17  0.037851            pi1_Ch3_pt\n",
      "24  0.036789            pi3_Ch3_dr\n",
      "21  0.034455            pi2_Ch3_dr\n",
      "\n",
      "================================================================================\n",
      "BDT OUTPUT COMPARISON (Train vs Test with Pull Plots)\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Plotting ROC curve...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "OPTIMIZING BDT CUT USING PUNZI FoM\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optimizing BDT cut for different confidence levels:\n",
      "\n",
      "  90% CL  : Optimal cut = 0.829, Punzi FoM = 16.972\n",
      "  95% CL  : Optimal cut = 0.829, Punzi FoM = 16.9719\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  3σ      : Optimal cut = 0.829, Punzi FoM = 16.9716\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Saved Punzi FoM plot to: /home/belle2/amubarak/Ds2D0enue_Analysis/05-ML/Figures/FakeD0_BDT/km3pi/punzi_fom_optimization.png\n",
      "\n",
      "================================================================================\n",
      "GENERATING BDT SUMMARY\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Saved JSON summary to: /home/belle2/amubarak/Ds2D0enue_Analysis/05-ML/Figures/FakeD0_BDT/km3pi/bdt_summary.json\n",
      "✓ Saved text summary to: /home/belle2/amubarak/Ds2D0enue_Analysis/05-ML/Figures/FakeD0_BDT/km3pi/bdt_summary.txt\n",
      "\n",
      "================================================================================\n",
      "FAKE D⁰ BDT TRAINING SUMMARY - KM3PI\n",
      "================================================================================\n",
      "\n",
      "Generated: 2025-12-12T16:33:45.042282\n",
      "\n",
      "DATASET INFORMATION\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Training set:   574,528 events\n",
      "  - Signal:     304,710 events\n",
      "  - Background: 269,818 events\n",
      "\n",
      "Test set:       246,227 events\n",
      "  - Signal:     130,590 events\n",
      "  - Background: 115,637 events\n",
      "\n",
      "Features:       25 variables\n",
      "\n",
      "PERFORMANCE METRICS\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "ROC AUC:\n",
      "  Training:     0.8535\n",
      "  Test:         0.8507\n",
      "  Difference:   -0.0028\n",
      "\n",
      "Kolmogorov-Smirnov Test (p-values):\n",
      "  Signal:       0.5950\n",
      "  Background:   0.0446\n",
      "\n",
      "Additional Distribution Metrics:\n",
      "  Jensen-Shannon Divergence (Background): 0.0090\n",
      "  Jensen-Shannon Divergence (Signal):     0.0068\n",
      "  Wasserstein Distance (Background):      0.0022\n",
      "  Wasserstein Distance (Signal):          0.0011\n",
      "\n",
      "Pull Statistics (Background):\n",
      "  Mean:         +0.188\n",
      "  Std Dev:      1.209\n",
      "  Max |Pull|:   4.225\n",
      "\n",
      "OPTIMAL BDT CUTS (Punzi FoM)\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "  90% CL  : 0.8291\n",
      "  95% CL  : 0.8291\n",
      "  3σ      : 0.8291\n",
      "\n",
      "TOP 10 MOST IMPORTANT FEATURES\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "   1. D0_useCMSFrame_p                         0.1693\n",
      "   2. D0_chiProb                               0.1476\n",
      "   3. D0_flightDistance                        0.0934\n",
      "   4. pi2_Ch3_pt                               0.0672\n",
      "   5. D0_flightDistanceErr                     0.0647\n",
      "   6. K_Ch3_kaonID                             0.0520\n",
      "   7. pi3_Ch3_pt                               0.0443\n",
      "   8. pi1_Ch3_pt                               0.0379\n",
      "   9. pi3_Ch3_dr                               0.0368\n",
      "  10. pi2_Ch3_dr                               0.0345\n",
      "\n",
      "HYPERPARAMETERS\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "  objective                : binary:logistic\n",
      "  base_score               : None\n",
      "  booster                  : None\n",
      "  callbacks                : None\n",
      "  colsample_bylevel        : None\n",
      "  colsample_bynode         : None\n",
      "  colsample_bytree         : 0.658714\n",
      "  device                   : None\n",
      "  early_stopping_rounds    : 20\n",
      "  enable_categorical       : False\n",
      "  eval_metric              : logloss\n",
      "  feature_types            : None\n",
      "  gamma                    : 1.203074\n",
      "  grow_policy              : None\n",
      "  importance_type          : None\n",
      "  interaction_constraints  : None\n",
      "  learning_rate            : 0.089796\n",
      "  max_bin                  : None\n",
      "  max_cat_threshold        : None\n",
      "  max_cat_to_onehot        : None\n",
      "  max_delta_step           : 1\n",
      "  max_depth                : 4\n",
      "  max_leaves               : None\n",
      "  min_child_weight         : 6\n",
      "  missing                  : nan\n",
      "  monotone_constraints     : None\n",
      "  multi_strategy           : None\n",
      "  n_estimators             : 147\n",
      "  n_jobs                   : 1\n",
      "  num_parallel_tree        : None\n",
      "  random_state             : 42\n",
      "  reg_alpha                : None\n",
      "  reg_lambda               : 1.845837\n",
      "  sampling_method          : None\n",
      "  scale_pos_weight         : 0.885491\n",
      "  subsample                : 0.773484\n",
      "  tree_method              : None\n",
      "  validate_parameters      : None\n",
      "  verbosity                : 0\n",
      "\n",
      "MODEL QUALITY ASSESSMENT\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Overall Quality: GOOD\n",
      "\n",
      "Strengths:\n",
      "  ✓ Excellent train/test AUC agreement (no overtraining)\n",
      "  ✓ Signal KS test p-value = 0.595 (good agreement)\n",
      "  ✓ Low Jensen-Shannon divergence for background (0.009)\n",
      "\n",
      "Concerns:\n",
      "  ⚠ Low background KS test p-value = 0.045 (statistical difference detected)\n",
      "\n",
      "Recommendations:\n",
      "  → Check background train/test distribution shape - KS test is sensitive even if practical performance is good\n",
      "  → Model is usable. KS test is sensitive to shape differences even if discrimination power (AUC) is identical.\n",
      "\n",
      "================================================================================\n",
      "USAGE NOTES FOR OVERALL BACKGROUND BDT\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "\n",
      "This Fake D⁰ suppression BDT can be applied in two ways:\n",
      "\n",
      "1. HARD CUT: Apply recommended Punzi FoM cut as a selection requirement\n",
      "   - Use the optimal cut from the confidence level appropriate for your analysis\n",
      "   - Recommended for simple event selection\n",
      "\n",
      "2. BDT VARIABLE: Include Fake D⁰ BDT output as input to overall background BDT\n",
      "   - Allows overall BDT to learn optimal combination with other discriminants\n",
      "   - Recommended for maximum performance\n",
      "\n",
      "Recommendation: Test both approaches and compare final significance/FoM.\n",
      "\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "████████████████████████████████████████████████████████████████████████████████\n",
      "PROCESSING MODE: kmpippi0_eff20_May2020\n",
      "████████████████████████████████████████████████████████████████████████████████\n",
      "\n",
      "Decay chain: $D_s^{+} \\rightarrow [D^{0} \\rightarrow K^{-} \\pi^{+} \\pi^{0}] e^{+} \\nu_{e}$\n",
      "Using 18 variables from final_variables.py\n",
      "\n",
      "================================================================================\n",
      "Loading data for mode: kmpippi0_eff20_May2020\n",
      "Tree: DstreeCh2\n",
      "D⁰ mass cut: (-0.052152 <= D0_dM) & (D0_dM <= 0.024237)\n",
      "================================================================================\n",
      "\n",
      "Loading Signal MC from: /home/belle2/amubarak/C01-Simulated_Events/Signal/Ds2D0e-Signal_1_kmpippi0_eff20_May2020.root\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  → Signal MC: 9,717 events after D⁰ mass cut\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Loading generic MC:   0%|                                                                                                                                                                       | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Loading generic MC:  17%|██████████████████████████▎                                                                                                                                   | 1/6 [05:43<28:36, 343.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  → ccbar: 884,012 events after D⁰ mass cut\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Loading generic MC:  33%|████████████████████████████████████████████████████▋                                                                                                         | 2/6 [06:12<10:33, 158.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  → charged: 35,976 events after D⁰ mass cut\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Loading generic MC:  50%|███████████████████████████████████████████████████████████████████████████████                                                                               | 3/6 [06:43<05:00, 100.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  → ddbar: 48,948 events after D⁰ mass cut\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Loading generic MC:  67%|██████████████████████████████████████████████████████████████████████████████████████████████████████████                                                     | 4/6 [07:00<02:14, 67.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  → mixed: 21,497 events after D⁰ mass cut\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Loading generic MC:  83%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                          | 5/6 [07:50<01:01, 61.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  → ssbar: 77,372 events after D⁰ mass cut\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Loading generic MC: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [09:53<00:00, 82.34s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Loading generic MC: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [09:53<00:00, 98.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  → uubar: 196,677 events after D⁰ mass cut\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  → Total Generic MC: 1,264,482 events\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Creating training set with CORRECTED labeling\n",
      "================================================================================\n",
      "Real D⁰ from Signal MC: 7,835 events\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real D⁰ from Generic MC: 466,357 events\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Real D⁰: 474,192 events\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fake D⁰ from Generic MC: 798,125 events\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training set: 1,272,317 events\n",
      "  - Real D⁰ (label=1): 474,192\n",
      "  - Fake D⁰ (label=0): 798,125\n",
      "  - Class ratio (Real/Fake): 0.594\n",
      "  - Number of features: 18\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "TRAINING BDT (WITH ANTI-OVERTRAINING FIXES)\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 890,621 events\n",
      "Test set: 381,696 events\n",
      "\n",
      "Class balance in training set:\n",
      "  Real D⁰ (positive, label=1): 331,934 (37.3%)\n",
      "  Fake D⁰ (negative, label=0): 558,687 (62.7%)\n",
      "  Class ratio (Fake/Real): 1.68\n",
      "  XGBoost scale_pos_weight: 1.68 (for balancing)\n",
      "\n",
      "================================================================================\n",
      "HYPERPARAMETER OPTIMIZATION WITH EARLY STOPPING\n",
      "================================================================================\n",
      "\n",
      "Running RandomizedSearchCV with 50 iterations...\n",
      "Using CONSERVATIVE parameter ranges to prevent overfitting\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Train (for fitting): 712,496 events\n",
      "  Validation (for early stopping): 178,125 events\n",
      "\n",
      "Conservative hyperparameter ranges:\n",
      "  learning_rate: [0.01, 0.10]\n",
      "  max_depth: [2, 6] (shallower trees)\n",
      "  n_estimators: [50, 200] (fewer trees)\n",
      "  reg_lambda: [1, 10] (stronger L2 regularization)\n",
      "  gamma: [1, 5] (require more gain)\n",
      "  subsample: [0.6, 0.9]\n",
      "  min_child_weight: [3, 10] (more samples per leaf)\n",
      "  colsample_bytree: [0.5, 0.9]\n",
      "\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "OPTIMIZATION RESULTS\n",
      "================================================================================\n",
      "\n",
      "Best hyperparameters:\n",
      "  colsample_bytree: 0.6587135308855554\n",
      "  gamma: 1.2030741241575877\n",
      "  learning_rate: 0.08979554340555938\n",
      "  max_depth: 4\n",
      "  min_child_weight: 6\n",
      "  n_estimators: 147\n",
      "  reg_lambda: 1.845837458567821\n",
      "  subsample: 0.7734840422988521\n",
      "\n",
      "Best CV score (ROC AUC): 0.8577\n",
      "\n",
      "Cross-validation performance for best model:\n",
      "  Mean train score: 0.8599\n",
      "  Mean test score:  0.8577\n",
      "  Difference:       +0.0022\n",
      "  ✓ Good train/test agreement in CV\n",
      "\n",
      "✓ No boundary violations detected\n",
      "\n",
      "✓ BDT training complete\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "BDT STATISTICS\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Set Performance:\n",
      "  Accuracy: 0.7765\n",
      "  ROC AUC:  0.8592\n",
      "\n",
      "Test Set Performance:\n",
      "  Accuracy: 0.7757\n",
      "  ROC AUC:  0.8586\n",
      "\n",
      "Overtraining Check (Test - Train):\n",
      "  ΔAccuracy: -0.0009\n",
      "  ΔAUC:      -0.0005\n",
      "  ✓ Good agreement (no overtraining detected)\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "FEATURE IMPORTANCE\n",
      "\n",
      "Total variables being used for training: 18\n",
      "\n",
      "Verifying all variables from final_variables.py are included...\n",
      "  ✓ All 18 variables from final_variables.py are being used\n",
      "================================================================================\n",
      "\n",
      "✓ Saved complete feature importance table to: /home/belle2/amubarak/Ds2D0enue_Analysis/05-ML/Figures/FakeD0_BDT/kmpippi0_eff20_May2020/feature_importance_all.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 Most Important Features:\n",
      "       Value               Feature\n",
      "4   0.187079      D0_useCMSFrame_p\n",
      "17  0.090898            pi0_Ch2_pt\n",
      "10  0.078728  D0_flightDistanceErr\n",
      "6   0.077284  D0_daughterAngle_0_1\n",
      "9   0.075191     D0_flightDistance\n",
      "2   0.071759   D0_daughterInvM_0_2\n",
      "3   0.053523   D0_daughterInvM_1_2\n",
      "11  0.052263          K_Ch2_kaonID\n",
      "7   0.048726   D0_cos_decayAngle_0\n",
      "15  0.045707             pi_Ch2_pt\n",
      "\n",
      "================================================================================\n",
      "BDT OUTPUT COMPARISON (Train vs Test with Pull Plots)\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Plotting ROC curve...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "OPTIMIZING BDT CUT USING PUNZI FoM\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optimizing BDT cut for different confidence levels:\n",
      "\n",
      "  90% CL  : Optimal cut = 0.678, Punzi FoM = 9.41137\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  95% CL  : Optimal cut = 0.678, Punzi FoM = 9.41136\n",
      "  3σ      : Optimal cut = 0.678, Punzi FoM = 9.41129\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Saved Punzi FoM plot to: /home/belle2/amubarak/Ds2D0enue_Analysis/05-ML/Figures/FakeD0_BDT/kmpippi0_eff20_May2020/punzi_fom_optimization.png\n",
      "\n",
      "================================================================================\n",
      "GENERATING BDT SUMMARY\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Saved JSON summary to: /home/belle2/amubarak/Ds2D0enue_Analysis/05-ML/Figures/FakeD0_BDT/kmpippi0_eff20_May2020/bdt_summary.json\n",
      "✓ Saved text summary to: /home/belle2/amubarak/Ds2D0enue_Analysis/05-ML/Figures/FakeD0_BDT/kmpippi0_eff20_May2020/bdt_summary.txt\n",
      "\n",
      "================================================================================\n",
      "FAKE D⁰ BDT TRAINING SUMMARY - KMPIPPI0_EFF20_MAY2020\n",
      "================================================================================\n",
      "\n",
      "Generated: 2025-12-12T16:53:47.754122\n",
      "\n",
      "DATASET INFORMATION\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Training set:   890,621 events\n",
      "  - Signal:     331,934 events\n",
      "  - Background: 558,687 events\n",
      "\n",
      "Test set:       381,696 events\n",
      "  - Signal:     142,258 events\n",
      "  - Background: 239,438 events\n",
      "\n",
      "Features:       18 variables\n",
      "\n",
      "PERFORMANCE METRICS\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "ROC AUC:\n",
      "  Training:     0.8592\n",
      "  Test:         0.8586\n",
      "  Difference:   -0.0005\n",
      "\n",
      "Kolmogorov-Smirnov Test (p-values):\n",
      "  Signal:       0.5640\n",
      "  Background:   0.7825\n",
      "\n",
      "Additional Distribution Metrics:\n",
      "  Jensen-Shannon Divergence (Background): 0.0058\n",
      "  Jensen-Shannon Divergence (Signal):     0.0066\n",
      "  Wasserstein Distance (Background):      0.0005\n",
      "  Wasserstein Distance (Signal):          0.0008\n",
      "\n",
      "Pull Statistics (Background):\n",
      "  Mean:         +0.003\n",
      "  Std Dev:      1.131\n",
      "  Max |Pull|:   2.653\n",
      "\n",
      "OPTIMAL BDT CUTS (Punzi FoM)\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "  90% CL  : 0.6784\n",
      "  95% CL  : 0.6784\n",
      "  3σ      : 0.6784\n",
      "\n",
      "TOP 10 MOST IMPORTANT FEATURES\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "   1. D0_useCMSFrame_p                         0.1871\n",
      "   2. pi0_Ch2_pt                               0.0909\n",
      "   3. D0_flightDistanceErr                     0.0787\n",
      "   4. D0_daughterAngle_0_1                     0.0773\n",
      "   5. D0_flightDistance                        0.0752\n",
      "   6. D0_daughterInvM_0_2                      0.0718\n",
      "   7. D0_daughterInvM_1_2                      0.0535\n",
      "   8. K_Ch2_kaonID                             0.0523\n",
      "   9. D0_cos_decayAngle_0                      0.0487\n",
      "  10. pi_Ch2_pt                                0.0457\n",
      "\n",
      "HYPERPARAMETERS\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "  objective                : binary:logistic\n",
      "  base_score               : None\n",
      "  booster                  : None\n",
      "  callbacks                : None\n",
      "  colsample_bylevel        : None\n",
      "  colsample_bynode         : None\n",
      "  colsample_bytree         : 0.658714\n",
      "  device                   : None\n",
      "  early_stopping_rounds    : 20\n",
      "  enable_categorical       : False\n",
      "  eval_metric              : logloss\n",
      "  feature_types            : None\n",
      "  gamma                    : 1.203074\n",
      "  grow_policy              : None\n",
      "  importance_type          : None\n",
      "  interaction_constraints  : None\n",
      "  learning_rate            : 0.089796\n",
      "  max_bin                  : None\n",
      "  max_cat_threshold        : None\n",
      "  max_cat_to_onehot        : None\n",
      "  max_delta_step           : 1\n",
      "  max_depth                : 4\n",
      "  max_leaves               : None\n",
      "  min_child_weight         : 6\n",
      "  missing                  : nan\n",
      "  monotone_constraints     : None\n",
      "  multi_strategy           : None\n",
      "  n_estimators             : 147\n",
      "  n_jobs                   : 1\n",
      "  num_parallel_tree        : None\n",
      "  random_state             : 42\n",
      "  reg_alpha                : None\n",
      "  reg_lambda               : 1.845837\n",
      "  sampling_method          : None\n",
      "  scale_pos_weight         : 1.683127\n",
      "  subsample                : 0.773484\n",
      "  tree_method              : None\n",
      "  validate_parameters      : None\n",
      "  verbosity                : 0\n",
      "\n",
      "MODEL QUALITY ASSESSMENT\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Overall Quality: EXCELLENT\n",
      "\n",
      "Strengths:\n",
      "  ✓ Excellent train/test AUC agreement (no overtraining)\n",
      "  ✓ Signal KS test p-value = 0.564 (good agreement)\n",
      "  ✓ Background KS test p-value = 0.783 (good agreement)\n",
      "  ✓ Low Jensen-Shannon divergence for background (0.006)\n",
      "\n",
      "================================================================================\n",
      "USAGE NOTES FOR OVERALL BACKGROUND BDT\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "\n",
      "This Fake D⁰ suppression BDT can be applied in two ways:\n",
      "\n",
      "1. HARD CUT: Apply recommended Punzi FoM cut as a selection requirement\n",
      "   - Use the optimal cut from the confidence level appropriate for your analysis\n",
      "   - Recommended for simple event selection\n",
      "\n",
      "2. BDT VARIABLE: Include Fake D⁰ BDT output as input to overall background BDT\n",
      "   - Allows overall BDT to learn optimal combination with other discriminants\n",
      "   - Recommended for maximum performance\n",
      "\n",
      "Recommendation: Test both approaches and compare final significance/FoM.\n",
      "\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "✓ ALL PROCESSING COMPLETE\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "if TRAIN_MODE == \"all\":\n",
    "    # Train on all modes sequentially\n",
    "    for mode in DECAY_CONFIG.keys():\n",
    "        process_mode(mode)\n",
    "else:\n",
    "    # Train on single mode\n",
    "    if TRAIN_MODE not in DECAY_CONFIG:\n",
    "        raise ValueError(f\"Unknown mode: {TRAIN_MODE}. Choose from {list(DECAY_CONFIG.keys())} or 'all'\")\n",
    "    process_mode(TRAIN_MODE)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✓ ALL PROCESSING COMPLETE\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Control Sample Processing (Optional)\n",
    "\n",
    "Load and apply BDT to control samples for systematic studies.\n",
    "\n",
    "**Available control samples**:\n",
    "- `WCh`: Wrong charge (opposite sign combinations)\n",
    "- `ReverseID`: Reversed particle ID (e.g., π identified as K)\n",
    "- `ReverseID_WCh`: Combination of both\n",
    "\n",
    "**Note**: This section is currently disabled. Set `LOAD_CONTROL_SAMPLES = True` in the configuration to enable.\n",
    "\n",
    "Control samples are useful for:\n",
    "- Validating BDT performance on data-driven backgrounds\n",
    "- Estimating systematic uncertainties\n",
    "- Cross-checking signal/background discrimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T07:53:47.761655Z",
     "iopub.status.busy": "2025-12-12T07:53:47.761517Z",
     "iopub.status.idle": "2025-12-12T07:53:47.766880Z",
     "shell.execute_reply": "2025-12-12T07:53:47.766504Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Control sample processing is disabled (LOAD_CONTROL_SAMPLES = False)\n"
     ]
    }
   ],
   "source": [
    "if LOAD_CONTROL_SAMPLES:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"PROCESSING CONTROL SAMPLES\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Get the trained BDT (need to reload or keep from previous cell)\n",
    "    # For now, we'll process control samples separately per mode\n",
    "    \n",
    "    for control_sample in CONTROL_SAMPLES:\n",
    "        print(f\"\\nProcessing control sample: {control_sample}\")\n",
    "        \n",
    "        # Load data\n",
    "        df_signal_ctrl, df_generic_ctrl = load_mode_data(TRAIN_MODE, control_sample=control_sample)\n",
    "        \n",
    "        if df_signal_ctrl.empty or df_generic_ctrl.empty:\n",
    "            print(f\"  ✗ No data for control sample {control_sample}\")\n",
    "            continue\n",
    "        \n",
    "        # Get variables for this mode\n",
    "        mode_variables = VARIABLES[TRAIN_MODE][\"all_vars\"]\n",
    "        mode_variables_filtered = [v for v in mode_variables if v in df_signal_ctrl.columns]\n",
    "        \n",
    "        # Apply BDT (would need to reload trained model from saved file)\n",
    "        # For demonstration, assuming we have bdt_final from main training\n",
    "        # In practice, you'd save the model and reload it here\n",
    "        \n",
    "        print(f\"  Loaded {len(df_signal_ctrl):,} signal events\")\n",
    "        print(f\"  Loaded {len(df_generic_ctrl):,} generic events\")\n",
    "        \n",
    "        # Save outputs\n",
    "        if SAVE_OUTPUT:\n",
    "            output_dir_ctrl = os.path.join(OUTPUT_BASE_DIR, f\"FakeD0_{control_sample}\", TRAIN_MODE)\n",
    "            os.makedirs(output_dir_ctrl, exist_ok=True)\n",
    "            \n",
    "            # Save signal\n",
    "            signal_path = os.path.join(output_dir_ctrl, f\"Ds2D0enu-Signal_{TRAIN_MODE}_withBDT.root\")\n",
    "            with uproot.recreate(signal_path) as f:\n",
    "                f[\"Dstree\"] = df_signal_ctrl\n",
    "            print(f\"  Saved: {signal_path}\")\n",
    "            \n",
    "            # Save generic\n",
    "            generic_path = os.path.join(output_dir_ctrl, f\"Ds2D0e-Generic_{TRAIN_MODE}_withBDT.root\")\n",
    "            with uproot.recreate(generic_path) as f:\n",
    "                f[\"Dstree\"] = df_generic_ctrl\n",
    "            print(f\"  Saved: {generic_path}\")\n",
    "        \n",
    "        # Clear memory\n",
    "        del df_signal_ctrl, df_generic_ctrl\n",
    "        gc.collect()\n",
    "        \n",
    "    print(\"\\n✓ Control sample processing complete\")\n",
    "else:\n",
    "    print(\"\\nControl sample processing is disabled (LOAD_CONTROL_SAMPLES = False)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Control Sample / Data-MC Comparison (Optional)\n",
    "\n",
    "This section is for validating the BDT on control samples or sideband data.\n",
    "\n",
    "**Use cases**:\n",
    "1. **Control Samples**: Load control samples (WCh, ReverseID, etc.) and apply the trained BDT\n",
    "2. **Sideband Data**: Load signal region sideband data for data-MC comparison\n",
    "3. **BDT Output Comparison**: Compare BDT distributions between MC and data/control samples\n",
    "4. **Variable Comparison**: Compare input variables between MC and control samples\n",
    "\n",
    "**Available control samples**:\n",
    "- `WCh`: Wrong charge (opposite sign combinations)\n",
    "- `ReverseID`: Reversed particle ID (e.g., π identified as K)\n",
    "- `ReverseID_WCh`: Combination of both\n",
    "\n",
    "**Note**: Set `LOAD_CONTROL_SAMPLES = True` in the configuration to enable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T07:53:47.768197Z",
     "iopub.status.busy": "2025-12-12T07:53:47.768074Z",
     "iopub.status.idle": "2025-12-12T07:53:47.867420Z",
     "shell.execute_reply": "2025-12-12T07:53:47.866910Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Control sample processing is disabled (LOAD_CONTROL_SAMPLES = False)\n",
      "Set LOAD_CONTROL_SAMPLES = True in the configuration to enable data-MC comparison.\n"
     ]
    }
   ],
   "source": [
    "# This cell provides a template for data-MC comparison using control samples\n",
    "# Modify as needed for your specific analysis\n",
    "\n",
    "if LOAD_CONTROL_SAMPLES:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"DATA-MC COMPARISON WITH CONTROL SAMPLES\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # TODO: Load the trained BDT model for the mode you want to validate\n",
    "    # You may need to save/load the model from the training step\n",
    "    # For now, assuming you have bdt_final and mode_variables_filtered from training\n",
    "    \n",
    "    for control_sample in CONTROL_SAMPLES:\n",
    "        print(f\"\\n{'-'*80}\")\n",
    "        print(f\"Processing control sample: {control_sample}\")\n",
    "        print(f\"{'-'*80}\")\n",
    "        \n",
    "        # Load control sample data\n",
    "        df_signal_ctrl, df_generic_ctrl = load_mode_data(TRAIN_MODE, control_sample=control_sample)\n",
    "        \n",
    "        if df_signal_ctrl.empty or df_generic_ctrl.empty:\n",
    "            print(f\"  ✗ No data for control sample {control_sample}\")\n",
    "            continue\n",
    "        \n",
    "        # Filter variables (same as training)\n",
    "        mode_variables_ctrl = filter_valid_variables(\n",
    "            pd.concat([df_signal_ctrl, df_generic_ctrl], ignore_index=True),\n",
    "            VARIABLES[TRAIN_MODE][\"all_vars\"],\n",
    "            nan_threshold=0.5\n",
    "        )\n",
    "        \n",
    "        print(f\"  Valid variables: {len(mode_variables_ctrl)}\")\n",
    "        \n",
    "        # Apply BDT to control samples (requires trained model)\n",
    "        # df_signal_ctrl[\"Ds_FakeD0BDT\"] = bdt_final.predict_proba(\n",
    "        #     df_signal_ctrl[mode_variables_ctrl]\n",
    "        # )[:, 1].astype(np.float32)\n",
    "        \n",
    "        # df_generic_ctrl[\"Ds_FakeD0BDT\"] = bdt_final.predict_proba(\n",
    "        #     df_generic_ctrl[mode_variables_ctrl]\n",
    "        # )[:, 1].astype(np.float32)\n",
    "        \n",
    "        # Example: Compare variable distributions (MC vs control sample)\n",
    "        # Select a few important variables to compare\n",
    "        # vars_to_compare = mode_variables_ctrl[:5]  # First 5 variables\n",
    "        \n",
    "        # for var in vars_to_compare:\n",
    "        #     plt.figure(figsize=(10, 6))\n",
    "        #     \n",
    "        #     # Plot MC\n",
    "        #     plt.hist(df_signal[var].dropna(), bins=50, alpha=0.5, \n",
    "        #              label='MC', density=True, histtype='step', linewidth=2)\n",
    "        #     \n",
    "        #     # Plot control sample\n",
    "        #     plt.hist(df_signal_ctrl[var].dropna(), bins=50, alpha=0.5,\n",
    "        #              label=f'Control ({control_sample})', density=True, \n",
    "        #              histtype='step', linewidth=2)\n",
    "        #     \n",
    "        #     plt.xlabel(var)\n",
    "        #     plt.ylabel('Event Density')\n",
    "        #     plt.legend()\n",
    "        #     plt.title(f'{MODE_TITLES[TRAIN_MODE]} - {var}', loc='left')\n",
    "        #     plt.show()\n",
    "        \n",
    "        # Example: Compare BDT output distributions\n",
    "        # plt.figure(figsize=(10, 6))\n",
    "        # plt.hist(df_signal[\"Ds_FakeD0BDT\"], bins=50, alpha=0.5,\n",
    "        #          label='MC', density=True, range=(0, 1), histtype='step', linewidth=2)\n",
    "        # plt.hist(df_signal_ctrl[\"Ds_FakeD0BDT\"], bins=50, alpha=0.5,\n",
    "        #          label=f'Control ({control_sample})', density=True, range=(0, 1),\n",
    "        #          histtype='step', linewidth=2)\n",
    "        # plt.xlabel('Fake D⁰ BDT Output')\n",
    "        # plt.ylabel('Event Density')\n",
    "        # plt.legend()\n",
    "        # plt.title(f'{MODE_TITLES[TRAIN_MODE]} - BDT Output Comparison', loc='left')\n",
    "        # plt.show()\n",
    "        \n",
    "        print(f\"  Loaded {len(df_signal_ctrl):,} signal events\")\n",
    "        print(f\"  Loaded {len(df_generic_ctrl):,} generic events\")\n",
    "        \n",
    "        # Save outputs if requested\n",
    "        if SAVE_OUTPUT:\n",
    "            output_dir_ctrl = os.path.join(OUTPUT_BASE_DIR, f\"FakeD0_{control_sample}\", TRAIN_MODE)\n",
    "            os.makedirs(output_dir_ctrl, exist_ok=True)\n",
    "            \n",
    "            # Save signal\n",
    "            signal_path = os.path.join(output_dir_ctrl, f\"Ds2D0enu-Signal_{TRAIN_MODE}_withBDT.root\")\n",
    "            with uproot.recreate(signal_path) as f:\n",
    "                f[\"Dstree\"] = df_signal_ctrl\n",
    "            print(f\"  Saved: {signal_path}\")\n",
    "            \n",
    "            # Save generic\n",
    "            generic_path = os.path.join(output_dir_ctrl, f\"Ds2D0e-Generic_{TRAIN_MODE}_withBDT.root\")\n",
    "            with uproot.recreate(generic_path) as f:\n",
    "                f[\"Dstree\"] = df_generic_ctrl\n",
    "            print(f\"  Saved: {generic_path}\")\n",
    "        \n",
    "        # Clear memory\n",
    "        del df_signal_ctrl, df_generic_ctrl\n",
    "        gc.collect()\n",
    "        \n",
    "    print(\"\\n✓ Control sample processing complete\")\n",
    "else:\n",
    "    print(\"\\nControl sample processing is disabled (LOAD_CONTROL_SAMPLES = False)\")\n",
    "    print(\"Set LOAD_CONTROL_SAMPLES = True in the configuration to enable data-MC comparison.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Belle II PyROOT (ROOT 6.36.02, Py 3.9) [env]",
   "language": "python",
   "name": "belle2-pyroot-py39-root63602"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
